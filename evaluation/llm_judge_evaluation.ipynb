{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM-as-a-Judge Evaluation with OpenRouter\n",
        "\n",
        "This notebook evaluates different LLM models (via OpenRouter) on a QA dataset using GPT-4o as a judge.\n",
        "\n",
        "## How it works:\n",
        "1. Load a QA dataset (questions generated by GPT-4o)\n",
        "2. Test 3 different models via OpenRouter on the same questions:\n",
        "   - Claude 3.5 Sonnet (Anthropic)\n",
        "   - GPT-3.5-turbo (OpenAI)\n",
        "   - Google Gemini Pro (Google)\n",
        "3. Use GPT-4o as a judge to evaluate answer quality\n",
        "4. Get scores on 3 dimensions:\n",
        "   - **Retrieval Relevance**: How relevant are the retrieved documents to the query?\n",
        "   - **Faithfulness (Groundedness)**: Is the answer faithful to the retrieved context?\n",
        "   - **Answer Quality**: Overall quality of the answer\n",
        "\n",
        "## Setup\n",
        "\n",
        "### 1. Environment Variables\n",
        "In your `.env` file, you need:\n",
        "- `OPENAI_API_KEY`: Required for the judge model (GPT-4o)\n",
        "- `OPENROUTER_API_KEY`: Required for testing different models via OpenRouter\n",
        "- `FORCE_OPENROUTER=true`: Add this to force the system to use OpenRouter for all model calls\n",
        "\n",
        "Example `.env`:\n",
        "```\n",
        "OPENAI_API_KEY=sk-...\n",
        "OPENROUTER_API_KEY=sk-or-v1-...\n",
        "FORCE_OPENROUTER=true\n",
        "```\n",
        "\n",
        "### 2. Server Setup\n",
        "- FastAPI server must be running on `http://localhost:8000`\n",
        "- PDFs must be uploaded to the system before running evaluation\n",
        "- Restart the server after setting `FORCE_OPENROUTER=true` in `.env`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip install openai requests python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported\n",
            "API Base URL: http://localhost:8000\n",
            "Output Directory: llm_judge_results\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import List, Dict, Optional\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "API_BASE_URL = \"http://localhost:8000\"\n",
        "EVALUATION_OUTPUT_DIR = Path(\"llm_judge_results\")\n",
        "EVALUATION_OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"✓ Libraries imported\")\n",
        "print(f\"API Base URL: {API_BASE_URL}\")\n",
        "print(f\"Output Directory: {EVALUATION_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Judge Class\n",
        "\n",
        "This class uses an LLM to evaluate answer quality on 5 dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LLMJudge class defined\n"
          ]
        }
      ],
      "source": [
        "class LLMJudge:\n",
        "    \"\"\"Use LLM as a judge to evaluate answer quality.\"\"\"\n",
        "    \n",
        "    def __init__(self, judge_model: str = \"gpt-4o\", api_key: Optional[str] = None):\n",
        "        \"\"\"Initialize the LLM judge.\"\"\"\n",
        "        self.judge_model = judge_model\n",
        "        \n",
        "        # Check if judge model is from OpenRouter (has provider prefix like anthropic/, google/, etc.)\n",
        "        use_openrouter = '/' in judge_model\n",
        "        \n",
        "        if use_openrouter:\n",
        "            # Use OpenRouter for judge model\n",
        "            openrouter_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n",
        "            if not openrouter_key:\n",
        "                raise ValueError(\"OpenRouter API key required for judge model. Set OPENROUTER_API_KEY environment variable.\")\n",
        "            self.client = OpenAI(\n",
        "                api_key=openrouter_key,\n",
        "                base_url=\"https://openrouter.ai/api/v1\",\n",
        "                default_headers={\n",
        "                    \"HTTP-Referer\": \"http://localhost:8000\",\n",
        "                    \"X-Title\": \"PDF RAG Q&A System - Judge\"\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            # Use OpenAI API directly for judge model\n",
        "            openai_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "            if not openai_key:\n",
        "                raise ValueError(\"OpenAI API key required for LLM judge. Set OPENAI_API_KEY environment variable.\")\n",
        "            self.client = OpenAI(api_key=openai_key)\n",
        "    \n",
        "    def evaluate_answer(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrieved_docs: List[Dict],\n",
        "        model_answer: str\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate an answer using LLM as judge.\n",
        "        \n",
        "        Args:\n",
        "            query: The question/query\n",
        "            retrieved_docs: List of retrieved documents with 'text', 'pdf_filename', 'page'\n",
        "            model_answer: The answer generated by the model\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with scores and explanations\n",
        "        \"\"\"\n",
        "        # Format retrieved context\n",
        "        retrieved_context = \"\\n\\n\".join([\n",
        "            f\"[Document {i+1} - {doc.get('pdf_filename', 'Unknown')}, Page {doc.get('page', 'Unknown')}]:\\n{doc.get('text', '')}\"\n",
        "            for i, doc in enumerate(retrieved_docs)\n",
        "        ])\n",
        "        \n",
        "        # System prompt\n",
        "        system_prompt = \"\"\"You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
        "You must score the system's performance on the following dimensions:\n",
        "\n",
        "1. **Retrieval Relevance**: How relevant are the retrieved documents to the query?\n",
        "   - Score 1: Documents are completely irrelevant to the query\n",
        "   - Score 2: Documents are mostly irrelevant, with minimal connection to the query\n",
        "   - Score 3: Documents are somewhat relevant but miss key aspects of the query\n",
        "   - Score 4: Documents are highly relevant and cover most aspects of the query\n",
        "   - Score 5: Documents are perfectly relevant and comprehensively address the query\n",
        "\n",
        "2. **Faithfulness (Groundedness)**: Is the answer faithful to the retrieved context?\n",
        "   - Score 1: Answer contains significant information not in the retrieved documents or contradicts them\n",
        "   - Score 2: Answer includes some unsupported claims or minor contradictions\n",
        "   - Score 3: Answer is mostly faithful but includes some unsupported details\n",
        "   - Score 4: Answer is highly faithful with only minor unsupported additions\n",
        "   - Score 5: Answer is completely faithful and only uses information from retrieved documents\n",
        "\n",
        "3. **Answer Quality**: Overall quality of the answer\n",
        "   - Score 1: Answer is unclear, incomplete, or unhelpful\n",
        "   - Score 2: Answer addresses the query but is poorly structured or lacks clarity\n",
        "   - Score 3: Answer is adequate but could be improved in clarity or completeness\n",
        "   - Score 4: Answer is clear, well-structured, and mostly complete\n",
        "   - Score 5: Answer is excellent, clear, comprehensive, and well-structured\n",
        "\n",
        "Scores range from 1 to 5.\n",
        "You must provide a short explanation for each score.\"\"\"\n",
        "\n",
        "        # User prompt - emphasize JSON format\n",
        "        user_prompt = f\"\"\"Evaluate the following:\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Retrieved Context:\n",
        "{retrieved_context}\n",
        "\n",
        "Model Answer:\n",
        "{model_answer}\n",
        "\n",
        "IMPORTANT: You must respond ONLY with valid JSON. No additional text before or after.\n",
        "\n",
        "Return output in this exact JSON format:\n",
        "{{\n",
        "    \"relevance\": {{\"score\": X, \"explanation\": \"...\"}},\n",
        "    \"faithfulness\": {{\"score\": X, \"explanation\": \"...\"}},\n",
        "    \"answer_quality\": {{\"score\": X, \"explanation\": \"...\"}}\n",
        "}}\n",
        "\n",
        "Where X is a number from 1 to 5.\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Try without response_format first (some models don't support it)\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.judge_model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.3\n",
        "                )\n",
        "            except Exception as format_error:\n",
        "                # If that fails, try with response_format\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.judge_model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.3,\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "            \n",
        "            result_text = response.choices[0].message.content.strip() if response.choices[0].message.content else \"\"\n",
        "            \n",
        "            # Check if response is empty\n",
        "            if not result_text:\n",
        "                raise ValueError(f\"Judge model returned empty response. Model: {self.judge_model}\")\n",
        "            \n",
        "            # Try to parse JSON directly\n",
        "            try:\n",
        "                evaluation = json.loads(result_text)\n",
        "            except json.JSONDecodeError:\n",
        "                # If direct parsing fails, try to extract JSON from text\n",
        "                import re\n",
        "                json_match = re.search(r'\\{.*\\}', result_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    evaluation = json.loads(json_match.group(0))\n",
        "                else:\n",
        "                    raise ValueError(f\"Could not parse JSON from response. Response text (first 500 chars): {result_text[:500]}\")\n",
        "            \n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"scores\": {\n",
        "                    \"relevance\": evaluation.get(\"relevance\", {}).get(\"score\", 0),\n",
        "                    \"faithfulness\": evaluation.get(\"faithfulness\", {}).get(\"score\", 0),\n",
        "                    \"answer_quality\": evaluation.get(\"answer_quality\", {}).get(\"score\", 0)\n",
        "                },\n",
        "                \"explanations\": {\n",
        "                    \"relevance\": evaluation.get(\"relevance\", {}).get(\"explanation\", \"\"),\n",
        "                    \"faithfulness\": evaluation.get(\"faithfulness\", {}).get(\"explanation\", \"\"),\n",
        "                    \"answer_quality\": evaluation.get(\"answer_quality\", {}).get(\"explanation\", \"\")\n",
        "                },\n",
        "                \"judge_model\": self.judge_model\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "print(\"✓ LLMJudge class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_qa_dataset(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load QA dataset from JSON file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    elif isinstance(data, dict) and \"questions\" in data:\n",
        "        return data[\"questions\"]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset format. Expected list or dict with 'questions' key.\")\n",
        "\n",
        "\n",
        "def format_model_name_for_openrouter(model_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Format model name for OpenRouter API.\n",
        "    \n",
        "    OpenRouter requires format: provider/model-name\n",
        "    Examples:\n",
        "    - gpt-3.5-turbo -> openai/gpt-3.5-turbo\n",
        "    - gpt-4 -> openai/gpt-4\n",
        "    - claude-3.5-sonnet -> anthropic/claude-3-5-sonnet (note: hyphen, not dot)\n",
        "    - claude-3-sonnet -> anthropic/claude-3-sonnet\n",
        "    - llama-2 -> meta/llama-2\n",
        "    - gemini-pro -> google/gemini-pro\n",
        "    \"\"\"\n",
        "    # If already has provider prefix, check if it needs conversion\n",
        "    if '/' in model_name:\n",
        "        # Handle special cases for OpenRouter model IDs\n",
        "        # OpenRouter uses claude-3-5-sonnet (with hyphen) not claude-3.5-sonnet (with dot)\n",
        "        if 'claude-3.5' in model_name:\n",
        "            return model_name.replace('claude-3.5', 'claude-3-5')\n",
        "        else:\n",
        "            # Keep model name as-is if it already has provider prefix\n",
        "            return model_name\n",
        "    \n",
        "    # Map common model names to providers\n",
        "    if model_name.startswith('gpt'):\n",
        "        return f\"openai/{model_name}\"\n",
        "    elif model_name.startswith('claude'):\n",
        "        # OpenRouter uses claude-3-5-sonnet (hyphen) not claude-3.5-sonnet (dot)\n",
        "        formatted = model_name.replace('3.5', '3-5')\n",
        "        return f\"anthropic/{formatted}\"\n",
        "    elif model_name.startswith('llama'):\n",
        "        return f\"meta/{model_name}\"\n",
        "    elif model_name.startswith('gemini'):\n",
        "        return f\"google/{model_name}\"\n",
        "    else:\n",
        "        # Default: assume OpenAI format\n",
        "        return f\"openai/{model_name}\"\n",
        "\n",
        "\n",
        "def ask_question_with_model(question: str, model_name: str = None, use_openrouter: bool = True) -> Dict:\n",
        "    \"\"\"\n",
        "    Ask a question using the API.\n",
        "    \n",
        "    Args:\n",
        "        question: The question to ask\n",
        "        model_name: Model name (will be formatted for OpenRouter if needed)\n",
        "        use_openrouter: Whether to format model name for OpenRouter\n",
        "    \"\"\"\n",
        "    payload = {\"question\": question}\n",
        "    \n",
        "    if model_name:\n",
        "        # Format model name for OpenRouter if needed\n",
        "        if use_openrouter:\n",
        "            formatted_model = format_model_name_for_openrouter(model_name)\n",
        "        else:\n",
        "            formatted_model = model_name\n",
        "        payload[\"model_name\"] = formatted_model\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{API_BASE_URL}/api/ask\",\n",
        "            json=payload,\n",
        "            timeout=120\n",
        "        )\n",
        "        \n",
        "        response_time = time.time() - start_time\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"answer\": data.get(\"answer\", \"\"),\n",
        "                \"citations\": data.get(\"citations\", []),\n",
        "                \"retrieved_docs\": data.get(\"retrieved_docs\", []),  # Include retrieved documents\n",
        "                \"response_time\": response_time,\n",
        "                \"model_used\": data.get(\"model_used\", formatted_model if model_name else \"default\")\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": response.text,\n",
        "                \"response_time\": response_time\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"response_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "print(\"✓ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Dataset: qa_dataset.json\n",
            "  Models to test (via OpenRouter): ['claude-3.5-sonnet', 'gpt-3.5-turbo', 'google/gemini-2.5-flash']\n",
            "  Judge model: mistralai/mistral-large\n",
            "  Use OpenRouter: True\n",
            "\n",
            "⚠️  IMPORTANT: Make sure FORCE_OPENROUTER=true is set in .env file!\n",
            "   This ensures all model calls go through OpenRouter.\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATASET_PATH = \"qa_dataset.json\"  # Path to your QA dataset JSON file\n",
        "\n",
        "# Models to test via OpenRouter (3 models from different families)\n",
        "MODELS_TO_TEST = [\n",
        "    \"claude-3.5-sonnet\",   # Anthropic Claude 3.5 Sonnet\n",
        "    \"gpt-3.5-turbo\",       # OpenAI GPT-3.5 Turbo\n",
        "    \"google/gemini-2.5-flash\",    # Google Gemini 2.5 Flash\n",
        "]\n",
        "\n",
        "# Judge model: Must be from a different family than tested models\n",
        "# Since we're testing Claude, GPT, and Gemini, we need a model from another family\n",
        "# Using Mistral (different from Anthropic, OpenAI, and Google)\n",
        "# Note: llama-guard is a safety model, not suitable for evaluation tasks\n",
        "JUDGE_MODEL = \"mistralai/mistral-large\"  # Model to use as judge (different family: Mistral)\n",
        "OUTPUT_FILE = None  # None = auto-generate filename with timestamp\n",
        "USE_OPENROUTER = True  # Whether to use OpenRouter for testing models\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Dataset: {DATASET_PATH}\")\n",
        "print(f\"  Models to test (via OpenRouter): {MODELS_TO_TEST}\")\n",
        "print(f\"  Judge model: {JUDGE_MODEL}\")\n",
        "print(f\"  Use OpenRouter: {USE_OPENROUTER}\")\n",
        "print(\"\\n⚠️  IMPORTANT: Make sure FORCE_OPENROUTER=true is set in .env file!\")\n",
        "print(\"   This ensures all model calls go through OpenRouter.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load QA Dataset\n",
        "\n",
        "Load your questions and reference answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading QA dataset from: qa_dataset.json\n",
            "✓ Loaded 50 questions\n",
            "\n",
            "Example question:\n",
            "  Question: What are the main topics covered in the Machine Learning Interview Cheat Sheet?\n",
            "  Has reference answer: True\n",
            "  Has context: True\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "print(f\"Loading QA dataset from: {DATASET_PATH}\")\n",
        "qa_items = load_qa_dataset(DATASET_PATH)\n",
        "print(f\"✓ Loaded {len(qa_items)} questions\")\n",
        "\n",
        "# Show first question as example\n",
        "if qa_items:\n",
        "    print(\"\\nExample question:\")\n",
        "    print(f\"  Question: {qa_items[0].get('question', 'N/A')}\")\n",
        "    print(f\"  Has reference answer: {'answer' in qa_items[0] or 'reference_answer' in qa_items[0]}\")\n",
        "    print(f\"  Has context: {'context' in qa_items[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Judge\n",
        "\n",
        "Initialize the LLM judge that will evaluate all answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing LLM judge: mistralai/mistral-large\n",
            "✓ Judge initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize judge\n",
        "print(f\"Initializing LLM judge: {JUDGE_MODEL}\")\n",
        "judge = LLMJudge(judge_model=JUDGE_MODEL)\n",
        "print(\"✓ Judge initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n",
        "\n",
        "This will test each model (via OpenRouter) on each question and get judge evaluations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LLM-as-a-Judge Evaluation (via OpenRouter)\n",
            "============================================================\n",
            "Dataset: qa_dataset.json\n",
            "Questions: 50\n",
            "Models to test: ['claude-3.5-sonnet', 'gpt-3.5-turbo', 'google/gemini-2.5-flash']\n",
            "Judge model: mistralai/mistral-large\n",
            "Output: llm_judge_results/llm_judge_evaluation_20251205_143859.json\n",
            "============================================================\n",
            "\n",
            "\n",
            "Question 1/50: What are the main topics covered in the Machine Learning Int...\n",
            "  [1/150] Testing claude-3.5-sonnet... ✓ Got answer (6.10s) Judging... ✓ Avg Score: 3.00/5\n",
            "  [2/150] Testing gpt-3.5-turbo... ✓ Got answer (1.78s) Judging... ✓ Avg Score: 2.33/5\n",
            "  [3/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.26s) Judging... ✓ Avg Score: 3.00/5\n",
            "\n",
            "Question 2/50: What are some key concepts related to machine learning menti...\n",
            "  [4/150] Testing claude-3.5-sonnet... ✓ Got answer (12.46s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [5/150] Testing gpt-3.5-turbo... ✓ Got answer (5.81s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [6/150] Testing google/gemini-2.5-flash... ✓ Got answer (5.67s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 3/50: What are the procedures described for Feature Selection?...\n",
            "  [7/150] Testing claude-3.5-sonnet... ✓ Got answer (11.35s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [8/150] Testing gpt-3.5-turbo... ✓ Got answer (4.55s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [9/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.33s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 4/50: How do intrinsic feature selection methods function?...\n",
            "  [10/150] Testing claude-3.5-sonnet... ✓ Got answer (7.47s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [11/150] Testing gpt-3.5-turbo... ✓ Got answer (1.63s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [12/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.56s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 5/50: What are the pros of using intrinsic feature selection metho...\n",
            "  [13/150] Testing claude-3.5-sonnet... ✓ Got answer (5.96s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [14/150] Testing gpt-3.5-turbo... ✓ Got answer (1.74s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [15/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.00s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 6/50: What are the cons of intrinsic feature selection methods?...\n",
            "  [16/150] Testing claude-3.5-sonnet... ✓ Got answer (4.74s) Judging... ✓ Avg Score: 2.33/5\n",
            "  [17/150] Testing gpt-3.5-turbo... ✓ Got answer (2.19s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [18/150] Testing google/gemini-2.5-flash... ✓ Got answer (0.79s) Judging... ✓ Avg Score: 3.33/5\n",
            "\n",
            "Question 7/50: How is feature selection embedded in tree-based models?...\n",
            "  [19/150] Testing claude-3.5-sonnet... ✓ Got answer (8.85s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [20/150] Testing gpt-3.5-turbo... ✓ Got answer (3.67s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [21/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.21s) Judging... ✓ Avg Score: 2.67/5\n",
            "\n",
            "Question 8/50: What is the relationship between feature selection and the o...\n",
            "  [22/150] Testing claude-3.5-sonnet... ✓ Got answer (8.14s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [23/150] Testing gpt-3.5-turbo... ✓ Got answer (2.52s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [24/150] Testing google/gemini-2.5-flash... ✓ Got answer (0.86s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 9/50: Which models have intrinsic feature selection embedded in th...\n",
            "  [25/150] Testing claude-3.5-sonnet... ✓ Got answer (7.73s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [26/150] Testing gpt-3.5-turbo... ✓ Got answer (3.02s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [27/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.18s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 10/50: What are the pros and cons of linear models in machine learn...\n",
            "  [28/150] Testing claude-3.5-sonnet... ✓ Got answer (7.85s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [29/150] Testing gpt-3.5-turbo... ✓ Got answer (3.79s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [30/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.44s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 11/50: How do tree-based models compare to linear models?...\n",
            "  [31/150] Testing claude-3.5-sonnet... ✓ Got answer (12.86s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [32/150] Testing gpt-3.5-turbo... ✓ Got answer (4.95s) Judging... ✓ Avg Score: 3.00/5\n",
            "  [33/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.03s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 12/50: What is the difference between Random Forest and Gradient Bo...\n",
            "  [34/150] Testing claude-3.5-sonnet... ✓ Got answer (12.22s) Judging... ✓ Avg Score: 5.00/5\n",
            "  [35/150] Testing gpt-3.5-turbo... ✓ Got answer (4.47s) Judging... ✓ Avg Score: 5.00/5\n",
            "  [36/150] Testing google/gemini-2.5-flash... ✓ Got answer (3.33s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 13/50: What are the evaluation metrics discussed in the PDFs?...\n",
            "  [37/150] Testing claude-3.5-sonnet... ✓ Got answer (12.36s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [38/150] Testing gpt-3.5-turbo... ✓ Got answer (3.41s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [39/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.68s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 14/50: How are clustering models different from tree-based models?...\n",
            "  [40/150] Testing claude-3.5-sonnet... ✓ Got answer (11.70s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [41/150] Testing gpt-3.5-turbo... ✓ Got answer (5.17s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [42/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.01s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 15/50: What are the key differences between a Decision Tree and a R...\n",
            "  [43/150] Testing claude-3.5-sonnet... ✓ Got answer (9.96s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [44/150] Testing gpt-3.5-turbo... ✓ Got answer (5.29s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [45/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.13s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 16/50: How do loss functions relate to machine learning models?...\n",
            "  [46/150] Testing claude-3.5-sonnet... ✓ Got answer (14.10s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [47/150] Testing gpt-3.5-turbo... ✓ Got answer (5.32s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [48/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.37s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 17/50: What is the role of missing values in data and feature engin...\n",
            "  [49/150] Testing claude-3.5-sonnet... ✓ Got answer (7.57s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [50/150] Testing gpt-3.5-turbo... ✓ Got answer (2.70s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [51/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.64s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 18/50: What challenges are associated with imbalanced data?...\n",
            "  [52/150] Testing claude-3.5-sonnet... ✓ Got answer (12.48s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [53/150] Testing gpt-3.5-turbo... ✓ Got answer (2.37s) Judging... ✓ Avg Score: 3.00/5\n",
            "  [54/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.79s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 19/50: In what ways can feature selection enhance model performance...\n",
            "  [55/150] Testing claude-3.5-sonnet... ✓ Got answer (13.87s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [56/150] Testing gpt-3.5-turbo... ✓ Got answer (3.31s) Judging... ✓ Avg Score: 3.00/5\n",
            "  [57/150] Testing google/gemini-2.5-flash... ✓ Got answer (0.99s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 20/50: Summarize the main differences between SVMs and Logistic Reg...\n",
            "  [58/150] Testing claude-3.5-sonnet... ✓ Got answer (12.25s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [59/150] Testing gpt-3.5-turbo... ✓ Got answer (3.18s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [60/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.48s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 21/50: What are the advantages of using Logistic Regression over Li...\n",
            "  [61/150] Testing claude-3.5-sonnet... ✓ Got answer (6.14s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [62/150] Testing gpt-3.5-turbo... ✓ Got answer (2.98s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [63/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.04s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 22/50: Why might one choose Random Forest over Logistic Regression?...\n",
            "  [64/150] Testing claude-3.5-sonnet... ✓ Got answer (12.36s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [65/150] Testing gpt-3.5-turbo... ✓ Got answer (3.50s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [66/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.87s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 23/50: What are the main considerations in model comparison?...\n",
            "  [67/150] Testing claude-3.5-sonnet... ✓ Got answer (11.46s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [68/150] Testing gpt-3.5-turbo... ✓ Got answer (3.15s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [69/150] Testing google/gemini-2.5-flash... ✓ Got answer (4.36s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 24/50: How does feature selection relate to the objective function ...\n",
            "  [70/150] Testing claude-3.5-sonnet... ✓ Got answer (9.58s) Judging... ✓ Avg Score: 5.00/5\n",
            "  [71/150] Testing gpt-3.5-turbo... ✓ Got answer (2.52s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [72/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.23s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 25/50: What steps are involved in evaluating machine learning model...\n",
            "  [73/150] Testing claude-3.5-sonnet... ✓ Got answer (14.83s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [74/150] Testing gpt-3.5-turbo... ✓ Got answer (4.85s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [75/150] Testing google/gemini-2.5-flash... ✓ Got answer (3.95s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 26/50: What are some examples of intrinsic feature selection method...\n",
            "  [76/150] Testing claude-3.5-sonnet... ✓ Got answer (7.99s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [77/150] Testing gpt-3.5-turbo... ✓ Got answer (3.28s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [78/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.32s) Judging... ✓ Avg Score: 3.67/5\n",
            "\n",
            "Question 27/50: How does imbalanced data affect machine learning model train...\n",
            "  [79/150] Testing claude-3.5-sonnet... ✓ Got answer (12.70s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [80/150] Testing gpt-3.5-turbo... ✓ Got answer (3.57s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [81/150] Testing google/gemini-2.5-flash... ✓ Got answer (3.32s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 28/50: What are some methods for handling missing values in dataset...\n",
            "  [82/150] Testing claude-3.5-sonnet... ✓ Got answer (8.20s) Judging... ✓ Avg Score: 3.00/5\n",
            "  [83/150] Testing gpt-3.5-turbo... ✓ Got answer (2.61s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [84/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.57s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 29/50: How does Gradient Boosting differ from Random Forest in term...\n",
            "  [85/150] Testing claude-3.5-sonnet... ✓ Got answer (12.54s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [86/150] Testing gpt-3.5-turbo... ✓ Got answer (3.91s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [87/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.11s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 30/50: What should be considered when selecting features for a mach...\n",
            "  [88/150] Testing claude-3.5-sonnet... ✓ Got answer (9.62s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [89/150] Testing gpt-3.5-turbo... ✓ Got answer (2.89s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [90/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.48s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 31/50: How do linear models perform compared to tree-based models?...\n",
            "  [91/150] Testing claude-3.5-sonnet... ✓ Got answer (11.03s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [92/150] Testing gpt-3.5-turbo... ✓ Got answer (5.82s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [93/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.32s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 32/50: What are the benefits of using a Decision Tree over Random F...\n",
            "  [94/150] Testing claude-3.5-sonnet... ✓ Got answer (7.70s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [95/150] Testing gpt-3.5-turbo... ✓ Got answer (3.37s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [96/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.27s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 33/50: Summarize the pros and cons of using clustering models....\n",
            "  [97/150] Testing claude-3.5-sonnet... ✓ Got answer (10.24s) Judging... ✓ Avg Score: 2.33/5\n",
            "  [98/150] Testing gpt-3.5-turbo... ✓ Got answer (3.19s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [99/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.38s) Judging... ✓ Avg Score: 2.00/5\n",
            "\n",
            "Question 34/50: What are some common feature selection techniques in machine...\n",
            "  [100/150] Testing claude-3.5-sonnet... ✓ Got answer (8.14s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [101/150] Testing gpt-3.5-turbo... ✓ Got answer (2.60s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [102/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.53s) Judging... ✓ Avg Score: 3.67/5\n",
            "\n",
            "Question 35/50: How do loss functions influence the training of machine lear...\n",
            "  [103/150] Testing claude-3.5-sonnet... ✓ Got answer (12.56s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [104/150] Testing gpt-3.5-turbo... ✓ Got answer (4.48s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [105/150] Testing google/gemini-2.5-flash... ✓ Got answer (3.67s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 36/50: What are the differences between Linear Regression and Logis...\n",
            "  [106/150] Testing claude-3.5-sonnet... ✓ Got answer (12.75s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [107/150] Testing gpt-3.5-turbo... ✓ Got answer (4.01s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [108/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.53s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 37/50: In what scenarios would you use Logistic Regression over SVM...\n",
            "  [109/150] Testing claude-3.5-sonnet... ✓ Got answer (14.33s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [110/150] Testing gpt-3.5-turbo... ✓ Got answer (3.77s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [111/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.20s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 38/50: What are some challenges associated with data and feature en...\n",
            "  [112/150] Testing claude-3.5-sonnet... ✓ Got answer (9.79s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [113/150] Testing gpt-3.5-turbo... ✓ Got answer (2.68s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [114/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.11s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 39/50: How does model comparison aid in selecting the best machine ...\n",
            "  [115/150] Testing claude-3.5-sonnet... ✓ Got answer (15.13s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [116/150] Testing gpt-3.5-turbo... ✓ Got answer (6.22s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [117/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.40s) Judging... ✓ Avg Score: 3.67/5\n",
            "\n",
            "Question 40/50: What are the pros and cons of tree-based models?...\n",
            "  [118/150] Testing claude-3.5-sonnet... ✓ Got answer (9.36s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [119/150] Testing gpt-3.5-turbo... ✓ Got answer (2.61s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [120/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.46s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 41/50: What are the implications of using Linear Models in machine ...\n",
            "  [121/150] Testing claude-3.5-sonnet... ✓ Got answer (7.98s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [122/150] Testing gpt-3.5-turbo... ✓ Got answer (2.85s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [123/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.41s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 42/50: How does feature selection improve the efficiency of machine...\n",
            "  [124/150] Testing claude-3.5-sonnet... ✓ Got answer (14.59s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [125/150] Testing gpt-3.5-turbo... ✓ Got answer (4.50s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [126/150] Testing google/gemini-2.5-flash... ✓ Got answer (0.99s) Judging... ✓ Avg Score: 3.67/5\n",
            "\n",
            "Question 43/50: What are the main advantages of using Random Forest for clas...\n",
            "  [127/150] Testing claude-3.5-sonnet... ✓ Got answer (8.80s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [128/150] Testing gpt-3.5-turbo... ✓ Got answer (1.87s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [129/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.08s) Judging... ✓ Avg Score: 3.67/5\n",
            "\n",
            "Question 44/50: How do evaluation metrics impact the assessment of model per...\n",
            "  [130/150] Testing claude-3.5-sonnet... ✓ Got answer (17.50s) Judging... ✓ Avg Score: 4.67/5\n",
            "  [131/150] Testing gpt-3.5-turbo... ✓ Got answer (5.39s) Judging... ✓ Avg Score: 3.67/5\n",
            "  [132/150] Testing google/gemini-2.5-flash... ✓ Got answer (3.80s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 45/50: What steps are involved in the process of feature selection?...\n",
            "  [133/150] Testing claude-3.5-sonnet... ✓ Got answer (14.51s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [134/150] Testing gpt-3.5-turbo... ✓ Got answer (3.88s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [135/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.49s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "Question 46/50: How does one compare SVMs and Logistic Regression for a give...\n",
            "  [136/150] Testing claude-3.5-sonnet... ✓ Got answer (14.28s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [137/150] Testing gpt-3.5-turbo... ✓ Got answer (4.23s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [138/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.85s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 47/50: What factors should be considered in model comparison?...\n",
            "  [139/150] Testing claude-3.5-sonnet... ✓ Got answer (14.11s) Judging... ✓ Avg Score: 2.67/5\n",
            "  [140/150] Testing gpt-3.5-turbo... ✓ Got answer (4.30s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [141/150] Testing google/gemini-2.5-flash... ✓ Got answer (4.71s) Judging... ✓ Avg Score: 5.00/5\n",
            "\n",
            "Question 48/50: What are the main differences between Decision Tree and Rand...\n",
            "  [142/150] Testing claude-3.5-sonnet... ✓ Got answer (11.76s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [143/150] Testing gpt-3.5-turbo... ✓ Got answer (4.03s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [144/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.98s) Judging... ✓ Avg Score: 4.67/5\n",
            "\n",
            "Question 49/50: What is the role of loss functions in machine learning?...\n",
            "  [145/150] Testing claude-3.5-sonnet... ✓ Got answer (8.82s) Judging... ✓ Avg Score: 3.33/5\n",
            "  [146/150] Testing gpt-3.5-turbo... ✓ Got answer (3.39s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [147/150] Testing google/gemini-2.5-flash... ✓ Got answer (1.68s) Judging... ✓ Avg Score: 4.00/5\n",
            "\n",
            "Question 50/50: Summarize the process and benefits of feature selection in m...\n",
            "  [148/150] Testing claude-3.5-sonnet... ✓ Got answer (13.05s) Judging... ✓ Avg Score: 4.00/5\n",
            "  [149/150] Testing gpt-3.5-turbo... ✓ Got answer (4.63s) Judging... ✓ Avg Score: 4.33/5\n",
            "  [150/150] Testing google/gemini-2.5-flash... ✓ Got answer (2.74s) Judging... ✓ Avg Score: 4.33/5\n",
            "\n",
            "============================================================\n",
            "Evaluation Complete!\n",
            "Results saved to: llm_judge_results/llm_judge_evaluation_20251205_143859.json\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup output\n",
        "if not OUTPUT_FILE:\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    OUTPUT_FILE = f\"llm_judge_evaluation_{timestamp}.json\"\n",
        "\n",
        "output_path = EVALUATION_OUTPUT_DIR / OUTPUT_FILE\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"LLM-as-a-Judge Evaluation (via OpenRouter)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Dataset: {DATASET_PATH}\")\n",
        "print(f\"Questions: {len(qa_items)}\")\n",
        "print(f\"Models to test: {MODELS_TO_TEST}\")\n",
        "print(f\"Judge model: {JUDGE_MODEL}\")\n",
        "print(f\"Output: {output_path}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"dataset_path\": DATASET_PATH,\n",
        "        \"total_questions\": len(qa_items),\n",
        "        \"models_tested\": MODELS_TO_TEST,\n",
        "        \"judge_model\": JUDGE_MODEL,\n",
        "        \"api_base_url\": API_BASE_URL,\n",
        "        \"use_openrouter\": USE_OPENROUTER\n",
        "    },\n",
        "    \"evaluations\": []\n",
        "}\n",
        "\n",
        "models_to_test = MODELS_TO_TEST if MODELS_TO_TEST else [None]\n",
        "total_tests = len(qa_items) * len(models_to_test)\n",
        "current_test = 0\n",
        "\n",
        "for qa_item in qa_items:\n",
        "    question = qa_item.get(\"question\", \"\")\n",
        "    reference_answer = qa_item.get(\"answer\") or qa_item.get(\"reference_answer\")\n",
        "    context = qa_item.get(\"context\", \"\")\n",
        "    qa_id = qa_item.get(\"id\", len(results[\"evaluations\"]) + 1)\n",
        "    \n",
        "    if not question:\n",
        "        print(f\"⚠ Skipping item {qa_id}: No question found\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nQuestion {qa_id}/{len(qa_items)}: {question[:60]}...\")\n",
        "    \n",
        "    for model in models_to_test:\n",
        "        current_test += 1\n",
        "        model_display = model or \"default\"\n",
        "        print(f\"  [{current_test}/{total_tests}] Testing {model_display}...\", end=\" \", flush=True)\n",
        "        \n",
        "        # Get answer from model (via OpenRouter)\n",
        "        answer_result = ask_question_with_model(question, model, use_openrouter=USE_OPENROUTER)\n",
        "        \n",
        "        if not answer_result[\"success\"]:\n",
        "            print(f\"✗ Error: {answer_result.get('error', 'Unknown')[:50]}\")\n",
        "            evaluation = {\n",
        "                \"qa_id\": qa_id,\n",
        "                \"question\": question,\n",
        "                \"model\": model_display,\n",
        "                \"success\": False,\n",
        "                \"error\": answer_result.get(\"error\", \"Unknown error\")\n",
        "            }\n",
        "            results[\"evaluations\"].append(evaluation)\n",
        "            continue\n",
        "        \n",
        "        answer = answer_result[\"answer\"]\n",
        "        retrieved_docs = answer_result.get(\"retrieved_docs\", [])\n",
        "        print(f\"✓ Got answer ({answer_result.get('response_time', 0):.2f}s)\", end=\" \", flush=True)\n",
        "        \n",
        "        # Use context from QA dataset for Retrieval Relevance evaluation\n",
        "        # Convert context string to retrieved_docs format if context exists\n",
        "        context_for_evaluation = retrieved_docs\n",
        "        if context and context.strip():\n",
        "            # Convert context string to retrieved_docs format for evaluation\n",
        "            # This represents the \"ground truth\" context that should have been retrieved\n",
        "            context_for_evaluation = [{\n",
        "                \"text\": context,\n",
        "                \"pdf_filename\": \"Reference Context\",\n",
        "                \"page\": \"N/A\"\n",
        "            }]\n",
        "        \n",
        "        # Judge the answer\n",
        "        print(\"Judging...\", end=\" \", flush=True)\n",
        "        judge_result = judge.evaluate_answer(\n",
        "            query=question,\n",
        "            retrieved_docs=context_for_evaluation,\n",
        "            model_answer=answer\n",
        "        )\n",
        "        \n",
        "        if judge_result[\"success\"]:\n",
        "            # Calculate average score\n",
        "            scores = judge_result[\"scores\"]\n",
        "            avg_score = (scores[\"relevance\"] + scores[\"faithfulness\"] + scores[\"answer_quality\"]) / 3\n",
        "            print(f\"✓ Avg Score: {avg_score:.2f}/5\")\n",
        "        else:\n",
        "            print(f\"✗ Judge error: {judge_result.get('error', 'Unknown')[:50]}\")\n",
        "        \n",
        "        # Build evaluation record\n",
        "        evaluation = {\n",
        "            \"qa_id\": qa_id,\n",
        "            \"question\": question,\n",
        "            \"model\": model_display,\n",
        "            \"model_used\": answer_result.get(\"model_used\", model_display),\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"answer\": answer,\n",
        "            \"retrieved_docs\": retrieved_docs,\n",
        "            \"citations\": answer_result.get(\"citations\", []),\n",
        "            \"response_time\": answer_result.get(\"response_time\", 0),\n",
        "            \"num_citations\": len(answer_result.get(\"citations\", [])),\n",
        "            \"num_retrieved_docs\": len(retrieved_docs),\n",
        "            \"answer_length\": len(answer),\n",
        "            \"judge_evaluation\": judge_result if judge_result[\"success\"] else None,\n",
        "            \"judge_error\": judge_result.get(\"error\") if not judge_result[\"success\"] else None\n",
        "        }\n",
        "        \n",
        "        results[\"evaluations\"].append(evaluation)\n",
        "        \n",
        "        # Save progress after each question\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluation Complete!\")\n",
        "print(f\"Results saved to: {output_path}\")\n",
        "print(f\"{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Summary calculated and saved\n"
          ]
        }
      ],
      "source": [
        "# Calculate summary statistics\n",
        "summary = {}\n",
        "\n",
        "for eval_item in results[\"evaluations\"]:\n",
        "    if not eval_item.get(\"success\", True) or not eval_item.get(\"judge_evaluation\"):\n",
        "        continue\n",
        "    \n",
        "    model = eval_item[\"model\"]\n",
        "    if model not in summary:\n",
        "        summary[model] = {\n",
        "            \"total_questions\": 0,\n",
        "            \"successful_evaluations\": 0,\n",
        "            \"avg_scores\": {\n",
        "                \"relevance\": 0,\n",
        "                \"faithfulness\": 0,\n",
        "                \"answer_quality\": 0,\n",
        "                \"overall_score\": 0\n",
        "            },\n",
        "            \"total_response_time\": 0,\n",
        "            \"avg_response_time\": 0,\n",
        "            \"total_citations\": 0,\n",
        "            \"avg_citations\": 0,\n",
        "            \"total_retrieved_docs\": 0,\n",
        "            \"avg_retrieved_docs\": 0\n",
        "        }\n",
        "    \n",
        "    stats = summary[model]\n",
        "    stats[\"total_questions\"] += 1\n",
        "    \n",
        "    judge_eval = eval_item.get(\"judge_evaluation\", {})\n",
        "    if judge_eval and judge_eval.get(\"success\"):\n",
        "        stats[\"successful_evaluations\"] += 1\n",
        "        scores = judge_eval.get(\"scores\", {})\n",
        "        \n",
        "        # Calculate overall score as average of three dimensions\n",
        "        overall = (scores.get(\"relevance\", 0) + scores.get(\"faithfulness\", 0) + scores.get(\"answer_quality\", 0)) / 3\n",
        "        \n",
        "        stats[\"avg_scores\"][\"relevance\"] += scores.get(\"relevance\", 0)\n",
        "        stats[\"avg_scores\"][\"faithfulness\"] += scores.get(\"faithfulness\", 0)\n",
        "        stats[\"avg_scores\"][\"answer_quality\"] += scores.get(\"answer_quality\", 0)\n",
        "        stats[\"avg_scores\"][\"overall_score\"] += overall\n",
        "        \n",
        "        stats[\"total_response_time\"] += eval_item.get(\"response_time\", 0)\n",
        "        stats[\"total_citations\"] += eval_item.get(\"num_citations\", 0)\n",
        "        stats[\"total_retrieved_docs\"] += eval_item.get(\"num_retrieved_docs\", 0)\n",
        "\n",
        "# Calculate averages\n",
        "for model, stats in summary.items():\n",
        "    if stats[\"successful_evaluations\"] > 0:\n",
        "        for score_type in stats[\"avg_scores\"]:\n",
        "            stats[\"avg_scores\"][score_type] /= stats[\"successful_evaluations\"]\n",
        "        stats[\"avg_response_time\"] = stats[\"total_response_time\"] / stats[\"successful_evaluations\"]\n",
        "        stats[\"avg_citations\"] = stats[\"total_citations\"] / stats[\"successful_evaluations\"]\n",
        "        stats[\"avg_retrieved_docs\"] = stats[\"total_retrieved_docs\"] / stats[\"successful_evaluations\"]\n",
        "\n",
        "results[\"summary\"] = summary\n",
        "\n",
        "# Save final results\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✓ Summary calculated and saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Summary Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SUMMARY - Model Comparison\n",
            "============================================================\n",
            "\n",
            "Model: claude-3.5-sonnet\n",
            "  Total Questions: 50\n",
            "  Successful Evaluations: 50\n",
            "  Average Scores (1-5 scale):\n",
            "    Retrieval Relevance: 3.82/5\n",
            "    Faithfulness: 3.96/5\n",
            "    Answer Quality: 4.12/5\n",
            "    Overall Score: 3.97/5\n",
            "  Avg Response Time: 10.84s\n",
            "  Avg Citations: 2.98\n",
            "  Avg Retrieved Docs: 29.18\n",
            "\n",
            "Model: gpt-3.5-turbo\n",
            "  Total Questions: 50\n",
            "  Successful Evaluations: 50\n",
            "  Average Scores (1-5 scale):\n",
            "    Retrieval Relevance: 3.68/5\n",
            "    Faithfulness: 3.58/5\n",
            "    Answer Quality: 3.70/5\n",
            "    Overall Score: 3.65/5\n",
            "  Avg Response Time: 3.64s\n",
            "  Avg Citations: 3.56\n",
            "  Avg Retrieved Docs: 29.18\n",
            "\n",
            "Model: google/gemini-2.5-flash\n",
            "  Total Questions: 50\n",
            "  Successful Evaluations: 50\n",
            "  Average Scores (1-5 scale):\n",
            "    Retrieval Relevance: 3.94/5\n",
            "    Faithfulness: 4.20/5\n",
            "    Answer Quality: 4.32/5\n",
            "    Overall Score: 4.15/5\n",
            "  Avg Response Time: 2.17s\n",
            "  Avg Citations: 3.50\n",
            "  Avg Retrieved Docs: 29.18\n",
            "\n",
            "✓ Full results saved to: llm_judge_results/llm_judge_evaluation_20251205_143859.json\n",
            "\n",
            "You can now analyze the results to compare different models' performance.\n"
          ]
        }
      ],
      "source": [
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY - Model Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model, stats in summary.items():\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    print(f\"  Total Questions: {stats['total_questions']}\")\n",
        "    print(f\"  Successful Evaluations: {stats['successful_evaluations']}\")\n",
        "    print(f\"  Average Scores (1-5 scale):\")\n",
        "    print(f\"    Retrieval Relevance: {stats['avg_scores']['relevance']:.2f}/5\")\n",
        "    print(f\"    Faithfulness: {stats['avg_scores']['faithfulness']:.2f}/5\")\n",
        "    print(f\"    Answer Quality: {stats['avg_scores']['answer_quality']:.2f}/5\")\n",
        "    print(f\"    Overall Score: {stats['avg_scores']['overall_score']:.2f}/5\")\n",
        "    print(f\"  Avg Response Time: {stats['avg_response_time']:.2f}s\")\n",
        "    print(f\"  Avg Citations: {stats['avg_citations']:.2f}\")\n",
        "    print(f\"  Avg Retrieved Docs: {stats['avg_retrieved_docs']:.2f}\")\n",
        "\n",
        "print(f\"\\n✓ Full results saved to: {output_path}\")\n",
        "print(\"\\nYou can now analyze the results to compare different models' performance.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
