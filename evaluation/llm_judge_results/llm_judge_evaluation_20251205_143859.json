{
  "metadata": {
    "timestamp": "2025-12-05 14:38:59",
    "dataset_path": "qa_dataset.json",
    "total_questions": 50,
    "models_tested": [
      "claude-3.5-sonnet",
      "gpt-3.5-turbo",
      "google/gemini-2.5-flash"
    ],
    "judge_model": "mistralai/mistral-large",
    "api_base_url": "http://localhost:8000",
    "use_openrouter": true
  },
  "evaluations": [
    {
      "qa_id": 1,
      "question": "What are the main topics covered in the Machine Learning Interview Cheat Sheet?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:39:11",
      "answer": "Based on the provided context, the Machine Learning Interview Cheat Sheet covers the following main topics:\n\n**Main Topics:**\n1. **Data and Feature Engineering**\n2. **Pros and Cons of Machine Learning Models**\n3. **Model Comparison**\n4. **Loss Functions**\n5. **Model Evaluation Metrics**, including:\n   - **Accuracy**: Percentage of predicted labels matching true labels\n   - **Precision**: Percentage of accurate positives claimed vs total positives claimed\n   - **Recall**: Percentage of claimed positives vs actual positives\n   - **Specificity**: Percentage of claimed negatives vs actual negatives\n\nThe cheat sheet also includes specific subtopics such as handling **Missing Values** through:\n- Gathering more data\n- Imputation using existing data\n\n[Source 1] provides the main structure of the topics, while [Source 3] details the evaluation metrics, and [Source 2] covers the data and feature engineering aspects, specifically dealing with missing values.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 6.097560167312622,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 961,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document snippet is very brief and does not explicitly list the main topics covered in the cheat sheet. While the title suggests relevance, the content does not provide enough detail to confirm coverage of the topics mentioned in the model answer. Thus, the documents are somewhat relevant but miss key aspects of the query.",
          "faithfulness": "The model answer includes specific topics (e.g., Data and Feature Engineering, Loss Functions, Model Evaluation Metrics) and details (e.g., handling Missing Values) that are not explicitly present in the retrieved context. While the answer may be plausible, it introduces unsupported details, making it only mostly faithful to the provided context.",
          "answer_quality": "The answer is well-structured and addresses the query, but it includes information that is not clearly supported by the retrieved context. It is adequate but could be improved by sticking strictly to the provided context or clarifying which parts are inferred."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 1,
      "question": "What are the main topics covered in the Machine Learning Interview Cheat Sheet?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:39:20",
      "answer": "**Main topics covered in the Machine Learning Interview Cheat Sheet** include:\n\n- **Data and feature engineering**\n- **Pros and cons of machine learning models**\n- **Model comparison**\n- **Loss functions**\n- **Missing Values**\n- **Accuracy**\n- **Precision**\n- **Recall**\n- **Specificity**\n\nThese topics provide a comprehensive overview of concepts, algorithms, and techniques relevant for Machine Learning interviews as outlined in the provided PDF documents [Source 1], [Source 2], and [Source 3].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 1.7841429710388184,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 498,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is minimally relevant to the query. It only provides the title of the cheat sheet and some metadata but does not list or describe the main topics covered. Thus, it misses key aspects of the query.",
          "faithfulness": "The answer includes topics (e.g., data and feature engineering, pros and cons of models, etc.) that are not explicitly supported by the retrieved context. The context does not provide any details about the content of the cheat sheet, so the answer relies on unsupported claims.",
          "answer_quality": "The answer is structured and lists potential topics that might be relevant to a machine learning interview cheat sheet. However, it lacks clarity and completeness because it is not grounded in the provided context. The answer could be improved by acknowledging the lack of specific details in the retrieved documents."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 1,
      "question": "What are the main topics covered in the Machine Learning Interview Cheat Sheet?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:39:29",
      "answer": "The Machine Learning Interview Cheat Sheet covers various concepts, algorithms, and techniques [Source 1]. Specifically, it addresses the following question types in Machine Learning interviews:\n\n*   **Data and feature engineering** [Source 1]\n*   **Pros and cons of machine learning models** [Source 1]\n*   **Model comparison** [Source 1]\n*   **Loss functions** [Source 1]\n\nThe cheat sheet also discusses handling **missing values** by gathering more data or using imputation [Source 2]. Additionally, it defines metrics such as **Accuracy**, **Precision**, **Recall**, and **Specificity** [Source 3].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 1.2641000747680664,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 602,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is somewhat relevant as it confirms the cheat sheet covers 'various concepts, algorithms, and techniques,' but it does not provide specific details about the main topics. The explanation relies on inferred or external knowledge (e.g., 'question types in Machine Learning interviews'), which may not be directly supported by the retrieved context.",
          "faithfulness": "The answer is mostly faithful but includes some unsupported details. While it correctly cites the general scope of the cheat sheet, specific topics like 'Data and feature engineering,' 'Pros and cons of models,' and 'metrics' are not explicitly mentioned in the retrieved context. The references to [Source 2] and [Source 3] are also unclear since the provided context only includes one document.",
          "answer_quality": "The answer is adequate but could be improved. It provides a reasonable overview of potential topics but lacks specificity and clarity due to the vague retrieved context. The structure is logical, but the completeness is limited by the lack of detailed information in the retrieved document."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 2,
      "question": "What are some key concepts related to machine learning mentioned in the PDFs?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:39:48",
      "answer": "Based on the provided documents, here are the key concepts related to machine learning mentioned in the PDFs:\n\n## Main Categories of Machine Learning Concepts\n[Source 1] The documents cover several major areas:\n- **Data and feature engineering**\n- **Pros and cons of machine learning models**\n- **Model comparison**\n- **Loss functions**\n- **Evaluation metrics**\n\n## Specific Topics Include:\n\n### Data Handling and Preprocessing\n[Source 3, Source 4]\n- **Missing Values** handling through:\n  - Data gathering\n  - Imputation\n  - Model-based imputation\n  - Omission\n\n- **Feature Selection** approaches:\n  - Intrinsic feature selection\n  - Filter feature selection methods\n  - Selection based on correlation with target variables\n\n### Machine Learning Models\n[Source 5, Source 8]\n- **Tree-based Models**:\n  - Random Forest\n  - Gradient Boosting\n  - Decision Trees\n\n- **Clustering Models**:\n  - K-means\n  - Gaussian Mixture Model\n\n### Evaluation and Metrics\n[Source 6, Source 14]\n- **Classification Metrics**:\n  - Accuracy\n  - Precision\n  - Recall\n  - Specificity\n  - ROC Curve\n  - Precision-Recall Curve\n\n### Model Comparison\n[Source 5, Source 9, Source 10]\nThe documents cover detailed comparisons between:\n- Random Forest vs. Gradient Boosting\n- Decision Tree vs. Random Forest\n- Linear Regression vs. Logistic Regression\n- SVMs vs. Logistic Regression\n\n### Special Considerations\n[Source 2]\n- **Imbalanced Data** handling and its implications for both classification and regression tasks\n\nThis is a comprehensive machine learning interview guide that covers fundamental concepts and practical considerations for implementing machine learning solutions.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset."
        }
      ],
      "response_time": 12.460344791412354,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1650,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key machine learning concepts such as data preprocessing, model types, evaluation metrics, and model comparisons. However, the context provided is fragmented and does not fully represent the breadth of the answer, suggesting some key aspects might be missing or inferred from other parts of the documents not shown.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the topics mentioned (e.g., missing values, feature selection, tree-based models, and evaluation metrics). However, some details (e.g., clustering models like K-means or Gaussian Mixture Model) are not explicitly shown in the provided context, indicating minor unsupported additions.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It organizes the concepts into logical categories and provides specific examples under each, making it highly useful for understanding key machine learning concepts."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 2,
      "question": "What are some key concepts related to machine learning mentioned in the PDFs?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:39:59",
      "answer": "Based on the information from the provided PDF documents, some key concepts related to **machine learning** mentioned are:\n\n- **Data and Feature Engineering**:\n  - Missing Values handling techniques like imputation and data backfilling [Source 1], [Source 3].\n  - Feature Selection methods such as intrinsic feature selection and filter feature selection [Source 3].\n\n- **Model-Level Methods**:\n  - Techniques to make models robust to class imbalance, including penalizing wrong classifications of minority classes and selecting appropriate algorithms like tree-based models [Source 2].\n\n- **Evaluation Metrics**:\n  - Importance of choosing appropriate evaluation metrics for tasks, with considerations for interpretability and potential issues like overfitting in high-dimensional data [Source 2].\n\n- **Support Vector Machine (SVM)**:\n  - Pros include metrics like Precision-Recall curve for imbalanced data; cons relate to assumptions and potential issues with accuracy in imbalanced datasets [Source 2].\n\n- **Pros and Cons of ML Algorithms**:\n  - Characteristics of Linear Models, Tree-based Models, and Clustering Models discussed with their respective pros and cons [Source 1], [Source 2], [Source 5].\n\n- **Model Comparison**:\n  - Comparison between algorithms like Random Forest vs. Gradient Boosting, Decision Tree vs. Random Forest, highlighting differences in training, prediction, overfitting tendencies, hyperparameters, and parallelization [Source 4].\n\n- **Loss Functions**:\n  - Various loss functions explored, including Cross-Entropy, Hinge Loss, Mean Squared Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE) [Source 1].\n\n- **Evaluation Metrics for Regression**:\n  - Metrics like Mean Squared Error (MSE) discussed in the context of regression evaluation [Source 6].\n\nThese concepts provide a foundational understanding of key aspects of machine learning, including data preprocessing, model evaluation, algorithm comparison, and performance metrics.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved."
        }
      ],
      "response_time": 5.813844203948975,
      "num_citations": 6,
      "num_retrieved_docs": 40,
      "answer_length": 1985,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key machine learning concepts such as data and feature engineering, model-level methods, evaluation metrics, and algorithm comparisons. However, the context provided is fragmented and lacks depth in some areas, which slightly limits the comprehensiveness.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the concepts mentioned in the documents. There are minor unsupported additions (e.g., references to 'Source 3, 5, 6' when the context only explicitly mentions 'Source 1' and 'Source 2'), but these do not significantly detract from the overall groundedness.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive, covering a broad range of key machine learning concepts. However, it could be slightly improved by providing more specific details from the retrieved context and avoiding assumptions about sources not explicitly mentioned."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 2,
      "question": "What are some key concepts related to machine learning mentioned in the PDFs?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:40:14",
      "answer": "The provided PDF documents highlight several key concepts related to machine learning, categorized into different areas:\n\n*   **Interview Question Types**: The cheat sheet covers common machine learning interview question types, including:\n    *   Data and feature engineering [Source 1]\n    *   Pros and cons of machine learning models [Source 1]\n    *   Model comparison [Source 1]\n    *   Loss functions [Source 1]\n    *   Evaluation metrics [Source 1]\n\n*   **Data and Feature Engineering**:\n    *   **Missing Values**: Strategies include gathering more data, imputation (inferring missing values), model-based imputation (using non-missing data to predict missing data), and omission (removing missing observations or features) [Source 1, Source 4].\n    *   **Feature Selection**: This involves selecting a subset of original features for model training. Methods include:\n        *   **Filter Feature Selection**: Selects features that correlate well with target variables (e.g., univariate statistical analysis, feature importance scores) and is performed once, independent of the model algorithm [Source 3].\n        *   **Intrinsic Feature Selection**: Features are naturally embedded within the training process of models like tree-based models and regularization models [Source 3].\n    *   **Imbalanced Data**: This refers to datasets where one or more labels constitute the majority, leaving fewer examples of other labels, affecting both classification and regression tasks [Source 1, Source 7].\n\n*   **Pros and Cons of ML Algorithms**:\n    *   **Linear Models**: Can be effective at capturing large trends but can be prone to overfitting and sensitive to outliers [Source 1, Source 3, Source 7].\n    *   **Tree-based Models (Decision Trees, Random Forest, Gradient Boosting)**:\n        *   **Decision Trees**: Easy to understand and interpret, require little data pre-processing, but are prone to overfitting [Source 1, Source 8, Source 12].\n        *   **Random Forest**: Has better generalization performance, less sensitive to outliers, and generates feature importance, but is computationally expensive [Source 1, Source 5, Source 8].\n        *   **Gradient Boosting**: Produces very accurate models, handles missing data, but is computationally expensive and sacrifices interpretability for accuracy [Source 1, Source 8].\n    *   **Clustering Models (K-means, Gaussian Mixture Model)**:\n        *   **K-means**: Easy to implement [Source 1, Source 8].\n        *   **Gaussian Mixture Model (GMM)**: A soft clustering algorithm where clusters can have different shapes and sizes, but is less efficient to train [Source 1, Source 5].\n    *   **Support Vector Machines (SVMs)**: Effective in high-dimensional spaces, versatile with kernel functions, memory efficient, and can handle outliers [Source 3, Source 12].\n\n*   **Model Comparison**: The cheat sheet provides comparisons between various models, such as:\n    *   Random Forest vs. Gradient Boosting [Source 1, Source 5]\n    *   Decision Tree vs. Random Forest [Source 1, Source 5, Source 9]\n    *   Linear Regression vs. Logistic Regression [Source 1, Source 9]\n    *   SVMs vs. Logistic Regression [Source 1, Source 9, Source 10]\n    *   Logistic Regression vs. Random Forest [Source 1, Source 10]\n    *   K-means vs. Gaussian Mixture Model [Source 1, Source 10]\n\n*   **Loss Functions**:\n    *   Cross-Entropy [Source 1, Source 9, Source 10]\n    *   Hinge Loss [Source 1, Source 10]\n    *   Mean Squared Error (MSE) [Source 1, Source 9, Source 11, Source 13]\n    *   Root Mean Square Error (RMSE) [Source 1, Source 11]\n    *   Mean Absolute Error (MAE) [Source 1, Source 11]\n\n*   **Evaluation Metrics**:\n    *   **General Metrics**: Confusion Matrix, Accuracy, Precision, Recall, Specificity, F1 Score, ROC Curve, and Precision-Recall Curve [Source 1, Source 4, Source 6].\n    *   **Metrics for Imbalanced Data**: Precision-Recall curve, Precision, Recall, and F1 measure are important, while Accuracy and AUC of the ROC curve can be misleading or less sensitive to minority class improvements [Source 2].\n    *   **Regression Metrics**: Mean Squared Error (MSE), Root Mean Square Error (RMSE",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you thro..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        }
      ],
      "response_time": 5.668087959289551,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 4162,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are perfectly relevant to the query, as they comprehensively cover key machine learning concepts such as data and feature engineering, model types, loss functions, evaluation metrics, and model comparison. The context directly addresses the query's request for key concepts in machine learning.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information in the documents. However, there are minor instances where the answer might slightly extrapolate (e.g., organizing concepts into categories like 'Interview Question Types' which is not explicitly stated in the context but is a reasonable inference). Overall, the answer sticks closely to the provided context.",
          "answer_quality": "The answer is excellent in quality. It is clear, well-structured, and comprehensive, covering a wide range of key machine learning concepts in a logical and easy-to-follow manner. The inclusion of specific examples and categorization enhances its clarity and usefulness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 3,
      "question": "What are the procedures described for Feature Selection?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:40:31",
      "answer": "Based on the provided documents, there are three main types of feature selection methods described:\n\n1. **Filter Feature Selection Methods** [Source 2]\n- Selects features that correlate well with target variables\n- Uses univariate statistical analysis and feature importance scores\n- Process is performed only once\n- Independent of the model algorithm\n- **Pros**: Fast and simple\n- **Cons**: Tends to select redundant features and ignores relationships among features\n\n2. **Wrapper Feature Selection Methods** [Source 1]\n- An iterative process using sequential feature selection\n- Repeatedly adds subset features to the model\n- Uses resulting model performance to guide next subset selection\n- **Pros**: Searches for a wider variety of feature subsets than other methods\n- **Cons**:\n  - Significant computation time when there are many features\n  - Most potential to overfit features to training data\n\n3. **Intrinsic Feature Selection Methods** [Source 2]\n- Feature selection naturally embedded with training process\n- Used in tree-based models and regularization models\n- **Pros**:\n  - Fast and no external feature tool needed\n  - Provides direct connection between feature selection and object function\n- **Cons**:\n  - Model-dependent\n  - Limited choice of models\n\nThe feature selection process is typically used as a **pre-processing step** before conducting the actual learning [Source 2]. The main purpose is to select a subset of the original features for model training [Source 2].",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 11.348469972610474,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1488,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents mention key feature selection methods (forward selection, backward selection, cross-validation-based feature selection, and intrinsic methods), which align well with the query. However, the context is fragmented and does not explicitly detail all procedures (e.g., filter or wrapper methods), which are included in the answer. Thus, while highly relevant, it misses some procedural depth.",
          "faithfulness": "The answer is largely faithful to the retrieved context, correctly identifying intrinsic methods and the pre-processing role of feature selection. However, it introduces 'Filter' and 'Wrapper' methods, which are not explicitly mentioned in the provided context, though they are standard categories in feature selection. Minor unsupported additions slightly reduce the score.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It categorizes feature selection methods effectively, highlights pros and cons, and explains the purpose of feature selection. The inclusion of sources adds credibility, and the response directly addresses the query with high clarity."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 3,
      "question": "What are the procedures described for Feature Selection?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:40:41",
      "answer": "Based on the provided context from the PDF documents, the procedures described for **Feature Selection** include:\n\n1. **Wrapper Feature Selection Methods**:\n   - An iterative process that involves methods like sequential feature selection.\n   - Repeatedly adds subset features to the model and uses model performance to guide the selection of the next subset.\n   - **Pros**:\n     - Searches for a wider variety of feature subsets than other methods.\n   - **Cons**:\n     - Significant computation time with a large number of features.\n     - Potential to overfit features to the training data [Source 1].\n\n2. **Intrinsic Feature Selection Methods**:\n   - Have feature selection embedded naturally within the training process of models like tree-based models and regularization models.\n   - **Pros**:\n     - Fast and does not require external feature tools.\n     - Provides a direct connection between feature selection and the object function.\n   - **Cons**:\n     - Model-dependent and limited choice of models [Source 2].\n\n3. **Filter Feature Selection Methods**:\n   - Select features that correlate well with target variables using methods such as univariate statistical analysis and feature importance scores.\n   - The process is performed only once and is independent of the model algorithm [Source 2].\n\n4. **Model-based imputation**:\n   - Use non-missing data to predict missing data with methods like KNN, linear regression, or trees.\n   - **Omission**: Removing missing observations or features with lots of missing values in the dataset [Source 2].\n\n5. **Evaluation Metrics for Feature Selection**:\n   - Involves considering metrics like **Confusion Matrix**, **Accuracy**, **Precision**, **Recall**, **Specificity**, **F1 Score**, **ROC Curve**, and **Precision-Recall Curve** for evaluating feature selection methods [Source 2, Source 15].\n\nThese procedures encompass various methods and considerations for selecting features in machine learning models based on the information provided in the context.",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 4.5477540493011475,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 2011,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved context mentions 'forward selection', 'backward selection', and 'cross-validation based feature selection', which are key procedures for feature selection. However, the context is fragmented and does not explicitly detail all the methods (e.g., filter methods, intrinsic methods) that the model answer includes. Thus, while highly relevant, it does not cover all aspects comprehensively.",
          "faithfulness": "The model answer is mostly faithful to the retrieved context but includes additional details (e.g., pros/cons of methods, evaluation metrics) that are not explicitly mentioned in the provided context. Some claims, like the specifics of 'model-based imputation' and 'evaluation metrics', are unsupported by the given context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a comprehensive overview of feature selection procedures, including methods not explicitly detailed in the context. However, the inclusion of unsupported details slightly detracts from its completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 3,
      "question": "What are the procedures described for Feature Selection?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:40:51",
      "answer": "The procedures for **Feature Selection** involve selecting a subset of the original features for model training [Source 2]. These methods can be categorized as follows:\n\n## 1. Filter Feature Selection Methods\nThese methods select features that correlate well with target variables, such as **univariate statistical analysis** and **feature importance scores** [Source 2].\n*   **Pros**:\n    *   Fast and simple [Source 2].\n    *   Can be effective at capturing large trends in the dataset [Source 2].\n    *   The process is performed only once and is independent of the model algorithm [Source 2].\n*   **Cons**:\n    *   Tend to select redundant features [Source 1].\n    *   Ignore relationships among features [Source 1].\n\n## 2. Wrapper Feature Selection Methods\nThis is an iterative process, such as **sequential feature selection**, which repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset [Source 1].\n*   **Pros**:\n    *   Search for a wider variety of feature subsets than other methods [Source 1].\n*   **Cons**:\n    *   Significant computation time when the number of features is large [Source 1].\n    *   Have the most potential to overfit the features to the training data [Source 1].\n\n## 3. Intrinsic Feature Selection Methods\nThese methods have feature selection naturally embedded within the training process of models, such as **tree-based models** and **regularization models** [Source 2].\n*   **Pros**:\n    *   Fast and no external feature tool is needed [Source 2].\n    *   Provides a direct connection between feature selection and the object function [Source 2].\n*   **Cons**:\n    *   Model-dependent, and the choice of models is limited [Source 2].",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 2.3260080814361572,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1746,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document mentions key feature selection procedures like 'forward selection,' 'backward selection,' and 'cross-validation based feature selection,' which are highly relevant to the query. However, it is somewhat fragmented and does not explicitly categorize methods (e.g., filter, wrapper, intrinsic) as comprehensively as the model answer does. Thus, while the core content is relevant, some aspects of the query are addressed more thoroughly in the answer than in the retrieved context.",
          "faithfulness": "The answer is largely faithful to the retrieved context, correctly referencing 'forward selection,' 'backward selection,' and intrinsic methods. However, it introduces additional details (e.g., 'univariate statistical analysis,' 'feature importance scores,' and specific pros/cons) that are not explicitly mentioned in the provided context. These additions are reasonable and likely derived from general knowledge, but they are not directly supported by the retrieved document, leading to a minor deduction.",
          "answer_quality": "The answer is excellent in structure, clarity, and completeness. It categorizes feature selection methods logically, provides pros and cons for each, and addresses the query comprehensively. The use of bullet points and clear headings enhances readability, making it a high-quality response."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 4,
      "question": "How do intrinsic feature selection methods function?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:41:06",
      "answer": "Based on the provided documents, here is what we know about intrinsic feature selection methods:\n\n**Intrinsic feature selection methods** have the following key characteristics:\n\n- They have feature selection **naturally embedded within the training process** of specific models such as:\n  - Tree-based models\n  - Regularization models\n\n**Key advantages**:\n- Fast implementation\n- No external feature selection tools needed\n- Provides direct connection between feature selection and the object function\n\n**Key limitations**:\n- Model-dependent \n- Limited choice of models\n\n[Source 1]\n\nThis is the complete information available about intrinsic feature selection methods from the provided context. While the documents mention other types of feature selection methods (like filter and wrapper methods), I've focused only on the information specifically about intrinsic methods as per the question.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 7.474780082702637,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 894,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document contains relevant information about intrinsic feature selection methods, including their integration with the training process and some limitations. However, it misses some key aspects like specific examples of models (e.g., decision trees, Lasso) or a deeper explanation of how the selection process works intrinsically.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, including the key characteristics, advantages, and limitations of intrinsic feature selection methods without adding unsupported details.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively summarizes the information from the retrieved context and addresses the query directly. However, it could be slightly improved by elaborating on the 'how' (e.g., mechanisms or examples of intrinsic methods) if more context were available."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 4,
      "question": "How do intrinsic feature selection methods function?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:41:12",
      "answer": "**Intrinsic feature selection methods** function by having feature selection naturally embedded within the training process of models like tree-based models and regularization models. Here is how they operate according to the provided context:\n\n- **Function**: Have feature selection integrated into the training process of models like tree-based models and regularization models. \n- **Pros**:\n  - Fast and doesn't require external feature tools.\n  - Establishes a direct link between feature selection and the objective function.\n\n**Source**:\n- [Source 1 - Page 3]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features t..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, dec..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue ..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 12,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1,
          "text_snippet": "MACHINELEARNINGINTERVIEWCHEAT SHEET"
        },
        {
          "source": 13,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 14,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        }
      ],
      "response_time": 1.633944034576416,
      "num_citations": 14,
      "num_retrieved_docs": 40,
      "answer_length": 565,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods and their integration with the training process. However, it misses some details (e.g., specific examples of models or deeper explanations of how the selection occurs), which prevents a perfect score.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately describing intrinsic feature selection as embedded within the training process. However, the 'Pros' section includes claims (e.g., 'Fast and doesn't require external feature tools') not explicitly supported by the provided context, which slightly reduces the score.",
          "answer_quality": "The answer is clear, well-structured, and addresses the core aspects of the query. It could be improved by providing more specific examples or elaborating on the mechanisms of intrinsic methods, but it remains a strong response overall."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 4,
      "question": "How do intrinsic feature selection methods function?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:41:19",
      "answer": "**Intrinsic Feature Selection Methods** function by having **feature selection naturally embedded within the training process of models**. These methods are typically used as a pre-processing step before the actual learning occurs [Source 1].\n\nKey characteristics include:\n*   **Integration with Training**: Feature selection is an inherent part of the model's training, such as with tree-based models and regularization models [Source 1].\n*   **Efficiency**: They are fast and do not require an external feature tool [Source 1].\n*   **Direct Connection**: There is a direct link between feature selection and the objective function [Source 1].\n*   **Model Dependence**: A disadvantage is that they are model-dependent, and the choice of models is limited [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.560176134109497,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 767,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly addresses intrinsic feature selection methods and their integration with the training process. However, it misses some details (e.g., specific examples of models or deeper explanations of how selection occurs), which prevents a perfect score.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about intrinsic feature selection methods being embedded in the training process. Minor unsupported additions (e.g., 'fast and do not require an external feature tool') are not explicitly contradicted but are not directly stated in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete, addressing the core aspects of the query. However, it could be improved by providing more specific examples or elaborating further on the mechanisms of intrinsic feature selection."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 5,
      "question": "What are the pros of using intrinsic feature selection methods?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:41:31",
      "answer": "Based on the provided context, here are the pros of using intrinsic feature selection methods:\n\n- **Fast** and **no external feature tool** is needed\n- Provides a **direct connection between feature selection and the object function**\n\n[Source 1]\n\nThis is all the information provided in the context about the pros of intrinsic feature selection methods. While the documents discuss other types of feature selection methods (like filter and wrapper methods), I will only include the specific pros mentioned for intrinsic feature selection methods as stated in the source material.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 5.958998918533325,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 580,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods. However, it is slightly incomplete as it truncates the explanation (e.g., 'such as tr'), which may have provided additional pros.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately extracts and restates the pros mentioned ('fast', 'no external feature tool needed', and 'direct connection between feature selection and the object function') without adding unsupported claims.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively based on the provided context. However, it could be slightly improved by acknowledging the incompleteness of the retrieved context (e.g., 'The context provides limited details, but the mentioned pros are...')."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 5,
      "question": "What are the pros of using intrinsic feature selection methods?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:41:40",
      "answer": "Based on the provided context, the pros of using **intrinsic feature selection methods** are:\n\n- **Fast and no external feature tool is needed**.\n- **Provides a direct connection between feature selection and the object function** [Source 1 - Page 3].\n\nThese methods are advantageous for their speed and integration with the training process of models without the need for external tools. They establish a direct relationship between feature selection and the objective function, which can streamline the learning process.\n\nThese pros are sourced from [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.735151767730713,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 563,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods and their integration with the training process. However, it does not explicitly list all possible pros, so it misses some depth.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases the information provided and does not introduce unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It could be slightly more comprehensive by elaborating further on the advantages, but it is otherwise excellent."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 5,
      "question": "What are the pros of using intrinsic feature selection methods?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:41:45",
      "answer": "The pros of using **intrinsic feature selection methods** include:\n\n*   **Fast and no external feature tool is needed.** [Source 1]\n*   **Provides a direct connection between feature selection and the object function.** [Source 1]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.0015180110931396,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 230,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly discusses intrinsic feature selection methods. However, it is somewhat truncated and does not fully cover all potential pros, which limits the comprehensiveness of the retrieved context.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately extracts and paraphrases the information provided in the document without adding unsupported claims or contradictions.",
          "answer_quality": "The answer is clear and addresses the query, but it is somewhat incomplete. It only lists two pros without elaborating on them or providing additional context that might be expected for a comprehensive answer."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 6,
      "question": "What are the cons of intrinsic feature selection methods?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:41:55",
      "answer": "Based on the provided context, according to [Source 1], the cons of intrinsic feature selection methods are:\n\n- **Model-dependent** and the **choice of models is limited**\n\nThis is the only explicit mention of disadvantages (cons) for intrinsic feature selection methods in the provided documents. Intrinsic feature selection methods are described as having feature selection naturally embedded with the training process of models such as tree-based models and regularization models [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 4.743577241897583,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 494,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 2
        },
        "explanations": {
          "relevance": "The retrieved document mentions two cons of intrinsic feature selection methods ('Tend to select redundant features' and 'Ignore relationships among features'), but the model answer only partially addresses the query by highlighting a con ('Model-dependent and the choice of models is limited') that is not explicitly stated in the context. Thus, the relevance is somewhat limited as key aspects from the retrieved context are missed.",
          "faithfulness": "The answer includes a claim ('Model-dependent and the choice of models is limited') that is not directly supported by the retrieved context. While the context implies model dependency, it does not explicitly state this as a con. The answer also omits the cons mentioned in the context, leading to minor contradictions and unsupported additions.",
          "answer_quality": "The answer addresses the query but is incomplete and misleading. It fails to include the cons explicitly mentioned in the context and introduces an unsupported claim. The structure is clear, but the content lacks comprehensiveness and accuracy."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 6,
      "question": "What are the cons of intrinsic feature selection methods?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:42:04",
      "answer": "**Cons of Intrinsic Feature Selection Methods** as per the provided context include:\n\n- **Model-dependent and limited choice of models**: Intrinsic feature selection methods are limited by being model-dependent, restricting the choice of models that can be used [Source 1].\n- **Tend to select redundant features**: These methods may result in the selection of redundant features [Source 2].\n\nTherefore, the cons of intrinsic feature selection methods are their model dependency and tendency to select redundant features.\n\nCited Sources:\n- [Source 1]\n- [Source 2]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features t..."
        }
      ],
      "response_time": 2.1922929286956787,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 562,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses the cons of intrinsic feature selection methods, specifically mentioning the tendency to select redundant features and ignoring relationships among features. However, it does not fully cover all possible cons, as the model answer also includes 'model-dependent and limited choice of models,' which is not explicitly stated in the retrieved context.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes an unsupported claim ('Model-dependent and limited choice of models'). The retrieved context only explicitly mentions the tendency to select redundant features and ignoring relationships among features. The rest of the answer aligns with the provided context.",
          "answer_quality": "The answer is adequate and addresses the query, but it could be improved in completeness and clarity. It includes an unsupported point and does not fully explore the cons mentioned in the retrieved context (e.g., ignoring relationships among features). The structure is clear but slightly incomplete."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 6,
      "question": "What are the cons of intrinsic feature selection methods?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:42:19",
      "answer": "The cons of intrinsic feature selection methods are that they are **model-dependent** and the **choice of models is limited** [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 0.787524938583374,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 137,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document directly addresses the cons of intrinsic feature selection methods, mentioning that they 'tend to select redundant features' and 'ignore relationships among features.' While it covers key aspects, it does not explicitly mention 'model-dependent' or 'choice of models is limited,' which the answer includes, suggesting minor gaps in retrieval.",
          "faithfulness": "The answer introduces claims ('model-dependent' and 'choice of models is limited') that are not explicitly supported by the retrieved context. While these points may be generally true, they are not grounded in the provided document, leading to some unsupported details.",
          "answer_quality": "The answer is concise and addresses the query, but it is incomplete compared to the retrieved context (missing redundancy and feature relationships). It also includes unsupported claims, which slightly reduces clarity and completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 7,
      "question": "How is feature selection embedded in tree-based models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:42:34",
      "answer": "Based on the provided context, I can provide information about feature selection in tree-based models:\n\nTree-based models have **intrinsic feature selection** naturally embedded within their training process [Source 1]. This means they have several key characteristics related to feature selection:\n\n1. **No External Feature Selection Required**\n- Tree-based models don't require feature selection as a separate pre-processing step [Source 3, Source 4]\n- This is considered one of the key advantages of tree-based models\n\n2. **Built-in Feature Importance**\n- Random Forests specifically generate **feature importance** scores which help in interpreting results [Source 2]\n- This makes the models helpful for understanding which features are most relevant\n\nThe advantages of this intrinsic feature selection include:\n- **Fast processing** since no external feature selection tool is needed [Source 1]\n- **Direct connection** between feature selection and the objective function [Source 1]\n\nHowever, there is one limitation to note:\n- This approach is **model-dependent**, meaning the feature selection is specific to tree-based models and the choice of models is limited [Source 1]\n\nThis intrinsic feature selection capability contributes to why tree-based models like Random Forests are known for requiring minimal data pre-processing and parameter tuning [Source 2].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 8.85350513458252,
      "num_citations": 2,
      "num_retrieved_docs": 25,
      "answer_length": 1367,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents are somewhat relevant but do not directly address the query about how feature selection is embedded in tree-based models. The context provided focuses more on general pros and cons of tree-based models and random forests, with only indirect mentions of feature selection or feature importance.",
          "faithfulness": "The answer includes some unsupported claims and minor contradictions. For example, the answer references 'Source 1,' 'Source 2,' etc., but these sources are not provided in the retrieved context. Additionally, some statements (e.g., 'direct connection between feature selection and the objective function') are not explicitly supported by the given context.",
          "answer_quality": "The answer is adequate in addressing the query but could be improved in clarity and completeness. It correctly identifies that tree-based models have intrinsic feature selection and mentions feature importance, but the explanation lacks depth and relies on unsupported references. The structure is clear but could be more precise."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 7,
      "question": "How is feature selection embedded in tree-based models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:42:44",
      "answer": "**Feature selection** in tree-based models is embedded in the training process itself. Here is how feature selection is embedded in tree-based models according to the provided context:\n\n- **Decision Trees**:\n  - **Pros**:\n    - Easy to understand and interpret.\n    - Requires little data pre-processing.\n    - Doesn't require feature selection.\n    - Efficient in prediction.\n  - **Cons**:\n    - Prone to overfitting.\n    - Kernel functions and regularization terms are crucial.\n- **Random Forest**:\n  - **Pros**:\n    - Better generalization performance than individual decision trees.\n    - Doesn’t require feature selection.\n    - Less sensitive to outliers.\n    - Generates feature importance helpful for result interpretation.\n  - **Cons**:\n    - Computationally expensive.\n- **Gradient Boosting**:\n  - **Pros**:\n    - Produces accurate models.\n    - No data pre-processing required.\n    - Handles missing data without imputation.\n  - **Cons**:\n    - Slow training process.\n    - Computationally expensive.\n    - Sacrifices interpretability for accuracy.\n\nIn **tree-based models** like Decision Trees, Random Forest, and Gradient Boosting, feature selection is implicitly embedded because the models naturally determine the most important features during the training process. These models assess the relevance of features based on their ability to improve prediction accuracy, without the need for separate feature selection steps. Random Forest, for example, generates feature importance scores, indicating the contribution of each feature to the model's predictive performance [Source 3 - Page 6], [Source 4 - Page 7], [Source 10 - Page 13].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, dec..."
        }
      ],
      "response_time": 3.667438268661499,
      "num_citations": 10,
      "num_retrieved_docs": 25,
      "answer_length": 1649,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document provides some information about tree-based models (e.g., Random Forest pros/cons) but does not directly address how feature selection is embedded in these models. Key aspects of the query, such as the mechanism of feature selection in Decision Trees, Random Forest, or Gradient Boosting, are missing or only tangentially covered.",
          "faithfulness": "The answer includes unsupported claims (e.g., 'feature selection is implicitly embedded' without clear evidence from the retrieved context) and misrepresents some details (e.g., stating Decision Trees 'don’t require feature selection' is misleading, as they inherently perform feature selection during splits). The retrieved context does not explicitly discuss feature selection mechanisms.",
          "answer_quality": "The answer addresses the query and provides a structured overview of tree-based models, but it lacks clarity on the core question (how feature selection is embedded). Some points are repetitive or irrelevant (e.g., listing pros/cons unrelated to feature selection), and the explanation is incomplete."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 7,
      "question": "How is feature selection embedded in tree-based models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:42:55",
      "answer": "Feature selection is **naturally embedded** within the training process of **tree-based models** [Source 1]. This means that tree-based models, such as Decision Trees and Random Forests, **do not require external feature selection** as a separate pre-processing step [Source 3, Source 4].\n\nThe benefits of this intrinsic feature selection include:\n*   **Speed**: No external feature tool is needed [Source 1].\n*   **Direct connection**: There is a direct link between feature selection and the model's objective function [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 2.2115182876586914,
      "num_citations": 1,
      "num_retrieved_docs": 25,
      "answer_length": 532,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document snippet is only tangentially related to the query. It discusses general pros and cons of tree-based models (e.g., Random Forests) but does not explicitly address how feature selection is embedded in these models. Key aspects of the query, such as the mechanism of feature selection in tree-based models, are missing.",
          "faithfulness": "The answer includes claims (e.g., 'naturally embedded,' 'no external feature selection needed,' 'direct connection to the objective function') that are not supported by the provided context. The context does not mention feature selection at all, leading to unsupported assertions in the answer.",
          "answer_quality": "The answer is structured clearly and addresses the query, but it lacks depth and relies on unsupported claims. It could be improved by providing a more detailed explanation of *how* feature selection is embedded in tree-based models (e.g., splitting criteria, feature importance) and by ensuring all claims are grounded in the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 8,
      "question": "What is the relationship between feature selection and the objective function in intrinsic methods?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:43:10",
      "answer": "Based on the provided documents, I can provide information specifically about the relationship between feature selection and the objective function in intrinsic methods.\n\nAccording to [Source 1], **intrinsic feature selection methods** have the following key characteristics:\n\n- They have feature selection **naturally embedded within the training process** of models\n- This applies specifically to tree-based models and regularization models\n- One of their key advantages is that they provide a **direct connection between feature selection and the object function**\n\nThe documents highlight two main pros for intrinsic methods:\n- Fast implementation with no need for external feature selection tools\n- Direct connection between feature selection and the objective function\n\n[Source 1] also notes that there are some limitations:\n- They are **model-dependent**\n- The choice of models that can use intrinsic feature selection is limited\n\nBeyond these points, I cannot make additional claims about the specific nature of the relationship between feature selection and the objective function, as the provided documents do not contain more detailed information about this particular aspect.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 8.140196084976196,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 1187,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods and their relationship with the training process, which implicitly involves the objective function. However, it does not explicitly detail the *specific* relationship between feature selection and the objective function, missing some depth on this aspect.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, including the advantages (e.g., direct connection between feature selection and the objective function) and limitations (e.g., model-dependence) of intrinsic methods. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It directly addresses the query and highlights the key points from the retrieved context. However, it acknowledges the lack of detailed information about the *specific* relationship between feature selection and the objective function, which slightly limits its comprehensiveness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 8,
      "question": "What is the relationship between feature selection and the objective function in intrinsic methods?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:43:18",
      "answer": "The relationship between **feature selection** and the objective function in **intrinsic methods** is that intrinsic feature selection methods have **feature selection naturally embedded with the training process of models** such as **tree-based models** and **regularization models** [Source 1]. This integration of feature selection with the training process provides a direct connection between feature selection and the objective function of the model being trained.\n\n- **Intrinsic Feature Selection Methods**:\n  - Have feature selection embedded in the training process of models like tree-based models and regularization models.\n  - Provide a direct connection between feature selection and the objective function of the model.\n\nThis relationship ensures that feature selection is an integral part of the model training process, enhancing the efficiency and effectiveness of the learning process by selecting the most relevant features that contribute to optimizing the model's performance.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 2.5238378047943115,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 996,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses the relationship between feature selection and the objective function in intrinsic methods. However, it lacks depth and specific examples that could further clarify the connection.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases and expands on the information provided in the document without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively explains the relationship but could be improved by providing more specific examples or details about how feature selection is embedded in the objective function of different models."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 8,
      "question": "What is the relationship between feature selection and the objective function in intrinsic methods?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:43:25",
      "answer": "In **intrinsic feature selection methods**, there is a **direct connection between feature selection and the objective function** [Source 1]. This means that feature selection is naturally embedded within the training process of models, such as tree-based models and regularization models [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 0.8588109016418457,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 300,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods and their integration with the training process. However, it does not explicitly mention the 'objective function,' which is a key aspect of the query, so it misses a slight nuance.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases the document's description of intrinsic feature selection methods being embedded within the training process, without adding unsupported claims.",
          "answer_quality": "The answer is clear, well-structured, and addresses the core relationship between feature selection and the training process in intrinsic methods. However, it could be slightly improved by explicitly linking the 'objective function' to the training process, which is implied but not stated directly."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 9,
      "question": "Which models have intrinsic feature selection embedded in their training process?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:43:41",
      "answer": "Based on the provided context, according to [Source 1], **tree-based models** and **regularization models** have intrinsic feature selection naturally embedded in their training process.\n\nThis is further supported by evidence from [Source 2] and [Source 3] which mention that some specific tree-based models like:\n\n- **Random Forest**\n  - \"Doesn't require feature selection\"\n  - \"Generates feature importance which is helpful when interpreting the results\"\n[Source 2]\n\n- **Decision Trees**\n  - \"Doesn't require feature selection\"\n[Source 3]\n\nThis intrinsic feature selection method has the following characteristics:\n- **Pros**:\n  - Fast and no external feature tool is needed\n  - Provides a direct connection between feature selection and the object function\n\n- **Cons**:\n  - Model-dependent and the choice of models is limited\n[Source 1]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 7.725133180618286,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 839,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions 'models such as tr' (likely truncated text referring to tree-based models) and discusses intrinsic feature selection, which is relevant to the query. However, the context is incomplete (e.g., truncated text) and does not explicitly cover all possible models with intrinsic feature selection, such as regularization models, which were mentioned in the answer but not in the provided context.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes unsupported details. For example, the mention of 'regularization models' and specific pros/cons (e.g., 'Fast and no external feature tool is needed') are not explicitly stated in the provided context. The context only hints at tree-based models without full clarity.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides specific examples (Random Forest, Decision Trees) and outlines pros and cons. However, the lack of full support from the retrieved context slightly reduces its completeness and reliability."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 9,
      "question": "Which models have intrinsic feature selection embedded in their training process?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:43:57",
      "answer": "**Models that have intrinsic feature selection embedded in their training process** based on the provided context are as follows:\n\n1. **Tree-based Models**:\n    - **Decision Trees**:\n        - **Pros**:\n            - Easy to understand and interpret.\n            - Doesn't require feature selection.\n            - Requires little data pre-processing.\n        - **Cons**:\n            - Prone to overfitting.\n            - Do not directly provide probability estimates.\n    - **Random Forest**:\n        - **Pros**:\n            - Does not require feature selection.\n            - Effective in high-dimensional spaces.\n            - Memory efficient.\n        - **Cons**:\n            - Can be slow to train on large datasets.\n            - Less interpretable in nature.\n            - Sacrifices interpretability for accuracy.\n    - **Gradient Boosting**:\n        - **Cons**:\n            - Doesn't require feature selection. [Source 2]\n  \n2. **Support Vector Machine (SVM)**:\n    - **Cons**:\n        - Considers all features in the decision function, thus performing implicit feature selection. [Source 3]\n\nThese models have **intrinsic feature selection embedded** in their training process, allowing them to automatically select relevant features during model training without the need for external feature selection tools. [Source 1]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 3.0166521072387695,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1330,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions 'Intrinsic Feature Selection Methods' and hints at models like tree-based methods, which are relevant to the query. However, the context is incomplete (e.g., cut off text like 'Tree-b'), and it does not explicitly list all models with intrinsic feature selection. Thus, while the answer covers most aspects, it misses some clarity due to the fragmented context.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the pros and cons of tree-based models (e.g., 'Does not require feature selection') are not explicitly mentioned in the provided context. Additionally, the claim about SVM performing 'implicit feature selection' is not directly supported by the given context, though it may be a reasonable inference.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a good overview of models with intrinsic feature selection and organizes the information logically. However, it could be improved by explicitly noting which details are inferred rather than directly supported by the context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 9,
      "question": "Which models have intrinsic feature selection embedded in their training process?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:44:02",
      "answer": "**Intrinsic feature selection methods** have feature selection naturally embedded with the training process of models such as **tree-based models** and **regularization models** [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.1754522323608398,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 189,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods and mentions models with embedded feature selection. However, it is cut off (e.g., 'Tree-b') and does not fully list all possible models, missing some key aspects.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly identifying 'tree-based models' and implying 'regularization models' (though not explicitly stated in the truncated context). There is a minor unsupported addition ('regularization models') that isn't directly mentioned in the provided snippet.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. However, it could be slightly more comprehensive by explicitly listing more models (e.g., Lasso, decision trees) if the context had provided them."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 10,
      "question": "What are the pros and cons of linear models in machine learning?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:44:15",
      "answer": "Based on the provided context, I can only provide limited information about linear models, specifically from [Source 1] and [Source 3]:\n\n**Pros:**\n- Easy to explain, implement and interpret [Source 1, Source 3]\n\n**Cons:**\n- Makes linear assumptions between the features and the target [Source 1]\n- Prone to overfitting in high-dimensional data [Source 1, Source 3]\n- Sensitive to outliers [Source 1]\n\nNote: While the documents mention linear models in their topics [Source 2], they don't provide comprehensive information about all pros and cons of linear models in general. The information available is somewhat limited, and most of what's provided appears to be specifically about logistic regression as a type of linear model.\n\nIf you're looking for a complete analysis of linear models' pros and cons, I would need additional context from other documents, as the current context doesn't provide a full overview of all advantages and disadvantages of linear models.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 7.846772909164429,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 968,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about the pros and cons of linear models, covering key aspects such as interpretability, linear assumptions, overfitting, and sensitivity to outliers. However, the context is somewhat fragmented and does not provide a comprehensive overview of all possible pros and cons.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately cites the sources and does not include any unsupported claims or contradictions. The answer also explicitly acknowledges the limitations of the provided context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively based on the available context. It also transparently notes the limitations of the retrieved documents. However, it could be slightly improved by summarizing or synthesizing the information more concisely."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 10,
      "question": "What are the pros and cons of linear models in machine learning?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:44:27",
      "answer": "Based on the provided context from the PDF documents, the pros and cons of **linear models** in machine learning are as follows:\n\n**Pros:**\n- Easy to explain, implement, and interpret [Source 1], [Source 2].\n- Linear models are efficient in prediction [Source 2].\n- R-squared is easy to interpret [Source 3].\n\n**Cons:**\n- Linear models make linear assumptions between the features and the target [Source 1].\n- Linear models maximize the conditional likelihoods of the training data, making them prone to outliers [Source 2].\n- Linear models are sensitive to outliers [Source 1].\n- Linear models always increase upon adding a new variable [Source 3].\n\nThese points summarize the pros and cons of linear models in machine learning as outlined in the provided context.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        }
      ],
      "response_time": 3.7858049869537354,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 765,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of linear models such as their interpretability, assumptions, and sensitivity to outliers. However, the context also includes irrelevant information about Support Vector Machines and imbalanced data, which slightly detracts from the overall relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the pros and cons mentioned. However, it includes minor unsupported additions like 'Linear models are efficient in prediction [Source 2]' and 'R-squared is easy to interpret [Source 3]', which are not explicitly stated in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers the main pros and cons of linear models as per the context. However, it could be slightly improved by excluding unsupported claims and irrelevant references (e.g., [Source 3] when no such source is provided)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 10,
      "question": "What are the pros and cons of linear models in machine learning?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:44:35",
      "answer": "The pros and cons of linear models, specifically Logistic Regression, are as follows:\n\n**Pros:**\n*   **Easy to explain, implement, and interpret** [Source 1, Source 3].\n*   **Outputs the probability of class membership** [Source 2].\n\n**Cons:**\n*   **Makes linear assumptions** between the features and the target [Source 1].\n*   **Prone to overfitting** in high-dimensional data [Source 1, Source 3].\n*   **Sensitive to outliers** [Source 1].\n*   **Maximizes the conditional likelihoods of the training data**, which makes it prone to outliers [Source 2].",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 1.435459852218628,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 555,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key pros and cons of linear models (e.g., interpretability, linear assumptions, overfitting, and sensitivity to outliers). However, the context includes some irrelevant or fragmented information (e.g., details about Support Vector Machines and imbalanced data), which slightly detracts from perfect relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the pros and cons mentioned (e.g., linear assumptions, overfitting, sensitivity to outliers). However, it includes minor unsupported additions like '[Source 2]' for 'Outputs the probability of class membership' and 'Maximizes the conditional likelihoods,' which are not explicitly stated in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers the main pros and cons of linear models but could be slightly improved by excluding unsupported details (e.g., references to sources not fully aligned with the context) and ensuring all claims are directly tied to the retrieved documents."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 11,
      "question": "How do tree-based models compare to linear models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:44:54",
      "answer": "Based on the provided documents, I can compare tree-based models and linear models across several key aspects:\n\n## Tree-Based Models (Decision Trees & Random Forests)\n\n**Advantages:**\n- Easy to understand and interpret [Source 3]\n- Requires little data pre-processing [Source 3]\n- Does not require feature selection [Source 3]\n- Efficient in prediction - logarithmic cost for predictions [Source 3]\n- Can handle missing data (specifically for Gradient Boosting) [Source 4]\n- Less sensitive to outliers (for Random Forest) [Source 4]\n\n**Disadvantages:**\n- More prone to overfitting, especially for single decision trees [Source 1]\n- Computationally expensive (for Random Forest) [Source 4]\n\n## Linear Models (Linear/Logistic Regression)\n\n**Advantages:**\n- Easy to explain, implement, and interpret [Source 5, Source 8]\n- Simpler model with fewer parameters [Source 2]\n- Less prone to overfitting compared to random forests [Source 2]\n\n**Disadvantages:**\n- Makes linear assumptions between features and target [Source 5]\n- Prone to overfitting in high-dimensional data [Source 5]\n- Sensitive to outliers [Source 5]\n- Not good at extrapolation [Source 3]\n\n## Key Differences\n\n**Model Structure:**\n- Linear models assume linear relationships between features and target [Source 5]\n- Tree-based models can capture non-linear relationships through their hierarchical structure [Source 2]\n\n**Interpretability:**\n- Linear models can be interpreted through coefficient values [Source 2]\n- Tree-based models (especially Random Forests) can be interpreted through feature importance [Source 2]\n\n**Complexity:**\n- Linear models are simpler with fewer parameters [Source 2]\n- Tree-based models, especially ensembles like Random Forests, have more hyperparameters and greater complexity [Source 1]",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 12.86410403251648,
      "num_citations": 5,
      "num_retrieved_docs": 40,
      "answer_length": 1783,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about tree-based and linear models, covering their advantages, disadvantages, and key differences. However, some aspects of the comparison (e.g., extrapolation, handling missing data) are not explicitly mentioned in the provided context, indicating minor gaps in relevance.",
          "faithfulness": "The answer is largely faithful to the retrieved context, accurately reflecting the pros and cons of both model types. However, there are minor unsupported additions, such as the mention of 'not good at extrapolation' for linear models, which is not explicitly stated in the provided context.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It systematically compares tree-based and linear models across multiple dimensions (advantages, disadvantages, key differences) and presents the information in an organized and easy-to-understand manner."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 11,
      "question": "How do tree-based models compare to linear models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:45:05",
      "answer": "Tree-based models and linear models have distinct characteristics that set them apart. Here's how they compare based on the provided context:\n\n### Tree-based Models:\n- **Pros**:\n  - Easy to understand and interpret.\n  - Requires little data pre-processing.\n  - Efficient in prediction.\n  - Can handle outliers and overlapping classes.\n  - Effective in high dimensional spaces.\n- **Cons**:\n  - Prone to overfitting.\n  - Slow to train on large datasets.\n  - Do not directly provide probability estimates.\n- **Implementation**:\n  - Utilizes decision trees that work together to make predictions.\n  - Involves training many decision trees independently.\n  - Can be prone to overfitting if not regulated properly.\n- **Loss Functions**:\n  - Uses Cross-Entropy loss as a cost function to measure the difference between predicted and observed distributions. [Source 2], [Source 3]\n\n### Linear Models:\n- **Pros**:\n  - Easy to explain, implement, and interpret.\n  - Effective in high dimensional spaces.\n  - Versatile with different Kernel functions.\n  - Memory efficient in decision boundary determination.\n  - Can handle outliers and overlapping classes.\n- **Cons**:\n  - Makes linear assumptions between features and the target.\n  - Prone to overfitting in high-dimensional data.\n  - Sensitive to outliers.\n- **Training Process**:\n  - Learns a set of parameters from the dataset to make predictions.\n  - Assumes a linear relationship between input variables and the target.\n- **Loss Functions**:\n  - Often uses mean squared error (MSE) in many applications. [Source 1], [Source 4]\n\nIn summary, tree-based models like Random Forests offer interpretability, handle outliers well, but can be prone to overfitting. On the other hand, linear models like Logistic Regression are easy to interpret and implement but may struggle with non-linear relationships and high-dimensional data. Each type has its strengths and weaknesses, making them suitable for different use cases in machine learning.",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 4.948890209197998,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 1980,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document provides some relevant information about linear models (e.g., pros and cons) but is incomplete and fragmented. It does not explicitly mention tree-based models, missing key aspects of the query. The comparison is thus partially addressed but lacks depth and completeness.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes several unsupported details. For example, claims about tree-based models handling outliers, efficiency in prediction, and loss functions (e.g., Cross-Entropy) are not present in the provided context. Some pros/cons of linear models are correctly cited, but the answer extrapolates beyond the given information.",
          "answer_quality": "The answer is structured and addresses the query, but it includes inaccuracies and unsupported claims. While it provides a clear comparison, the completeness and reliability are compromised by the lack of full alignment with the retrieved context. The explanation could be improved with more precise and grounded details."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 11,
      "question": "How do tree-based models compare to linear models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:45:14",
      "answer": "I cannot directly compare tree-based models to linear models based on the provided PDF documents. The documents discuss various aspects of both tree-based models (like Decision Trees and Random Forests) and linear models (like Linear Regression and Logistic Regression), and even compare specific models within these categories. However, they **do not contain a direct, overarching comparison of \"tree-based models\" as a general category against \"linear models\" as a general category.**\n\nThe documents primarily focus on:\n*   **Differences between specific tree-based models** (e.g., Decision Tree vs. Random Forest [Source 1, Source 3]).\n*   **Differences between specific linear models** (e.g., Linear Regression vs. Logistic Regression [Source 1]).\n*   **Differences between a specific tree-based model and a specific linear model** (e.g., Random Forest vs. Logistic Regression [Source 2]).\n*   **Pros and Cons of individual models**, some of which are tree-based (Decision Trees [Source 3]) and some are linear (Logistic Regression [Source 3], Linear Regression [Source 4]).\n\nTherefore, I cannot provide a comprehensive comparison of \"tree-based models\" versus \"linear models\" as a whole.",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 2.0289018154144287,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 1192,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain information about tree-based and linear models individually, including their pros and cons, but they do not provide a direct or comprehensive comparison between the two categories as requested by the query. Thus, while the documents are somewhat relevant, they miss the key aspect of a general comparison.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the lack of a direct comparison between tree-based and linear models in the provided documents and correctly summarizes the content of the retrieved context without adding unsupported claims.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query by explaining why a direct comparison cannot be provided. It also usefully summarizes the type of information available in the documents. However, it could be slightly improved by suggesting how the available information *might* indirectly inform a comparison (e.g., by synthesizing pros/cons of individual models)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 12,
      "question": "What is the difference between Random Forest and Gradient Boosting?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:45:32",
      "answer": "Based on the provided documents, here are the key differences between Random Forest and Gradient Boosting:\n\n## Training Process\n- **Random Forest**: Each learner (tree) is trained independently\n- **Gradient Boosting**: Trees are trained sequentially to correct the errors made by previous trees\n[Source 1]\n\n## Prediction Method\n- **Random Forest**: Final prediction is an average of all trees' predictions\n- **Gradient Boosting**: Final prediction is a weighted sum of the collection\n[Source 1]\n\n## Overfitting Risk\n- **Random Forest**: Less prone to overfitting because it averages predictions from many different trees\n- **Gradient Boosting**: More prone to overfitting since it's trained sequentially and can continue fitting until data is perfectly fit\n[Source 1]\n\n## Performance Characteristics\n### Random Forest\n- Doesn't require much parameter tuning\n- Doesn't require feature selection\n- Less sensitive to outliers\n- Generates helpful feature importance\n- Computationally expensive\n[Source 2]\n\n### Gradient Boosting\n- Produces very accurate models (outperforms random forest in accuracy)\n- No data pre-processing required\n- Handles missing data without imputation\n- Computationally expensive - often requires many trees (>1000)\n- Slower to train due to sequential process\n- Less interpretable than random forest\n[Source 2]\n\n## Technical Implementation\n- **Random Forest**: Can be trained in parallel\n- **Gradient Boosting**: Cannot be trained in parallel\n[Source 1]\n\n## Hyperparameters\n- **Random Forest**: Fewer hyperparameters to tune\n- **Gradient Boosting**: More hyperparameters to tune\n[Source 1]\n\nBoth methods are **ensemble learning methods** that train multiple models and combine their predictions for more accurate results [Source 1].",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 12.217766761779785,
      "num_citations": 2,
      "num_retrieved_docs": 30,
      "answer_length": 1752,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are perfectly relevant to the query, covering all key aspects of the differences between Random Forest and Gradient Boosting, including training processes, prediction methods, overfitting risks, performance characteristics, technical implementation, and hyperparameters.",
          "faithfulness": "The answer is completely faithful to the retrieved context. All claims and details in the answer are directly supported by the provided documents, with no unsupported additions or contradictions.",
          "answer_quality": "The answer is excellent, clear, comprehensive, and well-structured. It systematically addresses all aspects of the query, uses bullet points for readability, and provides a thorough comparison of the two methods."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 12,
      "question": "What is the difference between Random Forest and Gradient Boosting?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:45:45",
      "answer": "Based on the information provided in the context from the PDF documents, here is the difference between **Random Forest** and **Gradient Boosting**:\n\n### Random Forest:\n- **Training**:\n  - Each tree in the ensemble is trained independently.\n- **Prediction**:\n  - The final prediction is an average of the predictions of all trees.\n- **Overfitting**:\n  - Less prone to overfitting due to averaging predictions of many different trees.\n- **Hyperparameters**:\n  - Fewer hyperparameters compared to Gradient Boosting.\n- **Interpretability**:\n  - The model can be interpreted based on feature importance.\n\n### Gradient Boosting:\n- **Training**:\n  - Each learner is trained sequentially to correct errors made by the previous tree.\n- **Prediction**:\n  - The overall prediction is given by a weighted sum of the collection.\n- **Overfitting**:\n  - More prone to overfitting as it is trained sequentially and can continue to fit the data until perfectly fit.\n- **Hyperparameters**:\n  - More hyperparameters compared to Random Forest.\n- **Interpretability**:\n  - Sacrifices interpretability for accuracy, making it less interpretative in nature.\n\nThe main differences between Random Forest and Gradient Boosting lie in their training process, prediction mechanism, handling of overfitting, the number of hyperparameters, and interpretability.\n\nSources: [Source 1], [Source 2]",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1,
          "text_snippet": "MACHINELEARNINGINTERVIEWCHEAT SHEET"
        }
      ],
      "response_time": 4.465663909912109,
      "num_citations": 2,
      "num_retrieved_docs": 30,
      "answer_length": 1365,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document directly addresses the query by comparing Random Forest and Gradient Boosting, covering key aspects such as training process, prediction mechanism, overfitting, hyperparameters, and interpretability. The context is perfectly relevant and comprehensive.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document without adding unsupported claims or contradictions. All points align with the source material.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It effectively breaks down the differences between Random Forest and Gradient Boosting in a logical manner, making it easy to understand. The inclusion of categories like training, prediction, and overfitting enhances clarity."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 12,
      "question": "What is the difference between Random Forest and Gradient Boosting?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:45:55",
      "answer": "**Random Forest vs. Gradient Boosting**\n\nRandom Forest and Gradient Boosting are both **ensemble learning methods** that combine predictions from multiple models to achieve a more accurate final prediction. However, they differ in their training approach and other characteristics [Source 1].\n\nHere's a breakdown of their differences:\n\n*   **Training Process**\n    *   **Random Forest:** Each learner (decision tree) is trained **independently** [Source 1].\n    *   **Gradient Boosting:** Each learner is trained **sequentially** to correct the errors made by the previous tree [Source 1].\n\n*   **Prediction Making**\n    *   **Random Forest:** The final prediction is an **average** of the predictions of all trees [Source 1].\n    *   **Gradient Boosting:** The overall prediction is given by a **weighted sum** of the collection of trees [Source 1].\n\n*   **Optimization**\n    *   **Random Forest:** Primarily reduces **variance** [Source 1].\n    *   **Gradient Boosting:** Reduces both **bias and variance** [Source 1].\n\n*   **Overfitting**\n    *   **Random Forest:** Less prone to overfitting as it averages the predictions of many different trees [Source 1, Source 3].\n    *   **Gradient Boosting:** More prone to overfitting because it's trained sequentially and can continue to fit the data until it is perfectly fit [Source 1].\n\n*   **Hyperparameters**\n    *   **Random Forest:** Has **fewer hyperparameters** [Source 1, Source 3].\n    *   **Gradient Boosting:** Has **more hyperparameters** [Source 1, Source 3].\n\n*   **Parallelization**\n    *   **Random Forest:** Can be trained **in parallel** [Source 1].\n    *   **Gradient Boosting:** **Cannot be trained in parallel** [Source 1].\n\n*   **Accuracy**\n    *   **Gradient Boosting:** Produces very accurate models and often **outperforms Random Forest in accuracy** [Source 2].\n\n*   **Computational Cost**\n    *   **Random Forest:** Can be **computationally expensive** [Source 2].\n    *   **Gradient Boosting:** Is a sequential process that can be **slow to train** and **computationally expensive**, often requiring many trees (>1000) which can be time and memory exhaustive [Source 2].\n\n*   **Interpretability**\n    *   **Random Forest:** Generates **feature importance**, which is helpful for interpretation [Source 2, Source 4]. The model can be interpreted based on feature importance [Source 3].\n    *   **Gradient Boosting:** Sacrifices interpretability for accuracy, being **less interpretative in nature** [Source 2].",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 3.3293769359588623,
      "num_citations": 3,
      "num_retrieved_docs": 30,
      "answer_length": 2484,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document is perfectly relevant to the query, as it directly addresses the differences between Random Forest and Gradient Boosting, covering all key aspects such as training process, prediction making, optimization, overfitting, hyperparameters, parallelization, accuracy, computational cost, and interpretability.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document without adding unsupported claims or contradictions. All points are directly sourced from the retrieved material.",
          "answer_quality": "The answer is excellent, clear, comprehensive, and well-structured. It provides a detailed breakdown of the differences between Random Forest and Gradient Boosting, covering all relevant dimensions in a logical and easy-to-follow manner."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 13,
      "question": "What are the evaluation metrics discussed in the PDFs?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:46:17",
      "answer": "Based on the provided documents, here are the key evaluation metrics discussed:\n\n## Classification Metrics\n- **Accuracy**: Percentage of predicted labels that match true labels [Source 4]\n- **Precision**: Percentage of accurate positives compared to total positives claimed [Source 4]\n- **Recall**: Percentage of positives claimed compared to actual positives in data [Source 4]\n- **Specificity**: Percentage of negatives claimed compared to actual negatives (true negative rate) [Source 4]\n- **F1 Score**: Harmonic mean of precision and recall [Source 4]\n\n## Curves and Area Metrics\n- **ROC Curve**:\n  - x-axis: False Positive Rate\n  - y-axis: Recall (TPR)\n  - Area under ROC curve is useful for comparing binary classification models [Source 2]\n\n- **Precision-Recall Curve**:\n  - x-axis: Recall\n  - y-axis: Precision\n  - Particularly useful for imbalanced data as it emphasizes the positive class [Source 2]\n\n## Regression Metrics\n- **Mean Squared Error (MSE)**:\n  - Measures average squared distance between predictions and true values\n  - Pros: Penalizes large errors, mathematically convenient\n  - Cons: Sensitive to outliers, hard to interpret due to squared units [Source 3]\n\n- **Mean Absolute Error (MAE)**:\n  - Measures average absolute difference between predictions and true values\n  - Pros: Simple to interpret, less sensitive to outliers than MSE\n  - Cons: Not differentiable at 0, making optimization more difficult [Source 3]\n\n## Important Considerations\n- **Accuracy** should be avoided when classes are imbalanced [Source 1]\n- **AUC of ROC curve** treats both classes equally and is less sensitive to minority class improvements [Source 1]\n- **Precision-Recall curve** is particularly helpful for dealing with imbalanced data [Source 1]",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "vote",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 16
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 12.359514951705933,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 1753,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents comprehensively address the query by discussing a wide range of evaluation metrics, including classification metrics (precision, recall, F1 score), curves (ROC and Precision-Recall), and regression metrics (MSE, MAE). The context directly aligns with the query's focus on evaluation metrics.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the metrics and their descriptions. However, minor details like the inclusion of 'Specificity' and some pros/cons of regression metrics (e.g., MAE not being differentiable at 0) are not explicitly mentioned in the provided context, though they are generally correct and not contradictory.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It categorizes metrics logically (classification, curves, regression), provides detailed explanations, and includes important considerations. The use of bullet points and subheadings enhances readability and completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 13,
      "question": "What are the evaluation metrics discussed in the PDFs?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:46:27",
      "answer": "Based on the information provided in the PDF documents, the evaluation metrics discussed are as follows:\n\n- **Precision-Recall Curve**:\n  - Gives more importance to the positive class and is helpful for dealing with imbalanced data.\n  - Emphasizes predictions the model got right out of the total number it predicted to be positive (i.e., precision) [Source 1], [Source 2].\n\n- **ROC Curve**:\n  - Compares binary classification models systematically and is independent of how the threshold is set [Source 2].\n\n- **Accuracy**:\n  - Percentage of predicted labels that match true labels [Source 3].\n\n- **Precision**:\n  - Percentage of accurate positives the model claims compared to the total number of positives it claims [Source 3].\n\n- **Recall**:\n  - Percentage of positives the model claims compared to the actual number of positives in the data [Source 3].\n\n- **Specificity**:\n  - Percentage of negatives the model claims compared to the actual number of negatives in the data [Source 3].\n\n- **F1 Score**:\n  - Harmonic mean of precision and recall [Source 3].\n\nThese evaluation metrics are important for assessing the performance of machine learning models in classification tasks.",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "vote",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 16
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 3.4101529121398926,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1182,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document discusses several evaluation metrics relevant to the query, including Precision-Recall Curve, Precision, Recall, F1 measure, and ROC Curve. However, it does not cover all the metrics mentioned in the model answer (e.g., Accuracy and Specificity), suggesting some aspects of the query are not fully addressed by the retrieved context.",
          "faithfulness": "The model answer is mostly faithful to the retrieved context, accurately describing the metrics mentioned in the document (Precision-Recall Curve, ROC Curve, Precision, Recall, F1). However, it includes additional metrics (Accuracy and Specificity) that are not explicitly supported by the provided context, which slightly reduces the faithfulness score.",
          "answer_quality": "The answer is clear, well-structured, and covers a comprehensive list of evaluation metrics. It provides useful explanations for each metric. However, the inclusion of metrics not explicitly mentioned in the retrieved context (Accuracy and Specificity) slightly detracts from its completeness relative to the provided documents."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 13,
      "question": "What are the evaluation metrics discussed in the PDFs?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:46:35",
      "answer": "The PDF documents discuss several evaluation metrics for machine learning models, categorized for different tasks:\n\n## Evaluation Metrics\n\n### For Binary Classification Problems:\n*   **Precision-Recall curve**: Gives more importance to the positive class and is useful for imbalanced datasets [Source 1, Source 3].\n*   **Precision**: The percentage of accurate positives the model claims compared to the total number of positives it claims [Source 3].\n*   **Recall (TPR)**: The percentage of positives the model claims compared to the actual number of positives in the data [Source 3]. Also known as the True Positive Rate [Source 2].\n*   **F1 Score**: The harmonic mean of precision and recall [Source 3]. A value of 1 indicates perfect precision and recall [Source 2].\n*   **ROC Curve**: Plots the False Positive Rate (x-axis) against Recall (y-axis). The area under the ROC curve is a systematic metric for comparing binary classification models, as it's independent of the threshold setting [Source 2].\n*   **Accuracy**: The percentage of predicted labels that match true labels [Source 3]. However, it is misleading when classes are imbalanced [Source 1].\n*   **Specificity**: The percentage of negatives the model claims compared to the actual number of negatives in the data, also known as the true negative rate [Source 3].\n\n### For Regression Problems:\n*   **Mean Squared Error (MSE)**: Measures the average squared distance between predictions and true values. It penalizes large errors but is sensitive to outliers and hard to interpret due to its squared unit [Source 4, Source 13].\n*   **Root Mean Square Error (RMSE)**: A variant of MSE [Source 5].\n*   **Mean Absolute Error (MAE)**: Measures the average absolute difference between predictions and true values. It is simpler to interpret and less sensitive to outliers compared to MSE, but it is not differentiable at 0 [Source 4, Source 13].\n*   **R-squared**: Easy to interpret and compare performance across models, but it always increases when new variables are added and doesn't show the model's predictive power [Source 4].\n*   **Adjusted R-squared**: Incorporates the model's degree of freedom [Source 4].\n\nThe documents also mention that **Cross-Entropy** and **Hinge Loss** are loss functions, with Cross-Entropy being used in logistic regression and Hinge Loss in SVMs [Source 2, Source 12, Source 13].",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "vote",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 16
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved."
        }
      ],
      "response_time": 2.679612159729004,
      "num_citations": 5,
      "num_retrieved_docs": 40,
      "answer_length": 2377,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are perfectly relevant to the query, as they comprehensively cover evaluation metrics for machine learning models, including both classification and regression tasks. The context directly addresses the query's focus on evaluation metrics.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately cites the metrics mentioned in the documents (e.g., Precision-Recall curve, ROC Curve, MSE, MAE) and does not introduce unsupported claims or contradictions. The sources are even referenced explicitly.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It categorizes metrics by task (binary classification vs. regression), provides definitions, and includes additional context (e.g., limitations of accuracy, R-squared). The inclusion of loss functions (Cross-Entropy, Hinge Loss) further enhances completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 14,
      "question": "How are clustering models different from tree-based models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:46:53",
      "answer": "Based on the provided context, I can highlight several key differences between clustering models and tree-based models:\n\n## Purpose and Learning Type\n- **Clustering models** (like K-means and GMM) are **unsupervised learning** algorithms used to group data into clusters [Source 1]\n- **Tree-based models** (like Decision Trees and Random Forests) are typically used for **supervised learning** tasks like classification and regression [Source 2]\n\n## Key Characteristics\n\n### Clustering Models:\n- **K-means characteristics**:\n  - Requires specifying number of clusters upfront\n  - Forms circular clusters with equal sizes\n  - Computationally efficient\n  - Simple to implement [Source 5, Source 7]\n\n- **GMM characteristics**:\n  - Offers soft clustering (probability-based assignments)\n  - Can handle clusters of different shapes and sizes\n  - Less efficient due to flexibility [Source 3, Source 7]\n\n### Tree-based Models:\n- **Decision Trees**:\n  - Easy to understand and interpret\n  - Requires little data pre-processing\n  - Makes predictions based on series of rules\n  - Prone to overfitting [Source 3]\n\n- **Random Forests**:\n  - Better generalization than single trees\n  - Less sensitive to outliers\n  - Generates feature importance\n  - Computationally expensive\n  - Involves training multiple trees independently [Source 5, Source 2]\n\n## Prediction Process\n- **Clustering models** assign data points to groups either definitively (K-means) or probabilistically (GMM) [Source 1]\n- **Tree-based models** make predictions based on learned decision rules (Decision Trees) or ensemble averaging (Random Forests) [Source 2]",
      "retrieved_docs": [
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-mea..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 11.70257306098938,
      "num_citations": 3,
      "num_retrieved_docs": 30,
      "answer_length": 1618,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about clustering models (K-means, GMM) and tree-based models (Decision Trees, Random Forests), which directly address the query. However, the context is fragmented and does not explicitly compare clustering and tree-based models in a structured way, missing some key comparative aspects (e.g., interpretability, use cases).",
          "faithfulness": "The answer is largely faithful to the retrieved context, accurately reflecting the differences between clustering and tree-based models as described in the documents. However, there are minor unsupported additions (e.g., 'Computationally efficient' for K-means and 'Less sensitive to outliers' for Random Forests), which are generally true but not explicitly stated in the provided context.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It effectively organizes the differences between clustering and tree-based models, highlights key characteristics, and provides a nuanced comparison. The response is easy to follow and addresses the query thoroughly."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 14,
      "question": "How are clustering models different from tree-based models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:47:07",
      "answer": "**Clustering Models**:\n- **K-means**:\n  - **Convergence**: Converges when the centroids of the clusters do not change significantly between iterations.\n  - **Shape of clusters**: Circular clusters with equal sizes.\n  - **Efficiency**: More efficient due to simplicity.\n  - **Loss function**: Uses Cross-Entropy [Source 2].\n\n- **Gaussian Mixture Model (GMM)**:\n  - **Convergence**: Converges when the parameters of the model (mean, covariance, and mixing coefficients) do not change significantly between iterations.\n  - **Shape of clusters**: Clusters can have different shapes and sizes.\n  - **Efficiency**: Less efficient due to flexibility [Source 2].\n\n**Tree-based Models**:\n- **Decision Tree**:\n  - **Training**: A single tree is trained on the dataset.\n  - **Prediction**: Based on a series of rules trained on the dataset.\n  - **Pros**: Easy to understand and interpret, requires little data pre-processing, and doesn't require feature selection [Source 3, Source 4].\n\n- **Random Forest**:\n  - **Training**: An ensemble of decision trees, each trained independently.\n  - **Prediction**: Final prediction is an average of all tree predictions.\n  - **Pros**: More efficient than individual decision trees, better generalization performance, and generates feature importance [Source 3, Source 4, Source 5].\n\n**Differences**:\n- **Clustering Models**:\n  - **K-means**:\n    - **Pros**:\n      - Easy to implement.\n      - Better generalization performance.\n      - Less sensitive to outliers.\n    - **Cons**:\n      - Computationally expensive [Source 1].\n\n- **Tree-based Models**:\n  - **Decision Tree and Random Forest**:\n    - **Pros**:\n      - Easy to understand and interpret.\n      - Efficient in prediction.\n      - Feature importance can be determined.\n    - **Cons**:\n      - Prone to overfitting [Source 5, Source 6].\n\nBased on the provided context, clustering models focus on grouping data into clusters based on similarities, whereas tree-based models use decision trees or ensembles of trees for prediction tasks. Clustering models like K-means and GMM assign data points to clusters, while tree-based models like Decision Trees and Random Forest use rules derived from the data for prediction.",
      "retrieved_docs": [
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, co..."
        }
      ],
      "response_time": 5.1673479080200195,
      "num_citations": 2,
      "num_retrieved_docs": 30,
      "answer_length": 2205,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about clustering models (K-means and GMM) and tree-based models (Decision Trees and Random Forest). However, the context is fragmented and does not directly compare the two categories as explicitly as the query demands. Key aspects like the fundamental difference between unsupervised (clustering) and supervised (tree-based) learning are implied but not clearly stated in the retrieved context.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported or incorrect details. For example, the claim that K-means uses 'Cross-Entropy' as a loss function is incorrect (it uses squared Euclidean distance). Additionally, some pros and cons (e.g., 'better generalization performance' for K-means) are not directly supported by the provided context. The answer also introduces terms like 'unsupervised' and 'supervised' learning, which are not mentioned in the retrieved documents.",
          "answer_quality": "The answer addresses the query and provides a structured comparison of clustering and tree-based models. However, it includes inaccuracies (e.g., loss function for K-means) and some claims that are not well-supported by the context. The explanation of differences is clear but could be more concise and accurate. The structure is logical, but the completeness and clarity could be improved."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 14,
      "question": "How are clustering models different from tree-based models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:47:14",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about the differences between clustering models and tree-based models. The documents discuss these two categories of models separately and compare models within each category (e.g., K-means vs. GMM, Decision Tree vs. Random Forest), but not a direct comparison between the two overarching categories.",
      "retrieved_docs": [
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-mea..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, co..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Fe..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        }
      ],
      "response_time": 1.0132789611816406,
      "num_citations": 10,
      "num_retrieved_docs": 30,
      "answer_length": 408,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents contain comparisons within the same model categories (e.g., K-means vs. GMM for clustering, Decision Tree vs. Random Forest for tree-based models) but do not provide a direct or meaningful comparison between clustering models and tree-based models. Thus, they are mostly irrelevant to the query.",
          "faithfulness": "The answer accurately reflects the content of the retrieved documents. It correctly states that the documents do not contain information about the differences between clustering models and tree-based models, only comparisons within each category.",
          "answer_quality": "The answer is excellent: clear, concise, and well-structured. It directly addresses the query by explaining the limitations of the retrieved context and does so in a helpful manner."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 15,
      "question": "What are the key differences between a Decision Tree and a Random Forest?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:47:29",
      "answer": "Based on the provided documents, here are the key differences between Decision Trees and Random Forests:\n\n## Training Process\n- **Decision Tree**: A single tree is trained on the dataset [Source 1]\n- **Random Forest**: An ensemble of multiple decision trees, where each tree is trained independently [Source 1]\n\n## Overfitting\n- **Decision Tree**: More prone to overfitting as the depth of the tree grows [Source 2]\n- **Random Forest**: Less prone to overfitting since it averages the predictions of many different trees [Source 2]\n\n## Hyperparameters\n- **Decision Tree**: Requires fewer hyperparameters [Source 2]\n- **Random Forest**: Has more hyperparameters to tune [Source 2]\n\n## Interpretability\n- **Decision Tree**: More interpretable as it's based on a series of simple rules that can be followed to make predictions [Source 2]\n- **Random Forest**: Less straightforward interpretation, though models can be interpreted based on feature importance [Source 2]\n\n## Additional Random Forest Advantages\nRandom Forests offer several benefits over single Decision Trees:\n- Better generalization performance due to randomness\n- Doesn't require much parameter tuning\n- Doesn't require feature selection\n- Less sensitive to outliers\n- Generates useful feature importance metrics [Source 3]\n\n## Decision Tree Characteristics\nNotable characteristics of Decision Trees include:\n- Easy to understand and interpret\n- Requires little data pre-processing\n- Efficient in prediction\n- However, they are prone to overfitting and sensitive to noise\n- Not good at extrapolation [Source 4]",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 9.959269046783447,
      "num_citations": 4,
      "num_retrieved_docs": 30,
      "answer_length": 1573,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain highly relevant information about the differences between Decision Trees and Random Forests, covering key aspects like training process, overfitting, and hyperparameters. However, some parts of the context (e.g., references to Logistic Regression) are irrelevant and could introduce noise.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the differences mentioned (e.g., training process, overfitting, hyperparameters). However, some details (e.g., 'feature importance metrics,' 'extrapolation') are not explicitly supported by the provided context, though they are generally true.",
          "answer_quality": "The answer is clear, well-structured, and comprehensive, addressing the query effectively. It could be slightly improved by omitting unsupported details (e.g., extrapolation) or clarifying which points are not directly from the context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 15,
      "question": "What are the key differences between a Decision Tree and a Random Forest?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:47:42",
      "answer": "**Key Differences Between Decision Tree and Random Forest:**\n\n1. **Training:**\n   - **Decision Tree:**\n     - Trained on the dataset as a single tree.\n   - **Random Forest:**\n     - Ensemble of decision trees where each tree is trained independently. \n     - Involves bagging and bootstrap techniques for training multiple trees [Source 3].\n\n2. **Prediction:**\n   - **Decision Tree:**\n     - Makes predictions based on rules learned from the dataset.\n   - **Random Forest:**\n     - Final prediction is an average of predictions from all trees in the ensemble [Source 1].\n\n3. **Overfitting:**\n   - **Decision Tree:**\n     - More prone to overfitting as the depth of the tree grows [Source 2].\n   - **Random Forest:**\n     - Less prone to overfitting due to averaging predictions from multiple trees [Source 2].\n\n4. **Hyperparameters:**\n   - **Decision Tree:**\n     - Fewer hyperparameters to tune.\n   - **Random Forest:**\n     - Requires tuning of more hyperparameters [Source 1, Source 2].\n\n5. **Interpretability:**\n   - **Decision Tree:**\n     - More interpretable due to simple rules.\n   - **Random Forest:**\n     - Model interpretability based on feature importance, which may be less straightforward compared to decision trees [Source 2, Source 6].\n\n6. **Optimization:**\n   - **Decision Tree:**\n     - Mainly reduces variance.\n   - **Random Forest:**\n     - Focuses on reducing variance but also addresses bias to some extent [Source 1].\n\n7. **Parallelization:**\n   - **Decision Tree:**\n     - Can be trained in parallel.\n   - **Random Forest:**\n     - Cannot be trained in parallel [Source 1].\n\nBy combining information from [Source 1] and [Source 2], the key differences between Decision Trees and Random Forests are highlighted in terms of training, prediction, overfitting, hyperparameters, interpretability, optimization, and parallelization.",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 5.287747859954834,
      "num_citations": 3,
      "num_retrieved_docs": 30,
      "answer_length": 1851,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences between Decision Trees and Random Forests such as training, overfitting, and hyperparameters. However, some aspects (e.g., interpretability, parallelization) are either partially addressed or fragmented across the documents, missing minor details.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the differences mentioned in the documents. However, there are minor unsupported additions (e.g., 'bagging and bootstrap techniques' and 'parallelization' claims are not explicitly stated in the provided context).",
          "answer_quality": "The answer is clear, well-structured, and covers most key differences comprehensively. It could be improved by ensuring all points are directly supported by the retrieved context and avoiding minor assumptions (e.g., parallelization)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 15,
      "question": "What are the key differences between a Decision Tree and a Random Forest?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:47:50",
      "answer": "The key differences between a Decision Tree and a Random Forest lie in their **training process**, **prediction mechanism**, **susceptibility to overfitting**, and **interpretability**.\n\nHere's a breakdown of the differences:\n\n*   **Training Process**\n    *   **Decision Tree**: A single tree is trained on the entire dataset. [Source 1]\n    *   **Random Forest**: An ensemble of decision trees, where each tree is trained independently. [Source 1]\n\n*   **Prediction Mechanism**\n    *   **Decision Tree**: Makes predictions based on a series of rules learned from the dataset. [Source 1]\n    *   **Random Forest**: The final prediction is an average of the predictions made by all the individual trees. [Source 1]\n\n*   **Overfitting**\n    *   **Decision Tree**: More prone to overfitting, especially as the depth of the tree grows. [Source 2]\n    *   **Random Forest**: Less prone to overfitting because it averages the predictions of many different trees, leading to better generalization performance due to randomness. [Source 1], [Source 2], [Source 4]\n\n*   **Hyperparameters**\n    *   **Decision Tree**: Has fewer hyperparameters. [Source 2]\n    *   **Random Forest**: Generally has more hyperparameters. [Source 2]\n\n*   **Interpretability**\n    *   **Decision Tree**: More interpretable as its predictions are based on a series of simple, followable rules. [Source 2], [Source 5]\n    *   **Random Forest**: Models can be interpreted based on feature importance, which is less straightforward compared to decision trees. [Source 2], [Source 3], [Source 4]\n\n*   **Generalization Performance**\n    *   **Random Forest**: Has better generalization performance than an individual decision tree due to randomness. [Source 4]",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 2.1251821517944336,
      "num_citations": 5,
      "num_retrieved_docs": 30,
      "answer_length": 1723,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences between Decision Trees and Random Forests, including training processes, overfitting, hyperparameters, and interpretability. The context directly addresses the query's requirements.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the differences mentioned in the documents. However, minor details like 'generalization performance' and 'interpretability' are slightly expanded beyond the exact wording of the retrieved context, though they are logically consistent with it.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It systematically breaks down the differences, provides logical explanations, and maintains a high level of clarity and completeness. The inclusion of sources adds credibility."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 16,
      "question": "How do loss functions relate to machine learning models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:48:11",
      "answer": "Based on the provided documents, I'll explain how loss functions relate to machine learning models:\n\n## Types of Loss Functions\n\nSeveral key loss functions are used in different machine learning models:\n\n1. **Cross-Entropy Loss**\n- Used in **logistic regression**\n- Measures the difference between predicted probability distribution and observed distribution\n- Used for binary classification problems\n[Source 1, Source 5]\n\n2. **Hinge Loss**\n- Used in **Support Vector Machines (SVM)**\n- Measures the amount by which the model's prediction is incorrect\n[Source 2, Source 5]\n\n3. **Mean Squared Error (MSE)**\n- Commonly used in regression problems\n- Measures average squared distance between predictions and true values\n- Pros:\n   - Penalizes large errors\n   - Mathematically convenient for gradient calculation\n- Cons:\n   - Sensitive to outliers\n   - Hard to interpret due to squared units\n[Source 5, Source 10]\n\n4. **Mean Absolute Error (MAE)**\n- Used in regression problems\n- Measures average absolute difference between predictions and true values\n- Pros:\n   - Simpler to interpret\n   - Less sensitive to outliers compared to MSE\n- Cons:\n   - Not differentiable at 0, making optimization more difficult\n[Source 5, Source 10]\n\n## Model-Specific Applications\n\nDifferent models use specific loss functions that are suited to their purpose:\n\n- **Linear Regression**: Typically uses MSE\n- **Logistic Regression**: Uses cross-entropy loss\n- **SVM**: Uses hinge loss\n[Source 1, Source 2]\n\nThe choice of loss function is important because it:\n- Helps measure the model's performance\n- Guides the optimization process during training\n- Affects how the model handles errors and outliers\n\n[Source 3, Source 5]",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 12,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 13,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 14,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 15,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1,
          "text_snippet": "MACHINELEARNINGINTERVIEWCHEAT SHEET"
        }
      ],
      "response_time": 14.099505186080933,
      "num_citations": 15,
      "num_retrieved_docs": 40,
      "answer_length": 1699,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, as they discuss loss functions (e.g., hinge loss and cross-entropy loss) and their association with specific machine learning models (e.g., SVM and logistic regression). However, the context is somewhat fragmented and does not cover all aspects of the query comprehensively, such as the broader role of loss functions in model training and optimization.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the explanation of Mean Squared Error (MSE) and Mean Absolute Error (MAE) is not present in the provided context, nor are the pros and cons of these loss functions. The core information about cross-entropy loss and hinge loss is accurate and aligned with the context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively explains the relationship between loss functions and machine learning models, particularly for classification tasks. However, the inclusion of unsupported details (e.g., MSE and MAE) slightly detracts from its completeness and accuracy, as these were not part of the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 16,
      "question": "How do loss functions relate to machine learning models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:48:23",
      "answer": "**Loss functions** play a crucial role in **machine learning models** by quantifying how well a model is performing. Here's how **loss functions** relate to **machine learning models** based on the provided context:\n\n1. **Types of Loss Functions**:\n   - **Mean Squared Error (MSE)**:\n     - Used in most applications of **prediction** in **machine learning models** [Source 1].\n     - Measures the average squared distance between predictions and true values [Source 7].\n   - **Cross-Entropy Loss**:\n     - Used in **logistic regression** to measure the difference between the predicted probability distribution and the observed distribution [Source 1].\n     - Evaluates the performance of a binary classification model by comparing the output distribution to observations [Source 4].\n   - **Hinge Loss**:\n     - Used in **SVMs** as a measure of the amount by which the model's prediction is incorrect [Source 2].\n  \n2. **Model-Specific Implications**:\n   - **Linear Models**:\n     - Linear regression is prone to overfitting in high-dimensional data [Source 3].\n     - **Support Vector Machine (SVM)**:\n       - Utilizes **hinge loss** as the loss function [Source 2].\n       - Can learn non-linear decision boundaries through kernel tricks [Source 2].\n   - **Tree-based Models**:\n     - **Decision Trees**:\n       - Prone to overfitting if not controlled [Source 5].\n     - **Random Forest**:\n       - Less prone to overfitting compared to decision trees [Source 5].\n  \n3. **Evaluation Metrics**:\n   - Metrics like **Precision-Recall curve**, **Precision**, **Recall**, and **F1** measure a model’s performance with respect to the positive class in binary classification [Source 3].\n   - Avoid using **accuracy** when classes are imbalanced [Source 3].\n   - **ROC Curve** and **Precision-Recall Curve** are helpful in evaluating models [Source 6].\n  \nIn conclusion, **loss functions** are essential components in **machine learning models** as they guide the optimization process by quantifying the model's performance against the true values. Different **loss functions** are used based on the requirements of the model and the nature of the data being analyzed. Each **loss function** has specific implications for different types of **machine learning models** and influences the evaluation metrics used to assess model performance.",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        }
      ],
      "response_time": 5.318225145339966,
      "num_citations": 7,
      "num_retrieved_docs": 40,
      "answer_length": 2337,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain some relevant information about loss functions (e.g., hinge loss, cross-entropy loss) and their use in specific models like SVM and logistic regression. However, they miss broader explanations of how loss functions relate to machine learning models in general, such as their role in optimization, gradient descent, or their impact on model training. Key aspects of the query are only partially addressed.",
          "faithfulness": "The answer includes several unsupported claims or details not present in the retrieved context. For example, it mentions Mean Squared Error (MSE), overfitting in linear regression, decision trees, random forests, and evaluation metrics like Precision-Recall curves, which are not covered in the provided context. Additionally, some terms (e.g., 'Source 1', 'Source 7') suggest references to documents not included in the retrieved context.",
          "answer_quality": "The answer is structured and addresses the query, but it includes irrelevant or unsupported details that reduce clarity. While it explains some aspects of loss functions (e.g., cross-entropy, hinge loss), the inclusion of unrelated topics (e.g., tree-based models, evaluation metrics) makes the answer less focused. The conclusion is reasonable but could be more concise and aligned with the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 16,
      "question": "How do loss functions relate to machine learning models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:48:31",
      "answer": "Loss functions are a fundamental component in machine learning, measuring the difference between predicted and observed distributions [Source 1]. They are used across various machine learning models to guide the learning process.\n\nHere's how different loss functions relate to specific machine learning models:\n\n*   **Cross-Entropy Loss**:\n    *   Used in **Logistic Regression** as the cost function to measure the difference between the predicted probability distribution and the observed distribution [Source 1, Source 2].\n    *   Measures the performance of a binary classification model by comparing the output distribution to observations [Source 2].\n\n*   **Hinge Loss**:\n    *   Used in **Support Vector Machines (SVM)** as the loss function, measuring the amount by which the model's prediction is incorrect [Source 2, Source 5].\n\n*   **Mean Squared Error (MSE)**:\n    *   Used in **Linear Regression** in most applications [Source 1].\n    *   Measures the average squared distance between predictions and true values [Source 3].\n    *   **Pros**: Penalizes large errors and is mathematically convenient for obtaining gradients [Source 3].\n    *   **Cons**: Sensitive to outliers and hard to interpret because the unit is squared [Source 3].\n\n*   **Root Mean Square Error (RMSE)**:\n    *   Another loss function listed in the context [Source 3, Source 5].\n\n*   **Mean Absolute Error (MAE)**:\n    *   Another loss function listed in the context [Source 3, Source 5].\n    *   Measures the average absolute difference between predictions and true values [Source 3].\n    *   **Pros**: Relatively simple to interpret and less sensitive to outliers compared to MSE [Source 3].\n    *   **Cons**: Not differentiable at 0, making it more difficult to optimize [Source 3].\n\nThese loss functions are crucial for training models by quantifying the error and enabling optimization algorithms to adjust model parameters to minimize this error [Source 1, Source 2].",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        }
      ],
      "response_time": 2.3705079555511475,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1958,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, as they directly address the relationship between loss functions and machine learning models, particularly focusing on specific examples like cross-entropy loss and hinge loss. However, the context is somewhat fragmented and lacks a comprehensive overview, missing some broader connections or general explanations about loss functions.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about cross-entropy loss, hinge loss, and other loss functions mentioned in the documents. However, it includes minor unsupported additions, such as details about Mean Squared Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE), which are not explicitly covered in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete, providing a good overview of how loss functions relate to machine learning models. It effectively categorizes different loss functions and their applications. However, it could be slightly improved by ensuring all details are strictly derived from the provided context or by clarifying which parts are general knowledge."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 17,
      "question": "What is the role of missing values in data and feature engineering?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:48:45",
      "answer": "Based on the provided context, I can explain the role of missing values in data and feature engineering. Here are the main approaches to handling missing values according to [Source 1]:\n\nThere are three primary strategies for dealing with missing values:\n\n1. **Data Gathering**\n- Attempt to **backfill the data**\n- Join with external datasets to complete missing information\n\n2. **Imputation**\n- **Infer missing values** using existing data\n- Can use various models for prediction:\n  - KNN\n  - Linear regression\n  - Trees\n\n3. **Omission**\n- Remove observations with missing values\n- Remove features that have many missing values in the dataset\n\nMissing values are considered an important aspect of data and feature engineering, as evidenced by its inclusion in the core topics listed in [Source 2] alongside other fundamental concepts like feature selection and imbalanced data.\n\nThe handling of missing values is typically a preprocessing step that needs to be addressed before moving forward with model training and evaluation.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        }
      ],
      "response_time": 7.572472810745239,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 1029,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document snippet is highly relevant to the query, as it directly addresses the topic of missing values in data and feature engineering. However, it is incomplete (cut off), which prevents a full assessment of whether all aspects of the query are covered.",
          "faithfulness": "The model answer is completely faithful to the retrieved context. It accurately expands on the snippet's mention of 'backfill the data' and 'join with external datasets' while adding logical, well-supported strategies (imputation and omission) that align with standard practices in data engineering.",
          "answer_quality": "The answer is clear, well-structured, and comprehensive in addressing the role of missing values. It covers key strategies and contextualizes their importance. However, it could be slightly improved by explicitly defining *why* missing values matter (e.g., impact on model performance) or addressing edge cases (e.g., when omission is inappropriate)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 17,
      "question": "What is the role of missing values in data and feature engineering?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:48:58",
      "answer": "**The role of missing values in data and feature engineering** includes the following aspects based on the provided context:\n\n- **Gathering more data** or **joining with external datasets** to address missing values. This can help in backfilling the data. [Source 1]\n- **Imputation** methods like **leveraging prior knowledge** or **model-based imputation** using techniques such as KNN, linear regression, or trees to predict missing data. [Source 1]\n- **Omission** of missing observations or features with a high number of missing values in the dataset. [Source 1]\n\nThese strategies help in handling missing values effectively as part of the data and feature engineering process.\n\nPlease note that these points are based on the information provided in the context from Source 1.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        }
      ],
      "response_time": 2.701723098754883,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 780,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document snippet is somewhat relevant to the query but is incomplete. It only mentions 'gathering more data' and 'joining with external datasets' as a way to handle missing values, but it cuts off and does not fully address the broader role of missing values in data and feature engineering. Key aspects like imputation methods, omission strategies, or the impact of missing values on model performance are implied but not explicitly covered in the provided context.",
          "faithfulness": "The answer is highly faithful to the retrieved context, as it directly references the snippet's mention of 'gathering more data' and 'joining with external datasets.' However, the answer introduces additional details (e.g., imputation methods like KNN, linear regression, or omission of missing observations) that are not explicitly present in the provided context, though they are reasonable and commonly associated with the topic. This results in minor unsupported additions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It addresses the query effectively by listing key strategies for handling missing values. However, it could be improved by explicitly noting the limitations of the provided context (e.g., the snippet is incomplete) and avoiding assumptions beyond the given information. The inclusion of additional imputation methods, while reasonable, slightly detracts from strict completeness based on the context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 17,
      "question": "What is the role of missing values in data and feature engineering?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:49:05",
      "answer": "Within data and feature engineering, **missing values** are a significant consideration. Various strategies can be employed to handle them:\n\n*   **Gather more data**: This involves trying to backfill the missing information or integrating external datasets to fill the gaps [Source 1].\n*   **Imputation**: This technique involves inferring missing values by using existing data and prior knowledge. Methods include **model-based imputation**, where non-missing data is used to predict missing data with techniques like KNN, linear regression, or trees [Source 1].\n*   **Omission**: This strategy involves removing observations or features that have a large number of missing values from the dataset [Source 1].\n\nThe general topics of \"Missing Values\" and \"Data and feature engineering\" are also listed as key areas in machine learning [Source 2].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        }
      ],
      "response_time": 1.640484094619751,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 846,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly addresses the topic of missing values in data and feature engineering. However, it is slightly truncated, missing some key details that could provide a more comprehensive answer (e.g., specific imputation methods or deeper explanations).",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, including the strategies for handling missing values (gathering more data, imputation, and omission), without adding unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively addresses the query by listing and briefly explaining strategies for handling missing values. However, it could be improved by elaborating further on imputation methods or providing examples to enhance comprehensiveness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 18,
      "question": "What challenges are associated with imbalanced data?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:49:26",
      "answer": "Here's what the documents say about the challenges of imbalanced data:\n\n**Imbalanced data** refers to datasets where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This issue affects both classification and regression tasks. [Source 1]\n\nHere are the key challenges and solutions:\n\n## Methods to Handle Imbalanced Data:\n\n1. **Resampling Methods**\n- Can modify the distribution of training data through:\n  - Over-sampling the minority class\n  - Under-sampling the majority class\n  - Generating synthetic examples (SMOTE)\n- **Pros**: Simple and fast\n- **Cons**: Risk of overfitting (over-sampling) or losing important information (under-sampling) [Source 1]\n\n2. **Model-level Methods**\n- Make models more robust by:\n  - Penalizing wrong classifications of minority class more heavily\n  - Selecting appropriate algorithms (like tree-based models)\n- **Pros**: Does not change the distribution of training data\n- **Cons**: Methods are specific to particular models [Source 2]\n\n## Evaluation Considerations:\n\n**Recommended Metrics**:\n- Precision-Recall curve (gives more importance to positive class)\n- Precision, recall, and F1 measure (for binary classification)\n\n**Metrics to Avoid**:\n- Accuracy (can be misleading with imbalanced classes)\n- AUC of ROC curve (treats classes equally and is less sensitive to improvements on minority class) [Source 2]\n\nThe documents emphasize that choosing appropriate evaluation metrics is crucial when dealing with imbalanced data to ensure proper assessment of model performance.",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        }
      ],
      "response_time": 12.47632384300232,
      "num_citations": 2,
      "num_retrieved_docs": 25,
      "answer_length": 1564,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document contains a small section explicitly mentioning 'Imbalanced Data,' but it is very brief and lacks depth. While it touches on a key aspect (model-level methods), it misses other critical challenges like evaluation biases, algorithmic biases, or data-level difficulties. Thus, the relevance is somewhat limited.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes additional details (e.g., resampling methods like SMOTE, evaluation metrics like Precision-Recall curve) that are not explicitly mentioned in the provided context. These additions are reasonable and relevant but are not directly supported by the retrieved document, leading to minor unsupported claims.",
          "answer_quality": "The answer is well-structured, clear, and addresses the query effectively by outlining challenges and solutions for imbalanced data. However, it could be slightly more comprehensive by explicitly listing challenges (e.g., evaluation bias, algorithmic limitations) rather than focusing primarily on solutions. The inclusion of evaluation metrics adds value but slightly detracts from the direct challenges."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 18,
      "question": "What challenges are associated with imbalanced data?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:49:36",
      "answer": "**Challenges associated with imbalanced data** include:\n- Significant computation time when the number of features is large [Source 1].\n- Potential to overfit the features to the training data [Source 1].\n- Risk of overfitting the training data with over-sampling methods and losing important information by under-sampling [Source 1].\n- Imbalanced datasets can lead to misleading results with accuracy metrics and AUC of the ROC curve [Source 2].\n- **Machine Learning models** like Linear Models and Decision Trees are sensitive to imbalanced data and can be prone to overfitting [Source 3].\n\nThese challenges highlight the importance of addressing imbalanced data through proper techniques like resampling, model-level methods, and selecting appropriate evaluation metrics to ensure the robustness and accuracy of the models [Source 1, Source 2].",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 2.3724260330200195,
      "num_citations": 3,
      "num_retrieved_docs": 25,
      "answer_length": 847,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document contains some relevant information about imbalanced data, specifically mentioning it as a challenge and briefly touching on model-level methods. However, it misses key aspects of the query, such as detailed challenges like biased model performance, poor generalization, and evaluation metric issues. The context is somewhat relevant but incomplete.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the mention of 'losing important information by under-sampling' and 'misleading results with accuracy metrics and AUC of the ROC curve' are not explicitly stated in the provided context. The answer also references sources (e.g., Source 2, Source 3) that are not part of the retrieved context.",
          "answer_quality": "The answer addresses the query and provides some relevant challenges, but it is incomplete and includes unsupported claims. The structure is clear, but the answer could be improved by focusing only on the information present in the retrieved context and expanding on the challenges more comprehensively."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 18,
      "question": "What challenges are associated with imbalanced data?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:49:45",
      "answer": "The primary challenge associated with imbalanced data is that **one or more labels make up the majority of the dataset, leaving far fewer examples of other labels**. This issue applies to both classification and regression tasks [Source 1].\n\nSpecifically, when dealing with imbalanced datasets:\n\n*   **Risk of Overfitting**:\n    *   **Over-sampling** methods, used to change the distribution of training data, carry a risk of overfitting the training data [Source 1].\n    *   **Under-sampling** methods risk losing important information by removing data [Source 1].\n\n*   **Misleading Evaluation Metrics**:\n    *   **Accuracy** can be misleading when classes are imbalanced [Source 2].\n    *   The **AUC of the ROC curve** treats both classes equally, making it less sensitive to model improvement on the minority class [Source 2].\n\nTo address these challenges, strategies include resampling the data, using model-level methods, and choosing appropriate evaluation metrics [Source 1, Source 2].",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        }
      ],
      "response_time": 1.7942469120025635,
      "num_citations": 2,
      "num_retrieved_docs": 25,
      "answer_length": 993,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses the topic of imbalanced data and mentions challenges like overfitting and the need for model-level methods. However, it does not cover all aspects of the query comprehensively (e.g., evaluation metrics are only briefly hinted at in the model answer but not explicitly in the retrieved context).",
          "faithfulness": "The answer is mostly faithful to the retrieved context, as it correctly references overfitting risks and resampling methods. However, it introduces additional details (e.g., misleading evaluation metrics like accuracy and AUC) that are not explicitly supported by the provided retrieved context, though these are generally accurate in the broader ML context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides specific challenges (overfitting, evaluation metrics) and solutions (resampling, model-level methods). However, it could be slightly more comprehensive by explicitly linking all points back to the retrieved context or expanding on other challenges (e.g., computational issues)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 19,
      "question": "In what ways can feature selection enhance model performance?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:50:04",
      "answer": "Based on the provided documents, I'll explain how feature selection can enhance model performance by discussing the different methods and their benefits.\n\n## Types of Feature Selection Methods\n\n1. **Intrinsic Feature Selection Methods** [Source 1]\n- Naturally embedded with training process of models like tree-based models and regularization models\n- **Pros**:\n  - Fast implementation with no external feature tools needed\n  - Direct connection between feature selection and object function\n- **Cons**:\n  - Model-dependent\n  - Limited choice of models\n\n2. **Filter Feature Selection Methods** [Source 1]\n- Selects features that correlate well with target variables\n- Uses univariate statistical analysis and feature importance scores\n- Process is performed once and is independent of model algorithm\n- **Cons**:\n  - Tends to select redundant features\n  - Ignores relationships among features\n\n3. **Wrapper Feature Selection Methods** [Source 2]\n- Uses an iterative process like sequential feature selection\n- Repeatedly adds subset features and evaluates model performance\n- **Pros**:\n  - Searches for a wider variety of feature subsets compared to other methods\n- **Cons**:\n  - Requires significant computation time with large feature sets\n  - Higher potential for overfitting features to training data\n\n## Benefits of Feature Selection\n\nThe documents indicate that feature selection serves as a **pre-processing step before actual learning** [Source 1] and helps by:\n- Selecting a subset of original features for model training\n- Potentially improving model performance by removing redundant or irrelevant features\n- Reducing computational complexity by working with fewer features\n\nIt's worth noting that some algorithms, like **Random Forest**, don't require feature selection as part of their advantages [Source 3], which can be beneficial when choosing modeling approaches.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 13.865774869918823,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1880,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it discusses feature selection methods and their role as a pre-processing step in model training. However, it does not comprehensively cover all possible ways feature selection can enhance model performance (e.g., noise reduction, interpretability improvements), which slightly limits its relevance.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately summarizes the methods and benefits of feature selection as described in the document, without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It effectively categorizes feature selection methods, highlights their pros and cons, and explains their benefits in a logical manner. The inclusion of a note about algorithms like Random Forest adds useful context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 19,
      "question": "In what ways can feature selection enhance model performance?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:50:14",
      "answer": "Feature selection can enhance model performance in several ways based on the provided context:\n\n1. **Intrinsic Feature Selection Methods**:\n   - Have feature selection integrated into the training process of models like tree-based and regularization models [Source 1].\n   - Pros:\n     - Fast and does not require external feature tools.\n     - Provides a direct link between feature selection and the objective function.\n\n2. **Filter Feature Selection Methods**:\n   - Select features that are highly correlated with target variables using methods like univariate statistical analysis and feature importance scores [Source 1].\n   - The process is model-independent and performed only once.\n   - Pros:\n     - Confusion Matrix, Accuracy, Precision, Recall, Specificity, F1 Score, ROC Curve, and Precision-Recall Curve are used for model-based imputation [Source 1].\n\n3. **Wrapper Feature Selection Methods**:\n   - Iteratively add subset features to the model and use model performance to guide the selection of the next subset [Source 2].\n   - Pros:\n     - Searches a broader range of feature subsets.\n   - Cons:\n     - Requires significant computation time for a large number of features.\n     - Has potential to overfit features to the training data.\n\nTherefore, feature selection methods like intrinsic, filter, and wrapper techniques play a crucial role in selecting relevant features, improving model efficiency, and enhancing model interpretability [Source 1] [Source 2].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 3.312080144882202,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1474,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document mentions intrinsic feature selection methods and briefly touches on feature selection as a pre-processing step, but it does not provide comprehensive details about filter or wrapper methods, nor does it explicitly discuss the ways feature selection enhances model performance. Key aspects of the query (e.g., specific benefits like reducing overfitting, improving accuracy, or computational efficiency) are missing or only indirectly addressed.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the explanation of filter methods (e.g., 'Confusion Matrix, Accuracy, Precision...') is not directly mentioned in the provided context, and the categorization of methods (intrinsic, filter, wrapper) is not fully grounded in the retrieved document. The answer also introduces terms like 'model-based imputation,' which are unrelated to the context.",
          "answer_quality": "The answer addresses the query and provides a structured breakdown of feature selection methods, but it includes inaccuracies and irrelevant details (e.g., model-based imputation). While the response is clear, it could be improved by focusing strictly on the retrieved context and avoiding unsupported claims. The completeness is adequate but not comprehensive."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 19,
      "question": "In what ways can feature selection enhance model performance?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:50:23",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about how feature selection can enhance model performance. They only define what feature selection is and list different methods of feature selection, along with their pros and cons.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, dec..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue ..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 12,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 13,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 14,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1,
          "text_snippet": "MACHINELEARNINGINTERVIEWCHEAT SHEET"
        },
        {
          "source": 15,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        }
      ],
      "response_time": 0.9873039722442627,
      "num_citations": 15,
      "num_retrieved_docs": 40,
      "answer_length": 290,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions feature selection methods and their role as a pre-processing step, which is somewhat relevant to the query. However, it does not explicitly address how feature selection enhances model performance, missing key aspects of the query.",
          "faithfulness": "The model answer accurately reflects the content of the retrieved document. It correctly states that the document does not provide information about how feature selection enhances model performance, only defining feature selection and listing methods.",
          "answer_quality": "The answer is clear, well-structured, and correctly identifies the limitations of the retrieved context. However, it could be slightly improved by acknowledging the partial relevance of the document (e.g., mentioning that feature selection is a pre-processing step, which is indirectly related to performance)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 20,
      "question": "Summarize the main differences between SVMs and Logistic Regression.",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:50:42",
      "answer": "Based on the provided documents, here are the key differences between SVMs and Logistic Regression:\n\n**Prediction & Probability Output**:\n- **SVM**: Uses a linear function to make predictions and can only predict class labels (not probabilities) [Source 1]\n- **Logistic Regression**: Uses a logistic function that gives a value between 0 and 1, which can be interpreted as a probability [Source 1]\n\n**Loss Function**:\n- **SVM**: Uses hinge loss as the loss function to measure prediction incorrectness [Source 2]\n- **Logistic Regression**: Uses cross-entropy loss to measure the difference between predicted probability distribution and observed distribution [Source 2]\n\n**Decision Boundary**:\n- **SVM**: \n  - Finds the separating hyperplane that maximizes the distance of the closest points to the margin\n  - Can learn non-linear decision boundaries using kernel tricks [Source 2]\n- **Logistic Regression**: Finds linear decision boundary that focuses on maximizing the likelihood of the data [Source 2]\n\n**Additional Characteristics**:\n- **SVM Advantages**:\n  - Effective in high dimensional spaces\n  - Can handle outliers and overlapping classes\n  - Memory efficient (depends on few support vectors)\n  - Versatile with different kernel functions [Source 3]\n\n- **Logistic Regression Advantages**:\n  - Easy to explain, implement and interpret\n  - Outputs probability of class membership [Source 3]\n\n**Limitations**:\n- **SVM**:\n  - Slow to train on large datasets\n  - Does not directly provide probability estimates [Source 3]\n- **Logistic Regression**:\n  - Prone to outliers\n  - Prone to overfitting in high-dimensional data [Source 3]",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 12.252895832061768,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1636,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about the differences between SVMs and Logistic Regression, covering key aspects like loss functions and prediction mechanisms. However, some details (e.g., decision boundary specifics, advantages/limitations) are either incomplete or incorrectly referenced in the provided context, suggesting minor gaps or misalignments.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the explanation of the decision boundary for SVM (e.g., kernel tricks) and the advantages/limitations of both models are not explicitly mentioned in the provided context. The loss function and prediction differences are accurately represented.",
          "answer_quality": "The answer is clear, well-structured, and covers most of the key differences between SVMs and Logistic Regression. However, it could be slightly improved by ensuring all claims are directly supported by the retrieved context and by refining the completeness of certain explanations (e.g., decision boundary details)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 20,
      "question": "Summarize the main differences between SVMs and Logistic Regression.",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:50:51",
      "answer": "**Differences between SVMs and Logistic Regression:**\n\n- **Prediction**:\n  - **SVM**:\n    - Uses a linear function for predictions.\n    - Provides real-value predictions.\n  - **Logistic Regression**:\n    - Utilizes a logistic function for predictions, giving values between 0 and 1 interpreted as probabilities.\n\n- **Loss Function**:\n  - **SVM**:\n    - Employs hinge loss to measure prediction correctness.\n  - **Logistic Regression**:\n    - Utilizes cross-entropy loss to measure the difference between predicted and observed distributions.\n\n- **Decision Boundary**:\n  - **SVM**:\n    - Finds a separating hyperplane maximizing margin distance.\n  - **Logistic Regression**:\n    - Establishes linear decision boundaries focusing on maximizing data likelihood.\n\n- **Assumptions**:\n  - **SVM**:\n    - Assumes a linear relationship between variables and the target.\n  - **Logistic Regression**:\n    - Does not make assumptions about the distribution of input variables.\n\n- **Interpretability**:\n  - **SVM**:\n    - Less interpretable due to complex decision boundaries.\n  - **Logistic Regression**:\n    - More interpretable based on feature importance [Source 1] [Source 2] [Source 3].\n\nThese key points highlight the main distinctions between Support Vector Machines (SVMs) and Logistic Regression based on the provided context.",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 3.183150053024292,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1324,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as loss functions and prediction mechanisms. However, the context is somewhat fragmented and incomplete (e.g., missing details on decision boundaries and assumptions), which prevents a perfect score.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly citing hinge loss for SVM and cross-entropy loss for Logistic Regression. However, it introduces minor unsupported details (e.g., 'linear function for predictions' for SVM and 'logistic function' for Logistic Regression) that are not explicitly mentioned in the context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the main differences effectively. It could be slightly more comprehensive (e.g., including kernel tricks for SVM or regularization details) to achieve a perfect score, but it is otherwise excellent."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 20,
      "question": "Summarize the main differences between SVMs and Logistic Regression.",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:51:02",
      "answer": "The main differences between Support Vector Machines (SVMs) and Logistic Regression can be summarized as follows:\n\n*   **Prediction Output**:\n    *   **SVMs** use a linear function to make predictions and primarily output class labels (e.g., 0 or 1, true or false) rather than probabilities [Source 1].\n    *   **Logistic Regression** uses a logistic function that outputs a value between 0 and 1, which can be interpreted as a probability of class membership [Source 1], [Source 3].\n\n*   **Loss Function**:\n    *   **SVMs** utilize **hinge loss** as their loss function, which measures the amount by which the model's prediction is incorrect [Source 2], [Source 5].\n    *   **Logistic Regression** employs **cross-entropy loss** as its cost function, measuring the difference between the predicted probability distribution and the observed distribution [Source 1], [Source 2], [Source 5].\n\n*   **Decision Boundary**:\n    *   **SVMs** aim to find a separating hyperplane that maximizes the distance of the closest points to the margin [Source 2]. They can also learn non-linear decision boundaries through the use of **kernel tricks** [Source 2].\n    *   **Logistic Regression** typically finds a linear decision boundary by maximizing the likelihood of the data, focusing on the distance from the data to the decision boundary [Source 2].\n\n*   **Sensitivity to Outliers**:\n    *   **SVMs** are generally able to handle outliers and overlapping classes [Source 3].\n    *   **Logistic Regression** is described as less sensitive to outliers compared to linear regression [Source 1], but it is prone to outliers because it maximizes the conditional likelihoods of the training data [Source 3].\n\n*   **Probabilistic Estimates**:\n    *   **SVMs** do not directly provide probability estimates [Source 3].\n    *   **Logistic Regression** outputs the probability of class membership [Source 3].\n\n*   **Overfitting**:\n    *   **Logistic Regression** can be prone to overfitting in high-dimensional data [Source 3], [Source 4].\n\nBoth are **parametric models** that learn a set of parameters from the dataset to make predictions and are **classification models** [Source 1].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE..."
        }
      ],
      "response_time": 2.4821889400482178,
      "num_citations": 5,
      "num_retrieved_docs": 40,
      "answer_length": 2165,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences between SVMs and Logistic Regression such as loss functions, prediction outputs, and decision boundaries. However, some aspects like sensitivity to outliers and overfitting are not as thoroughly addressed in the provided context, leading to a minor gap in comprehensiveness.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the differences mentioned (e.g., loss functions, prediction outputs). However, there are minor unsupported additions, such as the detailed explanation of kernel tricks in SVMs and the nuanced discussion on overfitting in Logistic Regression, which are not explicitly backed by the provided context.",
          "answer_quality": "The answer is excellent, clear, and well-structured. It comprehensively addresses the query by summarizing the main differences in a logical and easy-to-follow manner. The inclusion of multiple dimensions (e.g., loss functions, decision boundaries, sensitivity to outliers) ensures completeness, and the language is precise and informative."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 21,
      "question": "What are the advantages of using Logistic Regression over Linear Regression?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:51:15",
      "answer": "Based on the provided context, I can only partially answer this question. The documents don't directly compare the advantages of Logistic Regression over Linear Regression, but they do mention one key difference:\n\nThe main distinction is in their **use cases**:\n- **Linear Regression** is used when the target variable is continuous\n- **Logistic Regression** is used when the target variable is binary\n[Source 1]\n\nAdditionally, Logistic Regression has these general advantages:\n- **Easy to explain, implement and interpret**\n- Can provide **probability estimates** between 0 and 1\n[Source 1, Source 2]\n\nHowever, to fully answer what makes Logistic Regression advantageous over Linear Regression specifically, we would need additional context from the documents that directly compares these two algorithms' advantages and disadvantages.",
      "retrieved_docs": [
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        }
      ],
      "response_time": 6.135146141052246,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 835,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents touch on some differences between Logistic Regression and Linear Regression but do not comprehensively address the advantages of Logistic Regression over Linear Regression. Key aspects of the query, such as specific advantages like handling binary outcomes, probability estimates, or decision boundaries, are only partially covered or implied.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly identifying the use cases of both models and mentioning general advantages of Logistic Regression. However, the claim about 'probability estimates' is not explicitly supported by the provided context, though it is a known characteristic of Logistic Regression.",
          "answer_quality": "The answer is adequate but incomplete. It correctly identifies the primary use-case difference and mentions some general advantages of Logistic Regression. However, it lacks depth in comparing specific advantages over Linear Regression and explicitly notes the need for additional context, which limits its completeness and clarity."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 21,
      "question": "What are the advantages of using Logistic Regression over Linear Regression?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:51:25",
      "answer": "Based on the provided context from the PDF documents, the advantages of using **Logistic Regression** over **Linear Regression** are as follows:\n\n- **Logistic Regression** is **easy to explain, implement, and interpret**. [Source 1]\n- **Logistic Regression** is used when the target is **binary**, making it suitable for classification tasks, unlike **Linear Regression**, which is used when the target is continuous. [Source 2]\n- **Logistic Regression** uses a **logistic function** as the prediction function that gives a value between 0 and 1, which can be interpreted as a probability, whereas **Linear Regression** uses a linear function for predictions and can only predict class labels instead of probabilities. [Source 2]\n- **Logistic Regression** uses the **cross-entropy loss** as the cost function, which measures the difference between the predicted probability distribution and the observed distribution, while **Linear Regression** uses a different loss function. [Source 3]\n\nThese points highlight the advantages of **Logistic Regression** over **Linear Regression** based on the provided context.",
      "retrieved_docs": [
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs."
        }
      ],
      "response_time": 2.977513313293457,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1112,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain some relevant information about Logistic Regression and Linear Regression, such as their use cases (binary vs. continuous targets) and differences in prediction functions. However, the context is fragmented, incomplete, and does not directly address the *advantages* of Logistic Regression over Linear Regression in a structured or comprehensive way. Key aspects of the query (e.g., probabilistic outputs, handling of categorical data) are only partially covered.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported or generalized claims (e.g., 'easy to explain, implement, and interpret' is not explicitly stated in the context). The core points about binary targets, logistic functions, and cross-entropy loss align with the provided fragments, though the context is sparse. Minor additions (e.g., 'probabilistic outputs') are reasonable but not directly sourced.",
          "answer_quality": "The answer addresses the query and is structured logically, but it relies on incomplete or ambiguous context. Some points (e.g., 'easy to explain') are unsupported, and the explanation lacks depth due to the limitations of the retrieved documents. The response is adequate but could be improved with clearer sourcing and more comprehensive coverage of advantages."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 21,
      "question": "What are the advantages of using Logistic Regression over Linear Regression?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:51:32",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents compare Logistic Regression with SVMs, Random Forest, and discuss its general characteristics, but they do **not** contain information about the advantages of using Logistic Regression **over Linear Regression**. While [Source 1] differentiates between Linear and Logistic Regression, it does not state advantages of one over the other.",
      "retrieved_docs": [
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        }
      ],
      "response_time": 1.037876844406128,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 417,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents mention both Logistic Regression and Linear Regression but do not explicitly address the advantages of one over the other. The context is minimally connected to the query, as it only provides a superficial comparison without detailing advantages.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately states that the documents do not contain information about the advantages of Logistic Regression over Linear Regression and does not introduce unsupported claims.",
          "answer_quality": "The answer is excellent, clear, and well-structured. It directly addresses the limitation of the retrieved context and explains why the question cannot be answered based on the provided documents. The response is comprehensive and helpful in clarifying the gap."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 22,
      "question": "Why might one choose Random Forest over Logistic Regression?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:51:49",
      "answer": "Based on the provided documents, here are the key reasons to choose Random Forest over Logistic Regression:\n\n## Key Advantages of Random Forest\n\n- **Better Handling of Complex Data**:\n  - Random Forest can handle non-linear relationships through its ensemble of decision trees, while Logistic Regression is limited to linear relationships [Source 1]\n  - Doesn't require feature selection [Source 3]\n  - Less sensitive to outliers [Source 3]\n\n- **Feature Importance & Interpretation**:\n  - Generates feature importance which helps in interpreting results [Source 3]\n  - While both models offer interpretability, Random Forest provides insight through feature importance, whereas Logistic Regression relies on coefficient interpretation [Source 1]\n\n- **Minimal Parameter Tuning**:\n  - Doesn't require much parameter tuning [Source 3]\n  - Doesn't require extensive data pre-processing [Source 3]\n\n## Overfitting Considerations\nWhile Random Forest is more prone to overfitting than Logistic Regression if individual trees grow infinitely [Source 1], this can be mitigated because:\n- It has better generalization performance than individual decision trees due to randomness [Source 3]\n- It's less prone to overfitting as it averages the predictions of many different trees [Source 2]\n\n## Limitations to Consider\nFor balanced decision-making, it's worth noting that Random Forests are:\n- Computationally expensive [Source 3]\n- More complex in structure compared to Logistic Regression's simple linear model [Source 1]\n\nThe choice between these models might depend on your specific needs regarding computational resources, data complexity, and interpretability requirements.",
      "retrieved_docs": [
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 12.364329814910889,
      "num_citations": 3,
      "num_retrieved_docs": 25,
      "answer_length": 1667,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as the differences between Random Forest and Logistic Regression, their handling of overfitting, and structural differences. However, some details (e.g., feature importance, minimal parameter tuning) are not explicitly mentioned in the provided snippets, though they are logically inferred from the context.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about non-linearity, overfitting, and structural differences. Minor additions (e.g., feature importance, minimal parameter tuning) are reasonable extensions of the provided context but are not explicitly stated in the snippets.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It addresses the query thoroughly, highlights key advantages and limitations, and provides a balanced comparison. The inclusion of a 'Limitations to Consider' section adds depth and nuance."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 22,
      "question": "Why might one choose Random Forest over Logistic Regression?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:52:00",
      "answer": "**Why might one choose Random Forest over Logistic Regression?**\n\nOne might choose **Random Forest** over **Logistic Regression** for several reasons, based on the information from the provided sources:\n\n- **Overfitting**: Random Forest is **less prone to overfitting** compared to Logistic Regression. This is because Random Forest averages the predictions of many different trees, which helps in reducing overfitting [Source 2 - Page 8, Source 3 - Page 9].\n  \n- **Interpretability**: While Logistic Regression can be interpreted by the value and sign of the estimated coefficients, Random Forest offers interpretability based on **feature importance**, which can be more straightforward than interpreting the coefficients in Logistic Regression models [Source 1 - Page 10, Source 3 - Page 9].\n  \n- **Model Complexity**: Logistic Regression is a **simple, linear model** with few parameters, making it less complex compared to Random Forest, which is an ensemble of decision trees. The simplicity of Logistic Regression can be advantageous in scenarios where a complex model is not required [Source 1 - Page 10].\n\nConsidering these factors, one might choose Random Forest over Logistic Regression when dealing with scenarios where preventing overfitting, understanding feature importance, or managing model complexity are key considerations.",
      "retrieved_docs": [
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        }
      ],
      "response_time": 3.5023272037506104,
      "num_citations": 9,
      "num_retrieved_docs": 25,
      "answer_length": 1342,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents discuss the differences between Random Forest and other models (including Logistic Regression) in terms of structure, overfitting, and hyperparameters. While the documents do not explicitly compare Random Forest and Logistic Regression in all aspects, they cover key points like overfitting and model complexity, which are relevant to the query. However, some aspects like interpretability are not directly addressed in the provided context.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly highlighting overfitting and model complexity as key differences. However, the claim about interpretability (feature importance) is not explicitly supported by the provided context, though it is a generally known advantage of Random Forest. No contradictions are present.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively by listing key reasons to choose Random Forest over Logistic Regression. It could be slightly improved by explicitly noting which points are not directly supported by the retrieved context (e.g., interpretability)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 22,
      "question": "Why might one choose Random Forest over Logistic Regression?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:52:08",
      "answer": "One might choose **Random Forest** over **Logistic Regression** for several reasons, primarily due to differences in their structure, training process, and ability to handle various data characteristics:\n\n*   **Structure and Complexity**:\n    *   **Random Forest** is an ensemble of decision trees that work together to make predictions [Source 1]. This ensemble nature allows it to capture complex relationships in data.\n    *   **Logistic Regression** is a simpler, linear model used to predict the probability of a binary outcome [Source 1]. It makes linear assumptions between features and the target [Source 5].\n\n*   **Overfitting Tendencies**:\n    *   **Random Forest** is generally less prone to overfitting than individual decision trees because it averages the predictions of many different trees [Source 2, Source 3].\n    *   **Logistic Regression** can be prone to overfitting in high-dimensional data [Source 4, Source 7].\n\n*   **Interpretability**:\n    *   While **Logistic Regression** is easy to explain, implement, and interpret, based on the value and sign of estimated coefficients [Source 1, Source 4, Source 7], **Random Forest** can be interpreted based on feature importance [Source 1, Source 6]. However, interpreting Random Forest models based on feature importance is less straightforward compared to decision trees [Source 2].\n\n*   **Data Handling**:\n    *   **Random Forest** doesn't require much parameter tuning [Source 4] and doesn't require feature selection [Source 4, Source 6]. It is also less sensitive to outliers in the dataset [Source 4].\n    *   **Logistic Regression** maximizes the conditional likelihoods of the training data, which can make it prone to outliers [Source 3].\n\n*   **Decision Boundaries**:\n    *   **Random Forest**, being tree-based, can learn non-linear decision boundaries [Source 1].\n    *   **Logistic Regression** primarily finds linear decision boundaries [Source 1].\n\n*   **Generalization Performance**:\n    *   **Random Forest** often has better generalization performance than an individual decision tree due to its randomness [Source 4].\n\n*   **Computational Cost**:\n    *   It's worth noting that **Random Forest** can be computationally expensive [Source 4], whereas **Logistic Regression** is generally easier to implement [Source 3].",
      "retrieved_docs": [
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 2.865795135498047,
      "num_citations": 5,
      "num_retrieved_docs": 25,
      "answer_length": 2305,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as the structural differences, overfitting tendencies, interpretability, data handling, decision boundaries, generalization performance, and computational cost of Random Forest and Logistic Regression. The documents provide a comprehensive basis for comparing the two models.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information from the documents, including the ensemble nature of Random Forest, its resistance to overfitting, its ability to handle non-linear decision boundaries, and the trade-offs in interpretability and computational cost. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is excellent, clear, and comprehensive. It is well-structured, addressing all key reasons for choosing Random Forest over Logistic Regression in a logical and easy-to-follow manner. The inclusion of specific points like overfitting, interpretability, and data handling adds depth and clarity to the response."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 23,
      "question": "What are the main considerations in model comparison?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:52:25",
      "answer": "Based on the provided context, I can identify several key considerations for model comparison. Here are the main points:\n\n**Evaluation Metrics:**\n- **Precision, Recall, and F1 Score** are important for measuring model performance with respect to the positive class in binary classification [Source 1]\n- **Precision-Recall curve** is particularly useful for imbalanced data scenarios [Source 1]\n- **R-squared** is useful because it's:\n  - Easy to interpret\n  - Allows easy comparison between different models\n  However, it has limitations:\n  - Always increases when adding new variables\n  - Doesn't show true predictive power [Source 2]\n\n**Model-Specific Characteristics:**\n- When comparing **Decision Trees vs Random Forests**:\n  - Decision Trees are more prone to overfitting as depth increases\n  - Random Forests are more resistant to overfitting due to averaging multiple predictions\n  - Decision Trees have fewer hyperparameters to tune\n  - Decision Trees offer better interpretability through simple rules\n  - Random Forests provide feature importance but are less straightforward to interpret [Source 3]\n\n**Data Considerations:**\n- For **imbalanced datasets**, special attention must be paid to:\n  - Avoiding accuracy as a metric (can be misleading)\n  - Being cautious with ROC curves (less sensitive to minority class improvements) [Source 1]\n  - Using appropriate model-level methods that are robust to class imbalance [Source 1]\n\nThe documents suggest that model comparison should take into account both the specific characteristics of the models being compared and the nature of the data (such as class imbalance) while using appropriate evaluation metrics for the task at hand.",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 11.463773965835571,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1688,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as evaluation metrics (e.g., Precision-Recall curve, Precision, Recall, F1) and considerations for imbalanced data. However, some points in the answer (e.g., R-squared, Decision Trees vs. Random Forests) are not explicitly mentioned in the provided context, suggesting potential gaps or reliance on external knowledge.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes unsupported details. For example, the discussion of R-squared and the comparison between Decision Trees and Random Forests is not present in the provided context. The core metrics (Precision, Recall, F1, Precision-Recall curve) and imbalanced data considerations are correctly grounded.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a comprehensive overview of model comparison considerations, including metrics and data-specific factors. However, the inclusion of unsupported details slightly detracts from its completeness and precision."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 23,
      "question": "What are the main considerations in model comparison?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:52:33",
      "answer": "**Main considerations in model comparison** based on the provided context are:\n\n- **Evaluation metrics** such as precision, recall, F1 score, ROC curve, and Precision-Recall curve are crucial for comparing models. These metrics help assess a model's performance with respect to the positive class in binary classification, deal with imbalanced data, and avoid misleading results when classes are imbalanced ([Source 1], [Source 3]).\n  \n- **Feature selection** methods like univariate statistical analysis and feature importance scores play a significant role in model comparison by selecting features that correlate well with target variables ([Source 2]).\n\n- **Model-level methods** that make models more robust to class imbalance by penalizing wrong classifications of the minority class more or selecting appropriate algorithms, such as tree-based models, are important considerations in model comparison ([Source 3]).\n\n- **Consideration of overfitting**: Overfitting is a critical factor to assess when comparing models. Decision trees are more prone to overfitting compared to random forests due to the depth of the tree ([Source 4]).\n\n- **Interpretability**: The interpretability of models is essential for model comparison. Decision trees are more interpretable based on simple rules, while random forests offer interpretability based on feature importance, which is less straightforward compared to decision trees ([Source 4]).\n\nThese considerations are vital when comparing models to ensure the selection of the most suitable model for a specific task.",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 3.151215076446533,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 1561,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as evaluation metrics, feature selection, and model robustness. However, some points in the answer (e.g., overfitting and interpretability) are not explicitly mentioned in the provided context, suggesting minor gaps in direct relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, overfitting and interpretability are not directly referenced in the provided context, though they are reasonable additions. The core points (metrics, feature selection) align well with the context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query comprehensively. It could be slightly improved by explicitly noting which points are derived from the context versus general knowledge, but overall, it is high-quality and informative."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 23,
      "question": "What are the main considerations in model comparison?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:52:42",
      "answer": "The main considerations in model comparison include:\n\n*   **Evaluation metrics**: Various metrics are used to assess model performance, such as:\n    *   **Confusion Matrix** [Source 2]\n    *   **Accuracy** [Source 2]\n    *   **Precision** [Source 2, Source 3]\n    *   **Recall** [Source 2, Source 3]\n    *   **Specificity** [Source 2]\n    *   **F1 Score** [Source 2, Source 3]\n    *   **ROC Curve** and **Precision-Recall Curve** [Source 2, Source 3]\n    *   **Precision-Recall curve** is particularly useful for imbalanced data, as it prioritizes the positive class [Source 3].\n    *   **Accuracy** can be misleading with imbalanced classes [Source 3].\n    *   **AUC of the ROC curve** treats both classes equally, making it less sensitive to improvements in the minority class [Source 3].\n    *   For linear models, **R-squared** is easy to interpret but always increases with new variables and doesn't show predictive power [Source 4]. **Adjusted R-squared** incorporates the model's degree of freedom [Source 4].\n\n*   **Pros and Cons of ML Algorithms**: Understanding the advantages and disadvantages of different algorithms is crucial [Source 1, Source 3]. This includes comparing:\n    *   **Random Forest vs. Gradient Boosting** [Source 1]\n    *   **Decision Tree vs. Random Forest** [Source 1, Source 6] - For instance, Decision Trees are more prone to overfitting than Random Forests, which average predictions from multiple trees [Source 5]. Random Forests also have more hyperparameters [Source 5].\n    *   **Linear Regression vs. Logistic Regression** [Source 1, Source 6]\n    *   **SVMs vs. Logistic Regression** [Source 1]\n    *   **Logistic Regression vs. Random Forest** [Source 1]\n\n*   **Data and feature engineering**: This involves selecting features that correlate well with target variables, using methods like univariate statistical analysis and feature importance scores [Source 1, Source 2]. This process is performed once and is independent of the model algorithm [Source 2].\n\n*   **Missing Values** [Source 1]\n\n*   **Feature Selection** [Source 1]\n\n*   **Imbalanced Data**: This refers to datasets where one or more labels constitute the majority, leaving fewer examples for other labels. This issue affects both classification and regression tasks [Source 1, Source 5]. When dealing with imbalanced data, specific metrics like the **Precision-Recall curve** are more appropriate [Source 3]. Model-level methods can also make models more robust to class imbalance by penalizing misclassifications of the minority class or selecting suitable algorithms [Source 3].",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 4.355536937713623,
      "num_citations": 5,
      "num_retrieved_docs": 8,
      "answer_length": 2588,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as evaluation metrics, pros and cons of ML algorithms, and considerations like imbalanced data and feature engineering. However, some details (e.g., missing values, feature selection) are only briefly mentioned in the context, which slightly limits comprehensiveness.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the information about evaluation metrics, algorithm comparisons, and imbalanced data. However, there are minor unsupported additions (e.g., some algorithm comparisons like 'SVMs vs. Logistic Regression' are not explicitly detailed in the provided context).",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It addresses the query thoroughly, organizes information logically, and provides actionable insights (e.g., when to use specific metrics or algorithms). The inclusion of pros/cons and practical considerations enhances its quality."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 24,
      "question": "How does feature selection relate to the objective function of a model?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:52:58",
      "answer": "Based on the provided documents, I can explain the relationship between feature selection and the objective function, specifically focusing on intrinsic feature selection methods.\n\nAccording to [Source 1], intrinsic feature selection methods have feature selection naturally embedded within the model's training process. The key advantage is that these methods provide **a direct connection between feature selection and the object function**. This is seen in models like:\n- Tree-based models\n- Regularization models\n\nThe main pros of intrinsic feature selection methods are:\n- **Fast** implementation\n- **No external feature tool** is needed\n- **Direct connection** between feature selection and the objective function\n\nHowever, there are some limitations [Source 1]:\n- They are **model-dependent**\n- The **choice of models is limited**\n\nIt's worth noting that this is just one approach to feature selection. The documents also mention other methods like:\n- **Filter Feature Selection Methods**: Select features that correlate well with target variables through statistical analysis [Source 1]\n- **Wrapper Feature Selection Methods**: Use an iterative process that repeatedly adds subset features and evaluates model performance [Source 2]\n\nThe direct connection between feature selection and the objective function is particularly important because it allows the model to optimize both simultaneously during the training process, rather than treating feature selection as a separate preprocessing step.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 9.580353021621704,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1504,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document directly addresses the relationship between feature selection and the objective function, particularly focusing on intrinsic methods. It covers the key aspects of the query, including how feature selection is embedded in the training process and its connection to the objective function.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases and expands on the information provided in the document, without introducing unsupported claims or contradictions. All key points, including the pros, cons, and examples of intrinsic feature selection methods, are grounded in the retrieved context.",
          "answer_quality": "The answer is excellent in quality. It is clear, comprehensive, and well-structured, addressing the query thoroughly. The explanation of the relationship between feature selection and the objective function is detailed, and the additional context about other feature selection methods enhances the completeness of the response."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 24,
      "question": "How does feature selection relate to the objective function of a model?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:53:08",
      "answer": "**Feature selection** methods are connected to the **objective function** of a model in the following ways:\n\n1. **Intrinsic Feature Selection Methods**:\n   - Have feature selection integrated with the training process of models like tree-based models and regularization models.\n   - Provide a direct link between feature selection and the objective function of the model [Source 1].\n\n2. **Filter Feature Selection Methods**:\n   - Involve selecting features that are highly correlated with target variables, using methods like univariate statistical analysis and feature importance scores.\n   - The selection process is model-independent and performed only once, unrelated to the model algorithm [Source 1].\n\n3. **Wrapper Feature Selection Methods**:\n   - Utilize an iterative process where subset features are added to the model iteratively, using model performance to guide the selection of the next subset.\n   - Search for a broader range of feature subsets compared to other methods [Source 2].\n\nThese methods of feature selection play a crucial role in determining which features are relevant to the model's objective function by selecting the most impactful features that contribute to the model's performance.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 2.5225322246551514,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1215,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it discusses feature selection methods, including intrinsic methods that are embedded in the training process of models. However, it does not explicitly detail the relationship between feature selection and the objective function, which is a key aspect of the query. Thus, it covers most but not all aspects.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately describing intrinsic, filter, and wrapper feature selection methods. However, it introduces the idea of a 'direct link between feature selection and the objective function' without explicit support from the retrieved document, which is a minor unsupported addition.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively by explaining how different feature selection methods relate to the model's objective function. However, it could be slightly more comprehensive by explicitly defining the objective function and its direct connection to feature selection in more detail."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 24,
      "question": "How does feature selection relate to the objective function of a model?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:53:15",
      "answer": "Feature selection has a **direct connection** to the **objective function** of a model when using **Intrinsic Feature Selection Methods**. These methods have feature selection naturally embedded within the training process of models, such as tree-based models and regularization models [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.2314319610595703,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 297,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it discusses feature selection methods embedded within the training process of models, which directly relates to the objective function. However, it does not explicitly explain the relationship in depth, missing some key details about how feature selection interacts with the objective function.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, mentioning intrinsic feature selection methods and their integration with the training process without adding unsupported claims.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It could be slightly improved by elaborating on how feature selection specifically influences or relates to the objective function, but it is otherwise comprehensive and well-presented."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 25,
      "question": "What steps are involved in evaluating machine learning models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:53:39",
      "answer": "Based on the provided documents, here are the key steps and considerations for evaluating machine learning models:\n\n## Choosing Appropriate Metrics\n\n1. **Select evaluation metrics based on the type of problem:**\n\nFor **Classification**:\n- **Accuracy**: Percentage of predicted labels that match true labels\n- **Precision**: Percentage of accurate positives compared to total predicted positives\n- **Recall**: Percentage of positives claimed vs actual positives in data\n- **Specificity**: Percentage of negatives claimed vs actual negatives in data\n- **F1 Score**: Combines precision and recall (ranges from 0 to 1, where 1 is best)\n[Source 8, Source 11]\n\nFor **Regression**:\n- **Mean Squared Error (MSE)**\n  - Pros: Penalizes large errors, mathematically convenient\n  - Cons: Sensitive to outliers, hard to interpret due to squared units\n- **Mean Absolute Error (MAE)**\n  - Pros: Simple to interpret, less sensitive to outliers\n  - Cons: Not differentiable at 0, harder to optimize\n- **R-squared**\n  - Pros: Easy to interpret and compare models\n  - Cons: Always increases with new variables\n[Source 4, Source 6]\n\n## Special Considerations\n\n2. **For Imbalanced Data**:\n- Use **Precision-Recall curve** instead of ROC curve\n- Avoid using accuracy as it can be misleading\n- Consider metrics that give more importance to the positive class\n[Source 1]\n\n3. **Using Curves for Evaluation**:\n- **ROC Curve**:\n  - x-axis: False Positive Rate\n  - y-axis: True Positive Rate (Recall)\n- **Precision-Recall Curve**:\n  - x-axis: Recall\n  - y-axis: Precision\n  - Particularly useful for imbalanced datasets\n[Source 3]\n\n4. **Model-Specific Evaluation**:\n- Consider model-specific strengths and weaknesses\n- For example, tree-based models handle missing data better than linear models\n- Consider computational efficiency and training time requirements\n[Source 2]\n\nNote: The evaluation process should be aligned with the specific problem, data characteristics, and business requirements. The choice of metrics can significantly impact how we interpret model performance.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        }
      ],
      "response_time": 14.834811925888062,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2052,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document snippet is somewhat fragmented but does mention key aspects related to evaluating machine learning models, such as evaluation metrics, imbalanced data, and model comparison. However, it does not comprehensively cover all steps involved in the evaluation process (e.g., train-test split, cross-validation, or bias-variance tradeoff). Thus, it is highly relevant but misses some key details.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It expands on the mentioned evaluation metrics (e.g., classification and regression metrics) and special considerations (e.g., imbalanced data) without introducing unsupported claims or contradictions. The answer also explicitly notes where details are inferred or generalized, ensuring groundedness.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It addresses the query thoroughly by breaking down evaluation steps into logical categories (metrics, special considerations, curves, and model-specific evaluation). The inclusion of pros/cons and practical advice (e.g., avoiding accuracy for imbalanced data) adds depth and utility."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 25,
      "question": "What steps are involved in evaluating machine learning models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:53:49",
      "answer": "To evaluate **machine learning models**, the following steps are involved based on the provided context:\n\n1. **Evaluation Metrics**:\n   - Choose appropriate evaluation metrics for the task, considering:\n     - Precision-Recall curve for imbalanced data.\n     - Metrics like precision, recall, and F1 score for binary classification [Source 1].\n     - Avoid using accuracy when dealing with imbalanced classes [Source 1].\n     - Consider AUC of the ROC curve for a balanced view of model performance [Source 1].\n\n2. **Feature Selection**:\n   - Select features that correlate well with target variables using methods like univariate statistical analysis and feature importance scores [Source 2].\n   - Implement intrinsic feature selection methods that are naturally embedded in the training process of models, such as tree-based models and regularization models [Source 2].\n   - Utilize filter feature selection methods like model-based imputation and omission to handle missing values in the dataset [Source 2].\n   - Employ wrapper feature selection methods, like sequential feature selection, to iteratively add subset features to the model for better performance [Source 6].\n\n3. **Model Comparison**:\n   - Compare different machine learning models to select the most suitable one for the task [Source 3].\n   - Consider the pros and cons of linear models, tree-based models, clustering models, and other algorithms for model selection [Source 4].\n\n4. **Loss Functions**:\n   - Understand and utilize various loss functions like Cross-Entropy, Hinge Loss, Mean Squared Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE) for evaluating model performance [Source 3, Source 4, Source 15].\n\n5. **Imbalanced Data Handling**:\n   - Address challenges related to imbalanced datasets by selecting appropriate evaluation metrics and feature selection methods [Source 6].\n   \n6. **ROC Curve and Precision-Recall Curve**:\n   - Utilize ROC curve and Precision-Recall curve to evaluate binary classification models, considering factors like false positive rate, recall, precision, and the area under the curves [Source 5].\n\nThese steps encompass the key aspects involved in evaluating **machine learning models** as outlined in the provided context.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versat..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset."
        }
      ],
      "response_time": 4.853932857513428,
      "num_citations": 6,
      "num_retrieved_docs": 40,
      "answer_length": 2256,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document contains relevant information about evaluating machine learning models, including feature selection, evaluation metrics, and handling imbalanced data. However, it is fragmented and lacks a structured overview of the entire evaluation process, missing some key steps like data splitting, cross-validation, and hyperparameter tuning.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the information about evaluation metrics, feature selection, and imbalanced data handling. However, it includes some details (e.g., specific loss functions like Hinge Loss, RMSE, MAE) that are not explicitly mentioned in the provided context, which slightly reduces the groundedness.",
          "answer_quality": "The answer is well-structured, clear, and covers most of the relevant aspects of model evaluation based on the retrieved context. However, it could be improved by explicitly noting the limitations of the provided context (e.g., missing steps like cross-validation) and avoiding minor unsupported additions."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 25,
      "question": "What steps are involved in evaluating machine learning models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:53:59",
      "answer": "Evaluating machine learning models involves several steps, primarily centered around selecting appropriate **evaluation metrics** and understanding their implications.\n\n1.  **Choose Appropriate Evaluation Metrics**: The selection of metrics is crucial for the task at hand.\n    *   For **imbalanced data**, the **Precision-Recall curve** is often preferred as it gives more importance to the positive class. [Source 1]\n    *   **Precision, recall, and F1 score** measure a model's performance with respect to the positive class in binary classification problems. [Source 1]\n    *   Metrics to **avoid** with imbalanced data include **accuracy** (which can be misleading) and **AUC of the ROC curve** (which treats both classes equally and is less sensitive to minority class improvement). [Source 1]\n\n2.  **Understand Specific Metrics**:\n    *   **Confusion Matrix**: Provides a detailed breakdown of correct and incorrect classifications. [Source 2]\n    *   **Accuracy**: Represents the percentage of predicted labels that match true labels. [Source 5]\n    *   **Precision**: The percentage of accurate positives the model claims compared to the total number of positives it claims. [Source 5]\n    *   **Recall** (also known as True Positive Rate or TPR): The percentage of positives the model claims compared to the actual number of positives in the data. [Source 8, Source 11]\n    *   **Specificity**: The percentage of negatives the model claims compared to the actual number of negatives in the data. [Source 5]\n    *   **F1 Score**: A measure that combines precision and recall, with 0 being the worst and 1 being the best. [Source 2, Source 11]\n    *   **ROC Curve**: Plots the True Positive Rate (Recall) against the False Positive Rate. Its Area Under the Curve (AUC) is a systematic metric for comparing binary classification models, independent of the threshold. [Source 2, Source 11]\n    *   **Precision-Recall Curve**: Plots Precision against Recall. It emphasizes predictions the model got right out of the total number it predicted to be positive, making it useful for imbalanced datasets. [Source 2, Source 11]\n\n3.  **Regression Evaluation Metrics**:\n    *   **Mean Squared Error (MSE)**: Measures the average squared distance between predictions and true values. It penalizes large errors and is mathematically convenient but sensitive to outliers and hard to interpret due to squared units. [Source 4, Source 15]\n    *   **Mean Absolute Error (MAE)**: Measures the average absolute difference between predictions and true values. It is simpler to interpret and less sensitive to outliers compared to MSE but is not differentiable at 0. [Source 4, Source 15]\n    *   **R-squared**: Describes the percentage of the target variable variation explained by the model. It is easy to interpret and compare models but always increases with new variables and doesn't show predicting power. [Source 4, Source 6]\n    *   **Adjusted R-squared**: Incorporates the model's degree of freedom. [Source 4]\n    *   **Root Mean Square Error (RMSE)**: The square root of MSE. [Source 6]\n\nThis comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques in Machine Learning, including evaluation metrics. [Source 3]",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved."
        }
      ],
      "response_time": 3.951296091079712,
      "num_citations": 6,
      "num_retrieved_docs": 40,
      "answer_length": 3249,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it focuses on evaluation metrics for machine learning models, which is a core aspect of model evaluation. However, it does not cover all steps involved in evaluating ML models (e.g., data splitting, cross-validation, or model comparison techniques), which slightly limits its comprehensiveness.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately extracts and explains the evaluation metrics mentioned in the document without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and comprehensive in covering evaluation metrics. However, it could be improved by addressing other key steps in model evaluation (e.g., train-test split, cross-validation) to fully answer the query."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 26,
      "question": "What are some examples of intrinsic feature selection methods?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:54:14",
      "answer": "Based on the provided context, I can only partially answer this question. The documents mention that **intrinsic feature selection methods** have feature selection naturally embedded within the training process, and specifically mentions two types:\n\n- Tree-based models\n- Regularization models\n\n[Source 1] states that intrinsic feature selection methods have the following characteristics:\n- Feature selection is naturally embedded with the training process\n- They are **fast and no external feature tool is needed**\n- They provide **a direct connection between feature selection and the object function**\n\nThe main disadvantages are:\n- They are **model-dependent**\n- The **choice of models is limited**\n\n[Source 1]\n\nWhile the documents mention that these methods exist, they don't provide specific examples beyond tree-based and regularization models. To provide a complete answer with more specific examples of intrinsic feature selection methods, additional context would be needed.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 7.9931321144104,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 985,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses intrinsic feature selection methods, mentioning their integration with the training process and listing some characteristics. However, it is incomplete in providing specific examples beyond tree-based and regularization models, which slightly limits its relevance.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, including the characteristics and disadvantages of intrinsic feature selection methods, without adding unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively communicates the key points from the retrieved context and acknowledges its limitations. However, it could be improved by explicitly stating that the document itself did not provide more examples (rather than implying it might exist elsewhere)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 26,
      "question": "What are some examples of intrinsic feature selection methods?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:54:24",
      "answer": "**Examples of Intrinsic Feature Selection Methods**:\n\n1. **Feature Selection Methods** that are usually embedded within the training process of models include:\n   - **Tree-based models**\n   - **Regularization models** [Source 1].\n\n2. **Filter Feature Selection Methods**:\n   - **Select features** that correlate well with target variables using methods like univariate statistical analysis and feature importance scores [Source 1].\n\n3. **Model-based imputation**:\n   - **Use non-missing data to predict missing data** with techniques like KNN, linear regression, or trees [Source 1].\n\n4. **Omission**:\n   - **Removing missing observations** or features with lots of missing values in the dataset [Source 1].\n\nThese methods provide ways to **select a subset of original features for model training** as part of feature selection [Source 1].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 3.279111862182617,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 839,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 2
        },
        "explanations": {
          "relevance": "The retrieved document contains relevant information about intrinsic feature selection methods, such as tree-based models and regularization models, which are mentioned in the answer. However, the document is fragmented and incomplete, missing key examples (e.g., forward selection, backward selection) that are typically associated with intrinsic methods. Some irrelevant topics (e.g., missing value imputation, omission) are also included in the answer, which are not intrinsic feature selection methods.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported or misclassified details. For example, 'filter feature selection methods' and 'model-based imputation' are not intrinsic methods, and their inclusion contradicts the definition of intrinsic feature selection. The answer also omits some intrinsic methods (e.g., cross-validation-based feature selection) mentioned in the context.",
          "answer_quality": "The answer addresses the query but is poorly structured and lacks clarity. It mixes intrinsic methods with non-intrinsic methods (e.g., filter methods, imputation) and does not clearly distinguish between them. The explanation is incomplete and could confuse the user about what constitutes an intrinsic feature selection method."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 26,
      "question": "What are some examples of intrinsic feature selection methods?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:54:32",
      "answer": "**Intrinsic Feature Selection Methods** have feature selection naturally embedded within the training process of models. Examples include:\n\n*   **Tree-based models** [Source 1]\n*   **Regularization models** [Source 1]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.3211419582366943,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 217,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it explicitly mentions 'Intrinsic Feature Selection Methods' and provides partial examples (e.g., forward selection, backward selection). However, it cuts off before listing all possible examples, so it does not fully cover the topic.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes 'Regularization models' and 'Tree-based models,' which are not explicitly mentioned in the provided snippet. While these are valid examples of intrinsic methods, they are not directly supported by the given context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. However, it could be slightly improved by including the partial examples (e.g., forward/backward selection) mentioned in the retrieved context for completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 27,
      "question": "How does imbalanced data affect machine learning model training?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:54:49",
      "answer": "Based on the provided documents, here's a comprehensive explanation of how imbalanced data affects machine learning model training:\n\n## Definition\n**Imbalanced data** refers to datasets where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This issue affects both classification and regression tasks. [Source 2]\n\n## Problems with Evaluation\n- **Accuracy** is misleading when classes are imbalanced\n- **AUC of ROC curve** treats both classes equally and is less sensitive to model improvement on minority class [Source 1]\n\n## Better Evaluation Metrics\n- **Precision-Recall curve** is more appropriate because it:\n  - Gives more importance to the positive class\n  - Is helpful for dealing with imbalanced data\n  - Puts emphasis on predictions the model got right out of total positive predictions [Source 1, Source 11]\n\n## Solutions to Handle Imbalanced Data\n1. **Resampling Methods**:\n   - Change the distribution of training data to reduce class imbalance through:\n     - Over-sampling the minority class\n     - Under-sampling the majority class\n     - Generating synthetic examples (SMOTE)\n   \n   Pros:\n   - Simple and fast\n   \n   Cons:\n   - Risk of overfitting (over-sampling)\n   - Risk of losing important information (under-sampling)\n   [Source 2]\n\n2. **Model-level Methods**:\n   - Make models more robust by:\n     - Penalizing wrong classifications of minority class more heavily\n     - Selecting appropriate algorithms (e.g., tree-based models)\n   \n   Pros:\n   - Does not change the distribution of training data\n   \n   Cons:\n   - Methods are model specific\n   [Source 1]\n\nThe documents emphasize the importance of choosing appropriate evaluation metrics and implementing specific techniques to address imbalanced data problems in machine learning applications.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        }
      ],
      "response_time": 12.696263074874878,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1819,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of imbalanced data in machine learning, including its definition, evaluation challenges, and solutions. However, the context provided is incomplete (e.g., cut off mid-sentence), which slightly limits its comprehensiveness.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases and organizes the information from the documents without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It addresses the query thoroughly, explains key concepts, and provides actionable solutions while maintaining logical flow and readability."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 27,
      "question": "How does imbalanced data affect machine learning model training?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:54:59",
      "answer": "Imbalanced data can significantly impact machine learning model training in various ways, as discussed in the provided context:\n\n- **Imbalanced Data**: Refers to a dataset where one or more labels dominate the dataset, leading to far fewer examples of other labels [Source 1].\n  \n### Effects of Imbalanced Data on Machine Learning Model Training:\n1. **Challenges**: Imbalanced data poses challenges for both classification and regression tasks [Source 1].\n  \n2. **Handling Imbalanced Data**:\n   - **Resampling**: Altering the distribution of training data to reduce class imbalance through methods like over-sampling, under-sampling, and generating synthetic examples like SMOTE [Source 1].\n  \n3. **Implications**:\n   - **Pros**:\n     - **Simple and fast** approach to handling imbalanced data [Source 1].\n   - **Cons**:\n     - Risk of **overfitting** training data with over-sampling and loss of important information with under-sampling [Source 1].\n\nImbalanced data affects model training by requiring specific handling techniques to prevent biases towards the majority class and ensure accurate predictions in the presence of class imbalances [Source 1].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        }
      ],
      "response_time": 3.5653328895568848,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 1157,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses imbalanced data and its implications for machine learning models. However, it does not fully cover all aspects of how imbalanced data affects training (e.g., bias toward majority class, performance metrics issues), as it is cut off mid-sentence.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately paraphrases and cites the provided document without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively summarizes the effects of imbalanced data and handling techniques. However, it could be improved by addressing additional implications (e.g., bias, metric distortions) that are not covered in the truncated context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 27,
      "question": "How does imbalanced data affect machine learning model training?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:55:08",
      "answer": "Imbalanced data can significantly affect machine learning model training. An **imbalanced dataset** is characterized by one or more labels making up the majority of the dataset, leaving far fewer examples for other labels. This issue applies to both classification and regression tasks [Source 2].\n\nHere's how imbalanced data affects model training and evaluation:\n\n*   **Misleading Accuracy Metric**: When classes are imbalanced, **accuracy** can be a misleading evaluation metric because it treats both classes equally [Source 1]. A model might achieve high accuracy by simply predicting the majority class, while performing poorly on the minority class.\n*   **Insensitive AUC of ROC Curve**: The **AUC of the ROC curve** also treats both classes equally, making it less sensitive to improvements in the model's performance on the minority class [Source 1].\n*   **Risk of Overfitting with Resampling**:\n    *   **Over-sampling** the minority class to balance the dataset carries the risk of overfitting the training data [Source 2].\n    *   **Under-sampling** the majority class can lead to the loss of important information by removing data [Source 2].\n\nTo address the challenges posed by imbalanced data, certain strategies and metrics are recommended:\n\n*   **Model-level Methods**: Make the model more robust to class imbalance by **penalizing wrong classifications of the minority class more** or by selecting **appropriate algorithms** (e.g., tree-based models) [Source 1].\n*   **Resampling**: Change the distribution of the training data to reduce class imbalance. Methods include:\n    *   **Over-sampling** the minority class [Source 2].\n    *   **Under-sampling** the majority class [Source 2].\n    *   Generating **synthetic examples** like **SMOTE** (Synthetic Minority Over-sampling Technique) [Source 2].\n*   **Appropriate Evaluation Metrics**: Choose evaluation metrics that give more importance to the positive (minority) class:\n    *   **Precision-Recall curve**: This curve gives more importance to the positive class and is helpful for dealing with imbalanced data [Source 1, Source 11]. It emphasizes predictions the model got right out of the total number it predicted to be positive (precision) [Source 3].\n    *   **Precision, Recall, and F1 measure**: These metrics assess a model's performance specifically with respect to the positive class in a binary classification problem [Source 1].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        }
      ],
      "response_time": 3.3198280334472656,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2413,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly addresses the concept of imbalanced data and its implications for machine learning model training. It provides foundational context about imbalanced datasets and mentions model-level methods to mitigate the issue, which aligns well with the query's focus.",
          "faithfulness": "The answer is largely faithful to the retrieved context, accurately reflecting the key points about imbalanced data, its effects on model training, and mitigation strategies. However, some details (e.g., specific evaluation metrics like Precision-Recall curve and F1 measure) are not explicitly mentioned in the provided snippet of the retrieved document, though they are logically consistent with the broader topic.",
          "answer_quality": "The answer is excellent: clear, comprehensive, and well-structured. It addresses the query thoroughly by explaining the impact of imbalanced data, its effects on evaluation metrics, and strategies to mitigate the issue. The response is logically organized and provides actionable insights."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 28,
      "question": "What are some methods for handling missing values in datasets?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:55:22",
      "answer": "Based on the provided context, here are the methods for handling missing values in datasets:\n\n## Main Approaches\n\n1. **Gather More Data**\n- Try to backfill the data\n- Join with external datasets\n[Source 1]\n\n2. **Imputation Methods**\n- Use non-missing data to predict missing values through:\n   - KNN (K-Nearest Neighbors)\n   - Linear regression\n   - Trees\n[Source 1]\n\n3. **Omission**\n- Remove missing observations\n- Remove features that have lots of missing values in the dataset\n[Source 1]\n\nIt's worth noting that some algorithms, like **Gradient Boosting**, can handle missing data naturally without requiring imputation [Source 2].\n\n## Key Consideration\nThe approach should leverage our prior knowledge of the existing data when using imputation techniques [Source 1].\n\nI've included all methods explicitly mentioned in the provided documents. While there might be other methods for handling missing values, I've only included those specifically mentioned in the source materials.",
      "retrieved_docs": [
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Fe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 8.197224140167236,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 983,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is somewhat relevant but misses key aspects of the query. It only covers model-based imputation and omission, which are two common methods, but does not mention other widely used techniques like mean/median/mode imputation, forward/backward fill, or advanced methods like MICE (Multiple Imputation by Chained Equations).",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. The methods of imputation (KNN, linear regression, trees) and omission are correctly cited. However, the mention of 'Gather More Data' and 'Gradient Boosting' is not supported by the provided context, which only references [Source 1] and implies [Source 2] is not part of the retrieved documents.",
          "answer_quality": "The answer is adequate but could be improved in completeness and clarity. It correctly lists the methods mentioned in the context but includes additional unsupported methods ('Gather More Data' and 'Gradient Boosting'). The structure is clear, but the inclusion of unsupported information slightly detracts from the overall quality."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 28,
      "question": "What are some methods for handling missing values in datasets?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:55:32",
      "answer": "To handle missing values in datasets, some methods include:\n\n- **Imputation**:\n  - Infer the missing values by leveraging prior knowledge of the existing data [Source 1].\n  - Gather more data: Backfill the data or join with external datasets [Source 1].\n\n- **Model-based imputation**:\n  - Use non-missing data to predict missing data with techniques like KNN, linear regression, or trees [Source 1].\n\nThese methods aim to address missing values in the dataset by either inferring them based on existing data or utilizing models to predict missing values.",
      "retrieved_docs": [
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Fe..."
        }
      ],
      "response_time": 2.607590913772583,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 554,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, covering key methods like model-based imputation and omission (though omission is mentioned in the context but not fully expanded in the answer). It addresses the core aspects of the query but misses some common methods (e.g., mean/median/mode imputation, flagging missing values).",
          "faithfulness": "The answer is completely faithful to the retrieved context. It directly references the methods mentioned in the document (model-based imputation, imputation via prior knowledge, and gathering more data) without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is adequate and covers some key methods, but it could be improved in completeness and clarity. It omits other common techniques (e.g., omission, which is hinted at in the context) and lacks depth in explaining the methods. The structure is clear but not comprehensive."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 28,
      "question": "What are some methods for handling missing values in datasets?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:55:39",
      "answer": "Methods for handling missing values in datasets include:\n\n*   **Gathering more data**: This involves looking for ways to backfill the missing data or joining with external datasets. [Source 1]\n*   **Imputation**: This technique infers missing values by using prior knowledge of existing data. [Source 1]\n    *   **Model-based imputation**: This uses non-missing data to predict missing data. Examples of models that can be used for this purpose include K-Nearest Neighbors (KNN), linear regression, and trees. [Source 1]\n*   **Omission**: This method involves removing observations that have missing values or features where a significant number of values are missing from the dataset. [Source 1]",
      "retrieved_docs": [
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Fe..."
        }
      ],
      "response_time": 1.572402000427246,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 696,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, covering key methods like model-based imputation and omission. However, it misses some common methods (e.g., mean/median/mode imputation, indicator variables) that could have made the retrieval more comprehensive.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the methods mentioned (model-based imputation and omission). The addition of 'Gathering more data' and a broad definition of 'Imputation' is not explicitly contradicted by the context but is not directly supported by the retrieved snippet either, hence the minor deduction.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively explains the methods mentioned in the retrieved context and adds minor context (e.g., 'Gathering more data') that is reasonable but not strictly necessary. A perfect score would require covering all common methods or explicitly noting the limitations of the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 29,
      "question": "How does Gradient Boosting differ from Random Forest in terms of model performance?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:55:56",
      "answer": "Based on the provided documents, here are the key differences between Gradient Boosting and Random Forest in terms of model performance:\n\n## Training Process\n- **Random Forest**: \n  - Trees are trained independently [Source 1]\n  - Can be trained in parallel [Source 1]\n  - Requires less parameter tuning [Source 2]\n\n- **Gradient Boosting**:\n  - Trees are trained sequentially to correct errors made by previous trees [Source 1]\n  - Cannot be trained in parallel [Source 1]\n  - More hyperparameters to tune [Source 1]\n\n## Performance Characteristics\n- **Random Forest**:\n  - Less prone to overfitting due to averaging predictions of many different trees [Source 1]\n  - Better generalization performance than individual decision trees [Source 2]\n  - Less sensitive to outliers [Source 2]\n\n- **Gradient Boosting**:\n  - Produces very accurate models, outperforming random forest in accuracy [Source 2]\n  - More prone to overfitting as it's trained sequentially [Source 1]\n  - Can continue to fit the data until it is perfectly fit [Source 1]\n\n## Practical Considerations\n- **Random Forest**:\n  - Doesn't require much parameter tuning [Source 2]\n  - Doesn't require feature selection [Source 2]\n  - Generates feature importance for result interpretation [Source 2]\n  - Computationally expensive [Source 2]\n\n- **Gradient Boosting**:\n  - No data pre-processing required [Source 2]\n  - Handles missing data without imputation [Source 2]\n  - Computationally expensive - often requires many trees (>1000) [Source 2]\n  - Sacrifices interpretability for accuracy [Source 2]\n  - Training can be slow due to sequential process [Source 2]",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 12.542500734329224,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1623,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences between Gradient Boosting and Random Forest in terms of model performance. However, the context is slightly fragmented and incomplete (e.g., cut-off text), which prevents a perfect score.",
          "faithfulness": "The answer is completely faithful to the retrieved context. All claims are directly supported by the provided documents, with no unsupported additions or contradictions.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It systematically addresses the query by comparing both algorithms across training processes, performance characteristics, and practical considerations, while maintaining high readability."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 29,
      "question": "How does Gradient Boosting differ from Random Forest in terms of model performance?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:56:06",
      "answer": "**Gradient Boosting** and **Random Forest** are both ensemble learning methods, but they differ in terms of model performance:\n\n- **Gradient Boosting**:\n  - **Pros**:\n    - Produces very accurate models and outperforms random forest in accuracy [Source 2].\n    - No data pre-processing is required [Source 2].\n    - Handles missing data without imputation [Source 2].\n  - **Cons**:\n    - It is a sequential process that can be slow to train [Source 2].\n    - Computationally expensive, often requiring many trees (>1000) which can be time and memory exhaustive [Source 2].\n    - Sacrifices interpretability for accuracy and is less interpretable in nature [Source 2].\n\n- **Random Forest**:\n  - **Pros**:\n    - Less prone to overfitting as it averages the predictions of many different trees [Source 1].\n    - Can be more efficient to train compared to Gradient Boosting [Source 1].\n  - **Cons**:\n    - More prone to overfitting, especially if individual trees are allowed to grow infinitely [Source 3].\n    - Sacrifices some interpretability compared to Gradient Boosting [Source 3].\n\nIn summary, **Gradient Boosting** tends to be more accurate but computationally expensive and less interpretable, while **Random Forest** is less prone to overfitting, more efficient to train, but sacrifices some interpretability compared to Gradient Boosting. [Source 1], [Source 2], [Source 3].",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 3.9114978313446045,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1381,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly compares Gradient Boosting and Random Forest, focusing on their performance differences. However, the document is incomplete (cut off), which slightly limits its comprehensiveness.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the pros and cons of both methods as described. However, it includes some unsupported claims (e.g., 'less interpretable in nature' for Gradient Boosting and 'sacrifices some interpretability compared to Gradient Boosting' for Random Forest), which are not explicitly mentioned in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a balanced comparison of both methods. However, it could be slightly more complete by elaborating on why Gradient Boosting tends to be more accurate or how Random Forest avoids overfitting in more detail."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 29,
      "question": "How does Gradient Boosting differ from Random Forest in terms of model performance?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:56:13",
      "answer": "Gradient Boosting and Random Forest are both ensemble learning methods that combine predictions from multiple models [Source 1]. However, they differ in their training process and how they handle model performance:\n\n**Gradient Boosting:**\n*   **Accuracy:** Produces very accurate models and often **outperforms Random Forest in accuracy** [Source 2].\n*   **Overfitting:** More prone to overfitting as it's trained sequentially and can continue to fit the data until it is perfectly fit [Source 1].\n*   **Training:** Each learner is trained sequentially to correct errors made by the previous tree [Source 1]. This sequential process can be slow to train [Source 2].\n*   **Computational Cost:** Computationally expensive, often requiring many trees (e.g., >1000), which can be time and memory intensive [Source 2].\n*   **Interpretability:** Sacrifices interpretability for accuracy, being less interpretive in nature [Source 2].\n*   **Hyperparameters:** Has more hyperparameters compared to Random Forest [Source 1].\n*   **Parallelization:** Cannot be trained in parallel [Source 1].\n\n**Random Forest:**\n*   **Accuracy:** Has better generalization performance than an individual decision tree due to randomness [Source 2].\n*   **Overfitting:** Less prone to overfitting as it averages the predictions of many different trees [Source 1, Source 4].\n*   **Training:** Each learner is trained independently [Source 1].\n*   **Computational Cost:** Can be computationally expensive [Source 2].\n*   **Interpretability:** Generates feature importance, which is helpful for interpreting results [Source 2]. Models can be interpreted based on feature importance, though this is less straightforward than decision trees [Source 4, Source 5].\n*   **Hyperparameters:** Has fewer hyperparameters [Source 1].\n*   **Parallelization:** Can be trained in parallel [Source 1].",
      "retrieved_docs": [
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrain..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 2.110759735107422,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1856,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences between Gradient Boosting and Random Forest, particularly in terms of model performance (e.g., accuracy, overfitting, computational cost). However, the context is slightly fragmented and lacks a structured comparison, which could have made it even more comprehensive.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about Gradient Boosting and Random Forest. However, there are minor unsupported additions (e.g., specific claims about the number of trees in Gradient Boosting or interpretability details) that are not explicitly stated in the provided context.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It directly addresses the query by comparing the two methods across multiple dimensions (accuracy, overfitting, training, computational cost, etc.) and presents the information in a logical and easy-to-understand format."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 30,
      "question": "What should be considered when selecting features for a machine learning model?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:56:30",
      "answer": "Based on the provided context, here are the key considerations for feature selection in machine learning:\n\n**Types of Feature Selection Methods:**\n\n1. **Filter Feature Selection Methods:**\n- Performed as a pre-processing step before learning\n- Uses methods like univariate statistical analysis and feature importance scores\n- Process is performed once and is independent of the model algorithm\n[Source 1]\n\n2. **Intrinsic Feature Selection Methods:**\n- Naturally embedded within the training process\n- Used in specific models like:\n  - Tree-based models\n  - Regularization models\n- **Pros:**\n  - Fast implementation\n  - No external feature tools needed\n  - Direct connection between feature selection and object function\n- **Cons:**\n  - Model-dependent\n  - Limited choice of models\n[Source 1]\n\n**Important Considerations:**\n- When dealing with **high-dimensional data** (many features compared to samples), feature selection becomes particularly important to avoid overfitting [Source 3]\n- Some algorithms, like **Random Forest**, don't require feature selection as part of their advantages [Source 4]\n- Feature selection methods can be computationally intensive when dealing with a large number of features [Source 2]\n- There is a risk of overfitting features to the training data during the selection process [Source 2]\n\nNote: While feature selection is mentioned in the context as part of machine learning processes, the documents don't provide an exhaustive list of all feature selection considerations. The information provided focuses mainly on the types of methods and their general pros and cons.",
      "retrieved_docs": [
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 9.622805833816528,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 1603,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of feature selection methods (filter and intrinsic) and their pros and cons. However, they do not provide a comprehensive list of all possible considerations (e.g., domain knowledge, feature interpretability, or multicollinearity), which slightly limits the relevance.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information about feature selection methods, their pros and cons, and other considerations like computational cost and overfitting without adding unsupported details.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete, addressing the query effectively. It could be improved by explicitly mentioning additional considerations (e.g., domain knowledge or feature interpretability) that are not covered in the retrieved context, but it remains a high-quality response overall."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 30,
      "question": "What should be considered when selecting features for a machine learning model?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:56:39",
      "answer": "To select features for a machine learning model, the following considerations should be taken into account based on the provided context:\n\n1. **Feature Selection Methods**:\n   - **Select features that correlate well with target variables** using methods like univariate statistical analysis and feature importance scores [Source 1].\n   - **Intrinsic Feature Selection Methods** embedded in the training process of models like tree-based models and regularization models are fast and don't require external feature tools [Source 1].\n   - **Filter Feature Selection Methods** are model-dependent but provide a direct connection between feature selection and the object function [Source 1].\n\n2. **Evaluation Metrics**:\n   - Consider **Evaluation metrics** during the feature selection process [Source 2].\n\n3. **Imbalanced Data**:\n   - Take into account the presence of **Imbalanced Data**, where one or more labels dominate the dataset, impacting feature selection and model performance [Source 3].\n\nTherefore, when selecting features for a machine learning model, it is crucial to consider the correlation with target variables, intrinsic and filter feature selection methods, evaluation metrics, and the presence of imbalanced data in the dataset.",
      "retrieved_docs": [
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        }
      ],
      "response_time": 2.8859198093414307,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1246,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as feature selection methods (e.g., correlation with target variables, univariate statistical analysis, and feature importance scores). However, the context is somewhat fragmented and does not comprehensively address all potential considerations (e.g., computational efficiency, domain knowledge, or feature redundancy).",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the methods and considerations mentioned (e.g., feature correlation, intrinsic and filter methods, evaluation metrics, and imbalanced data). However, it includes minor unsupported additions like 'regularization models' and 'domain knowledge' is implied but not explicitly stated in the context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete, addressing the query effectively. It could be improved by explicitly stating limitations (e.g., computational cost) or additional considerations (e.g., feature redundancy or domain-specific factors) that are not fully covered in the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 30,
      "question": "What should be considered when selecting features for a machine learning model?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:56:47",
      "answer": "When selecting features for a machine learning model, several aspects should be considered:\n\n*   **Correlation with Target Variables**: It is important to select features that **correlate well with target variables**, which can be identified through methods like univariate statistical analysis and feature importance scores. This process is typically performed once and is independent of the model algorithm. [Source 1]\n*   **Feature Selection Methods**:\n    *   **Filter Feature Selection Methods** are used as a pre-processing step before the actual learning. [Source 1]\n    *   **Intrinsic Feature Selection Methods** have feature selection naturally embedded within the training process of models such as tree-based models and regularization models. These methods are fast, require no external tools, and provide a direct connection between feature selection and the objective function. However, they are model-dependent and limit the choice of models. [Source 1]\n*   **Impact on Model Performance**:\n    *   **Overfitting**: Be aware that feature selection can **overfit the features to the training data**, especially when the number of features is large. [Source 3] For instance, when the number of features is much greater than the number of samples, avoiding overfitting in choosing kernel functions and regularization terms is crucial for models like Support Vector Machines. [Source 4]\n    *   **Computational Time**: A large number of features can lead to **significant computation time**. [Source 3]\n*   **Model-Specific Considerations**:\n    *   Some models, like **Random Forest**, do not require feature selection. [Source 5]\n    *   For **Logistic Regression**, it's important to consider that it makes linear assumptions between features and the target, and it can be prone to overfitting in high-dimensional data and sensitive to outliers. [Source 6]\n    *   **Support Vector Machines** can be slow to train on large datasets due to the need to choose appropriate kernels and their parameters. [Source 4]\n*   **Data and Feature Engineering**: Feature selection is a key component of data and feature engineering, which also involves handling missing values and imbalanced data. [Source 2]",
      "retrieved_docs": [
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        }
      ],
      "response_time": 2.4800820350646973,
      "num_citations": 6,
      "num_retrieved_docs": 8,
      "answer_length": 2208,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as correlation with target variables, feature selection methods, overfitting, computational time, and model-specific considerations. The context comprehensively addresses the query.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about feature selection methods, overfitting, computational time, and model-specific considerations. However, some minor details (e.g., references to Sources 3, 4, 5, and 6) are not explicitly present in the provided context, which slightly reduces the score.",
          "answer_quality": "The answer is excellent, clear, and well-structured. It comprehensively addresses the query by covering all major considerations for feature selection in machine learning, including correlation, methods, performance impact, and model-specific factors. The response is logically organized and easy to follow."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 31,
      "question": "How do linear models perform compared to tree-based models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:57:06",
      "answer": "Based on the provided context, I can compare linear models and tree-based models by analyzing their pros and cons:\n\n## Linear Models (like Linear/Logistic Regression)\n**Pros:**\n- **Easy to explain, implement and interpret** [Source 5]\n- **Simple and straightforward** (less prone to overfitting compared to more complex models like random forests) [Source 2]\n\n**Cons:**\n- **Makes linear assumptions** between features and target [Source 5]\n- **Prone to overfitting** in high-dimensional data [Source 5]\n- **Sensitive to outliers** [Source 5]\n\n## Tree-based Models (Decision Trees, Random Forest, Gradient Boosting)\n**Pros:**\n- **Requires little data pre-processing** [Source 3]\n- **Doesn't require feature selection** [Source 5, Source 6]\n- **Efficient in prediction** (logarithmic cost for predictions) [Source 3]\n- **Better generalization** performance (especially Random Forest) [Source 4]\n- **Less sensitive to outliers** (Random Forest) [Source 4]\n- **Provides feature importance** for interpretation [Source 4]\n\n**Cons:**\n- **Prone to overfitting** (especially single decision trees) [Source 3]\n- **Computationally expensive** (especially for Random Forest and Gradient Boosting) [Source 4]\n- **Less interpretable** compared to linear models (especially for ensemble methods) [Source 1]\n- **More hyperparameters** to tune (for ensemble methods) [Source 1]\n\nThe key difference is that linear models make strong assumptions about linear relationships in the data but are simpler and more interpretable, while tree-based models are more flexible and powerful but require more computational resources and can be more complex to interpret.",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 11.030817031860352,
      "num_citations": 5,
      "num_retrieved_docs": 40,
      "answer_length": 1640,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents provide some relevant information about the differences between tree-based models (e.g., Random Forest) and linear models (e.g., Logistic Regression). However, they miss key aspects of the query, such as a broader comparison of performance metrics (e.g., accuracy, speed, scalability) and do not cover linear models comprehensively. The context is somewhat fragmented and lacks depth.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details or generalizations (e.g., 'less prone to overfitting compared to more complex models like random forests' is not directly supported by the provided context). The majority of the claims align with the retrieved documents, but minor additions or interpretations are present.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively by comparing linear and tree-based models. It covers pros and cons in a balanced way and is mostly complete. However, it could be improved by explicitly tying more points back to the retrieved context and avoiding minor unsupported claims."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 31,
      "question": "How do linear models perform compared to tree-based models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:57:21",
      "answer": "Based on the provided information from the PDF documents, the comparison between **linear models** and **tree-based models** can be summarized as follows:\n\n### Linear Models:\n- **Pros**:\n  - Easy to explain, implement, and interpret.\n  - Effective in high-dimensional spaces.\n  - Versatile in specifying different Kernel functions.\n  - Memory efficient.\n  - Can handle outliers and overlapping classes.\n- **Cons**:\n  - Prone to overfitting in high-dimensional data.\n  - Sensitive to noise.\n  - Not good at extrapolation.\n  - Do not directly provide probability estimates. [Source 1]\n\n### Tree-based Models:\n- **Decision Trees**:\n  - **Pros**:\n    - Easy to understand and interpret.\n    - Requires little data pre-processing.\n    - Doesn't require feature selection.\n    - Efficient in prediction.\n  - **Cons**:\n    - Prone to overfitting.\n    - Slow to train on large datasets.\n    - Do not directly provide probability estimates. [Source 1]\n\n- **Random Forest**:\n  - **Pros**:\n    - Better generalization performance due to randomness.\n    - Doesn’t require much parameter tuning.\n    - Generates feature importance helpful for interpretation.\n  - **Cons**:\n    - Computationally expensive. [Source 2]\n\n### Comparison:\n- **Linear Models vs. Tree-based Models**:\n  - **Structure**:\n    - Linear models are simple and linear, while tree-based models are ensembles of decision trees.\n  - **Training Process**:\n    - Linear models involve estimating parameters, whereas tree-based models train independently or sequentially.\n  - **Interpretability**:\n    - Linear models are easier to interpret based on coefficients, while feature importance is used for tree-based models.\n  - **Overfitting**:\n    - Linear models are prone to overfitting in high-dimensional data, while tree-based models can also suffer from overfitting. [Source 2, 4, 5, 6]\n\nBased on the comparison, linear models are simpler and more interpretable, while tree-based models like Random Forest offer better generalization due to ensemble learning but are computationally expensive. Linear models are more suitable when interpretability is crucial, whereas tree-based models are preferred for predictive accuracy.",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 5.824409008026123,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 2179,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents provide a partial comparison between tree-based models (specifically Random Forest and Decision Trees) and linear models (e.g., Logistic Regression). While the context covers key aspects like overfitting, structure, and some pros/cons, it does not comprehensively address all dimensions of the query (e.g., performance metrics like accuracy, use cases, or scalability).",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the pros/cons of linear models (e.g., 'Versatile in specifying different Kernel functions,' 'Memory efficient') are not explicitly mentioned in the provided context. The comparison of interpretability and overfitting is grounded in the context, but other claims lack direct support.",
          "answer_quality": "The answer is well-structured, clear, and addresses the query effectively. It provides a detailed comparison and organizes information logically. However, it could be improved by focusing only on the information explicitly supported by the retrieved context and avoiding unsupported generalizations."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 31,
      "question": "How do linear models perform compared to tree-based models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:57:29",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about how linear models perform compared to tree-based models. The context does mention \"Linear Models\" and \"Tree-based Models\" as general categories of ML algorithms, and it compares specific linear models (like Linear Regression and Logistic Regression) with specific tree-based models (like Decision Tree and Random Forest) in terms of their differences, pros, and cons. However, it does not offer a direct comparison of the overall performance of linear models as a group versus tree-based models as a group. [Source 1]",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        }
      ],
      "response_time": 1.3241279125213623,
      "num_citations": 1,
      "num_retrieved_docs": 40,
      "answer_length": 631,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents mention linear models (e.g., Logistic Regression) and tree-based models (e.g., Decision Tree, Random Forest) and compare some of their characteristics (e.g., overfitting, structure). However, they do not provide a direct or comprehensive comparison of the overall performance of linear models versus tree-based models, missing key aspects of the query.",
          "faithfulness": "The answer accurately reflects the content of the retrieved documents. It correctly states that the documents do not provide a direct comparison of the overall performance of linear models versus tree-based models, only mentioning specific models and their differences in limited contexts.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query by explaining the limitations of the retrieved context. It could be slightly improved by suggesting where such a comparison might be found or briefly summarizing the partial information available, but it is otherwise complete and helpful."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 32,
      "question": "What are the benefits of using a Decision Tree over Random Forest?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:57:42",
      "answer": "Based on the provided context, here are the key benefits of using a Decision Tree over Random Forest:\n\n1. **Better Interpretability**\n- Decision trees are **more interpretable** as they are based on a series of simple rules that can be followed to make a prediction [Source 1]\n- The interpretation is more straightforward compared to Random Forests, which rely on feature importance for interpretation [Source 1]\n\n2. **Computational Advantages**\n- **Fewer hyperparameters** to tune [Source 1, Source 2]\n- **Less computationally expensive** compared to Random Forest, which requires training multiple trees [Source 3]\n\n3. **Training Simplicity**\n- Only requires training a **single tree** on the dataset [Source 2]\n- **Easy to understand** and interpret [Source 4]\n- Requires **little data pre-processing** [Source 4]\n- **Efficient in prediction**: the cost of one prediction is logarithmic in the number of examples used to train the tree [Source 4]\n\nHowever, it's important to note that these benefits come with trade-offs, as Decision Trees are more prone to overfitting compared to Random Forests [Source 1, Source 2], which is one of their main disadvantages.",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 7.701401948928833,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 1163,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents provide relevant information about the differences between Decision Trees and Random Forests, including aspects like overfitting, hyperparameters, and training processes. However, the context is somewhat fragmented and does not explicitly list all the benefits of Decision Trees over Random Forests in a structured manner, missing a few key points that the answer addresses.",
          "faithfulness": "The answer is largely faithful to the retrieved context, correctly citing interpretability, computational advantages, and training simplicity. However, some points (e.g., 'little data pre-processing' and 'efficient in prediction') are not explicitly mentioned in the provided context, though they are generally true and not contradictory.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a comprehensive list of benefits with logical grouping. However, it could be slightly improved by explicitly noting which points are inferred rather than directly stated in the context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 32,
      "question": "What are the benefits of using a Decision Tree over Random Forest?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:57:51",
      "answer": "Based on the provided context from the PDF documents, the benefits of using a **Decision Tree** over **Random Forest** include:\n\n- **Interpretability**:\n  - Decision Trees are more interpretable as they are based on a series of simple rules that can be followed to make predictions. This contrasts with Random Forest, where interpreting the models is based on feature importance, which is less straightforward compared to decision trees. [Source 1]\n  - The model can be interpreted based on the feature importance in Decision Trees, while in Random Forest, interpretation is based on the value and sign of the estimated coefficients. [Source 3]\n\n- **Hyperparameters**:\n  - Decision Trees have fewer hyperparameters compared to Random Forest, making them simpler in terms of tuning parameters. [Source 1]\n  - Random Forest has more hyperparameters compared to Decision Trees, which can add complexity to the model. [Source 2]\n\n- **Training Process**:\n  - Decision Trees involve training a single tree on the dataset, while Random Forest consists of an ensemble of decision trees where each tree is trained independently. [Source 2]\n\nIn summary, the benefits of using a Decision Tree over Random Forest include better interpretability, fewer hyperparameters, and a simpler training process with a single tree trained on the dataset.",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 3.368736982345581,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1330,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents provide relevant information about the differences between Decision Trees and Random Forests, including aspects like overfitting, hyperparameters, and training processes. However, the context does not explicitly highlight the benefits of Decision Trees over Random Forests in a direct manner, missing some key points like interpretability in the retrieved snippets.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some interpretations (e.g., interpretability) that are not explicitly stated in the provided documents. The claims about hyperparameters and training processes are well-supported, but the interpretability argument is extrapolated rather than directly cited.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers key benefits like interpretability, hyperparameters, and training simplicity. However, it could be slightly more comprehensive by explicitly stating that the context did not directly mention interpretability, which was inferred logically."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 32,
      "question": "What are the benefits of using a Decision Tree over Random Forest?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:57:59",
      "answer": "Based on the provided documents, here are the benefits of using a Decision Tree over Random Forest:\n\n*   **Interpretability**: Decision Trees are **more interpretable** as they are based on a series of simple rules that can be followed to make a prediction [Source 1]. This is in contrast to Random Forests, where models can be interpreted based on feature importance, which is less straightforward [Source 1].\n*   **Fewer hyperparameters**: Decision Trees generally have **fewer hyperparameters** compared to Random Forests [Source 1, Source 2].",
      "retrieved_docs": [
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 1.2718558311462402,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 546,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents discuss differences between Decision Trees and Random Forests, including aspects like overfitting, hyperparameters, and training. While the documents do not explicitly list the benefits of Decision Trees over Random Forests, they provide enough context to infer some advantages (e.g., interpretability and fewer hyperparameters). However, the coverage is not exhaustive, missing some potential benefits like computational efficiency.",
          "faithfulness": "The answer is mostly faithful to the retrieved context. It correctly identifies interpretability and fewer hyperparameters as advantages of Decision Trees over Random Forests, which can be reasonably inferred from the documents. However, the claim about interpretability is not explicitly stated in the provided context, though it is a well-known characteristic of Decision Trees.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides two valid benefits of Decision Trees over Random Forests. However, it could be improved by including additional advantages (e.g., faster training time, lower computational cost) if they were supported by the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 33,
      "question": "Summarize the pros and cons of using clustering models.",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:58:15",
      "answer": "Based on the provided documents, I can summarize the pros and cons of two main clustering models - K-means and Gaussian Mixture Model (GMM):\n\n## K-means\n**Pros:**\n- Easy to implement\n- Computationally efficient\n[Source 1]\n\n**Cons:**\n- The number of clusters must be predetermined\n- Initial positions of centroids influence the final position\n- Clusters can only be circular with equal sizes\n[Source 1]\n- Less flexible due to its simplicity\n[Source 3]\n\n## Gaussian Mixture Model (GMM)\n**Pros:**\n- Soft clustering algorithm - assigns probability of belonging to each cluster\n- Clusters can have different shapes and sizes\n[Source 2]\n\n**Cons:**\n- Less efficient to train due to its flexibility\n[Source 2]\n\n## Key Differences Between K-means and GMM:\n- **Clustering Approach**: K-means is a hard clustering algorithm (assigns points to single cluster) while GMM is a soft clustering algorithm (assigns probabilities)\n- **Cluster Characteristics**: K-means produces circular clusters of equal size, while GMM can handle different shapes and sizes\n- **Efficiency**: K-means is more efficient due to simplicity, while GMM is less efficient due to flexibility\n- **Convergence**: K-means converges when centroids stop changing significantly, while GMM converges when model parameters (mean, covariance, mixing coefficients) stabilize\n[Source 8, Source 10]",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, co..."
        }
      ],
      "response_time": 10.240728855133057,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1346,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents do not focus on clustering models but instead discuss other machine learning models (e.g., decision trees, support vector machines, gradient boosting). While the answer attempts to summarize clustering models (K-means and GMM), the context provided is largely irrelevant to the query.",
          "faithfulness": "The answer includes information about clustering models (K-means and GMM) that is not present in the retrieved context. Some claims (e.g., pros/cons of K-means and GMM) are unsupported by the provided documents, and the context does not mention clustering at all.",
          "answer_quality": "The answer is structured and provides a reasonable summary of clustering models, but it is not grounded in the retrieved context. It is clear and mostly complete but could be improved by aligning with the provided documents or acknowledging their irrelevance."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 33,
      "question": "Summarize the pros and cons of using clustering models.",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:58:23",
      "answer": "**Pros and Cons of Clustering Models:**\n\n**K-means:**\n- **Pros:**\n  - Easy to implement.\n  - Computationally efficient.\n- **Cons:**\n  - Number of clusters has to be determined.\n  - Stability affected by initial centroid positions.\n  - Shapes of clusters are limited to circular with equal sizes. [Source 1]\n\n**Gaussian Mixture Model (GMM):**\n- **Pros:**\n  - Soft clustering with probability assignment to clusters.\n  - Clusters can have different shapes and sizes.\n- **Cons:**\n  - Less efficient to train due to flexibility.\n  - Less efficient due to flexibility. [Source 2]\n\n**Overall:**\n- **Pros:**\n  - GMM has better generalization performance due to randomness.\n  - K-means is computationally efficient.\n- **Cons:**\n  - GMM is computationally expensive.\n  - Both models can be prone to overfitting. [Source 3, Source 6]\n\nThese models offer a trade-off between simplicity and flexibility, with K-means being straightforward and efficient but limited in cluster shapes, while GMM allows for more complex clusters but at the cost of higher computational complexity.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        }
      ],
      "response_time": 3.1911323070526123,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1066,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents do not specifically address clustering models but instead discuss other machine learning models (e.g., linear models, SVM, tree-based methods). While the model answer correctly summarizes the pros and cons of clustering models (K-means and GMM), the retrieved context is not relevant to the query, missing key aspects of clustering entirely.",
          "faithfulness": "The answer is completely faithful to the *intended* topic (clustering models) and does not rely on or contradict the irrelevant retrieved context. It provides accurate and well-known information about K-means and GMM without introducing unsupported claims.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive, covering key pros and cons of K-means and GMM. However, it could be improved by including more clustering models (e.g., DBSCAN, hierarchical clustering) or addressing broader trade-offs in clustering (e.g., scalability, interpretability)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 33,
      "question": "Summarize the pros and cons of using clustering models.",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:58:34",
      "answer": "Clustering models offer various advantages and disadvantages depending on the specific algorithm used.\n\n## K-means Clustering\n\n**Pros:**\n*   **Easy to implement** [Source 1].\n*   **Computationally efficient** [Source 1].\n\n**Cons:**\n*   The **number of clusters** has to be predetermined [Source 1].\n*   **Stability**: The initial positions of centroids influence the final position [Source 1].\n*   The **shapes of clusters can only be circular** with equal sizes [Source 1].\n*   **Hard clustering algorithm**: Each data point is assigned to a single cluster [Source 3].\n*   **Clusters are circular and sizes are equal** [Source 4].\n\n## Gaussian Mixture Model (GMM)\n\n**Pros:**\n*   A **soft clustering algorithm**, where each data point is assigned a probability of belonging to each cluster [Source 2], [Source 3].\n*   **Clusters can have different shapes and sizes** [Source 2], [Source 4].\n\n**Cons:**\n*   **Less efficient to train** due to its flexibility [Source 2], [Source 4].",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Computationally efficient.Cons • The number of clusters, has to be determined. • Stability: Initial positions of centroids influence the final position. • The shapes of clusters can only be circular with equal sizes.obieri.veyǎiiiiiinuǎǐlessoverfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, covariance,and mixing coefficients) do notchange significantly betweeniterations.Shape of clustersClusters are circular and sizesare equal.Clusters can have differentshapes and sizes.EfficiencyMore efficient due to its simplicity.Less efficient due to its flexibility.Loss functionsCross-EntropyCross-e",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-mea..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "10DifferencesK-meansGaussian Mixture Model (GMM)ConvergenceConverges when the centroids ofthe clusters do not changesignificantly between iterations.Converges when the parametersof the model (mean, co..."
        }
      ],
      "response_time": 2.3758528232574463,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 980,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 1,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents do not discuss clustering models at all. Instead, they focus on other machine learning models like linear regression, support vector machines, decision trees, and gradient boosting. Thus, the relevance to the query is minimal.",
          "faithfulness": "The answer contains significant information not present in the retrieved documents. The retrieved context does not mention clustering models, K-means, or Gaussian Mixture Models, making the answer entirely unsupported by the provided context.",
          "answer_quality": "The answer is well-structured and provides a clear summary of the pros and cons of clustering models. However, since it is not grounded in the retrieved context, it cannot be considered fully reliable. The quality is adequate in terms of clarity and completeness but lacks validity due to the irrelevant source material."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 34,
      "question": "What are some common feature selection techniques in machine learning?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:58:50",
      "answer": "Based on the provided documents, there are three main categories of feature selection techniques:\n\n1. **Intrinsic Feature Selection Methods**\n- Naturally embedded within the training process\n- Used in tree-based models and regularization models\n- **Pros:**\n  - Fast and requires no external feature tools\n  - Direct connection between feature selection and object function\n- **Cons:**\n  - Model-dependent\n  - Limited choice of models\n[Source 1]\n\n2. **Filter Feature Selection Methods**\n- Selects features that correlate well with target variables\n- Uses univariate statistical analysis and feature importance scores\n- Process is performed once and is independent of model algorithm\n- **Cons:**\n  - Tends to select redundant features\n  - Ignores relationships among features\n[Source 1, Source 2]\n\n3. **Wrapper Feature Selection Methods**\n- Iterative process using sequential feature selection\n- Repeatedly adds subset features and evaluates model performance\n- **Pros:**\n  - Searches for a wider variety of feature subsets compared to other methods\n[Source 2]\n\nIt's worth noting that some algorithms, like **Random Forest**, don't require feature selection as part of their process, as mentioned in [Source 3].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 8.137932062149048,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1209,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions some key feature selection techniques (e.g., forward selection, backward selection, cross-validation-based feature selection) but does not cover all common categories (e.g., filter or wrapper methods). However, the retrieved context is highly relevant to the query and provides a foundation for the answer, which expands logically on the topic.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes additional details (e.g., pros/cons of methods, mention of Random Forest) that are not explicitly present in the provided document. Some claims, like the categorization into intrinsic, filter, and wrapper methods, are accurate but go beyond the retrieved context.",
          "answer_quality": "The answer is clear, well-structured, and comprehensive, covering the main categories of feature selection techniques. It provides useful details like pros and cons, which enhance understanding. However, it could be slightly improved by explicitly tying all details back to the retrieved context to ensure full groundedness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 34,
      "question": "What are some common feature selection techniques in machine learning?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:59:00",
      "answer": "**Common feature selection techniques in machine learning** based on the provided context include:\n\n1. **Intrinsic Feature Selection Methods**:\n   - Embedded feature selection in models like tree-based models and regularization models.\n   - Fast and doesn't require external feature tools. [Source 1]\n\n2. **Filter Feature Selection Methods**:\n   - Select features that correlate well with target variables using methods like univariate statistical analysis and feature importance scores.\n   - Model-dependent and limited choice of models. [Source 1]\n\n3. **Wrapper Feature Selection Methods**:\n   - An iterative process that adds subset features to the model based on model performance.\n   - Search for a wider variety of feature subsets compared to other methods. [Source 2]\n\nThese techniques offer different approaches to selecting features in machine learning models, each with its own advantages and limitations as described in the sources provided.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 2.603236198425293,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 952,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions some relevant feature selection techniques (e.g., forward selection, backward selection, cross-validation-based feature selection) but does not cover all common methods or provide sufficient detail. Key aspects like filter and wrapper methods are missing from the context, though they are correctly inferred in the answer.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes details (e.g., filter and wrapper methods, their advantages/limitations) that are not explicitly supported by the provided document. The intrinsic methods mentioned align with the context, but the other categories are unsupported additions.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. However, it could be improved by explicitly noting which parts are inferred (e.g., filter/wrapper methods) versus directly supported by the context. The completeness is good but slightly overreaches the provided information."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 34,
      "question": "What are some common feature selection techniques in machine learning?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:59:14",
      "answer": "Some common feature selection techniques in machine learning include:\n\n*   **Intrinsic Feature Selection Methods**: These methods have feature selection naturally embedded within the training process of models, such as tree-based models and regularization models. [Source 1]\n*   **Filter Feature Selection Methods**: These methods select features that correlate well with target variables, using techniques like univariate statistical analysis and feature importance scores. This process is performed once and is independent of the model algorithm. [Source 1]\n*   **Wrapper Feature Selection Methods**: This is an iterative process, such as sequential feature selection, where subset features are repeatedly added to the model, and the resulting model performance guides the selection of the next subset. Examples include forward selection, backward selection, and cross-validation-based feature selection. [Source 2]\n\nFeature selection is also listed as a topic within the broader context of data and feature engineering. [Source 3] Some models, like Random Forest, do not require feature selection. [Source 4]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 1.5337669849395752,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 1111,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly mentions key feature selection techniques like forward selection, backward selection, and cross-validation-based feature selection. However, it does not comprehensively cover all common techniques (e.g., filter methods are not explicitly mentioned in the retrieved context).",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details. For example, the mention of 'filter feature selection methods' and 'Random Forest not requiring feature selection' is not explicitly stated in the provided context. The core techniques (forward/backward selection, cross-validation) are correctly attributed.",
          "answer_quality": "The answer is clear, well-structured, and covers most common feature selection techniques. However, it could be slightly improved by ensuring all claims are directly supported by the retrieved context (e.g., avoiding unsupported additions like 'filter methods' or 'Random Forest')."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 35,
      "question": "How do loss functions influence the training of machine learning models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 14:59:33",
      "answer": "Based on the provided documents, I can explain several key loss functions and their influence on model training:\n\n## Common Loss Functions and Their Applications\n\n1. **Cross-Entropy Loss**\n- Used for binary classification models\n- Measures the difference between predicted probability distribution and observed distribution\n- Formula: L = y log(p) + (1-y)log(1-p), where y is the target label and p is the predicted probability\n[Source 4]\n\n2. **Hinge Loss**\n- Specifically used in Support Vector Machines (SVM)\n- Measures the amount by which the model's prediction is incorrect\n- Formula: L = max(0, 1-y)\n[Source 4]\n\n3. **Mean Squared Error (MSE)**\n- Commonly used in regression problems\n- Measures average squared distance between predictions and true values\n- **Pros**:\n  - Penalizes large errors\n  - Mathematically convenient for obtaining gradients\n- **Cons**:\n  - Sensitive to outliers\n[Source 4, Source 12]\n\n## Impact on Different Models\n\nDifferent models use specific loss functions that influence their training:\n\n- **Logistic Regression** uses cross-entropy loss as its cost function [Source 2, Source 3]\n- **SVM** uses hinge loss as its loss function [Source 3]\n- The choice of loss function affects how models learn from errors and optimize their parameters during training [Source 2]\n\n## Evaluation Considerations\n\nWhen working with imbalanced datasets, the choice of loss function becomes particularly important:\n- **Precision-Recall** metrics give more importance to the positive class\n- These metrics are especially helpful when dealing with imbalanced data\n[Source 1]\n\nThe documents provide these key loss functions and their mathematical formulations, but they don't contain detailed information about how loss functions specifically influence the training process beyond these basic descriptions.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        }
      ],
      "response_time": 12.558239936828613,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 1814,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about specific loss functions (e.g., cross-entropy, hinge loss, MSE) and their applications in machine learning models. However, they lack deeper insights into how loss functions *influence* the training process (e.g., optimization dynamics, convergence, or model behavior), which is a key aspect of the query.",
          "faithfulness": "The answer strictly adheres to the retrieved context, accurately describing the loss functions, their formulas, and their use cases without introducing unsupported claims or contradictions. The caveat about missing details on training influence is also faithful to the limitations of the context.",
          "answer_quality": "The answer is well-structured, clear, and comprehensive in covering the provided loss functions and their applications. However, it acknowledges the lack of detailed information about the *influence* of loss functions on training, which slightly limits its completeness. The inclusion of pros/cons and model-specific uses adds value."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 35,
      "question": "How do loss functions influence the training of machine learning models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 14:59:44",
      "answer": "### Loss Functions Influence on Training of Machine Learning Models\n\nLoss functions play a crucial role in training machine learning models by measuring the model's performance and guiding the optimization process. Here's how loss functions influence the training based on the provided context:\n\n1. **Types of Loss Functions**:\n   - **Cross-Entropy Loss**: Used in logistic regression to measure the difference between the predicted probability distribution and the observed distribution [Source 1].\n   - **Hinge Loss**: Utilized in SVM to quantify the amount by which the model's prediction is incorrect [Source 1].\n   - **Mean Squared Error (MSE)**: Commonly used in various applications to measure the average squared distance between predictions and true values [Source 3].\n   - **Root Mean Square Error (RMSE)**: Similar to MSE but considers the square root of the average squared distances [Source 1].\n   - **Mean Absolute Error (MAE)**: Measures the absolute errors between predictions and true values [Source 1].\n\n2. **Influence on Training**:\n   - **Optimization**: Loss functions guide the optimization process by providing a measure of how well the model is performing. The goal during training is to minimize the loss function to improve the model's accuracy [Source 1].\n   - **Model Performance**: The choice of loss function impacts how the model learns from the data and adjusts its parameters to make predictions [Source 1].\n   - **Handling Outliers**: Loss functions like MSE are sensitive to outliers, impacting the model's training process and performance [Source 5, Source 12].\n\nLoss functions are essential components in machine learning that directly impact the model's learning process, optimization, and overall performance. Understanding the characteristics and implications of different loss functions is crucial for effectively training machine learning models.\n\n### Sources:\n- [Source 1]\n- [Source 2]\n- [Source 3]",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        }
      ],
      "response_time": 4.484044075012207,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1941,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about loss functions (e.g., cross-entropy, hinge loss, MSE) and their role in model evaluation. However, they do not fully cover the broader influence of loss functions on training dynamics (e.g., gradient behavior, convergence, or trade-offs like bias-variance). Some aspects of the query are addressed, but key details are missing.",
          "faithfulness": "The answer is largely faithful to the retrieved context, correctly citing cross-entropy, hinge loss, MSE, and MAE. However, it introduces terms like 'Root Mean Square Error (RMSE)' and 'handling outliers' without explicit support from the provided context, though these are logically consistent with general ML knowledge.",
          "answer_quality": "The answer is well-structured, clear, and addresses the core aspects of the query. It explains the role of loss functions in optimization and model performance effectively. However, it could be improved by elaborating on *how* loss functions influence training (e.g., gradient updates, convergence) and discussing trade-offs (e.g., sensitivity to outliers)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 35,
      "question": "How do loss functions influence the training of machine learning models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 14:59:54",
      "answer": "Loss functions influence the training of machine learning models by measuring the **difference between predicted values and true values**, guiding the model to adjust its parameters to minimize this difference [Source 4, Source 2, Source 3]. Different loss functions are used depending on the type of task and model.\n\nHere's how specific loss functions work:\n\n*   **Cross-Entropy Loss**:\n    *   Measures the performance of a binary classification model by comparing the output distribution to observations [Source 2].\n    *   It is used in logistic regression to measure the difference between the predicted probability distribution and the observed distribution [Source 2, Source 3].\n    *   The formula is $L = -[y \\log(p) + (1-y) \\log(1-p)]$, where $y$ is the target (data label) and $p$ is the predicted probability [Source 2].\n\n*   **Hinge Loss**:\n    *   Measures the amount by which the model's prediction is incorrect [Source 2].\n    *   It is specifically used in Support Vector Machines (SVMs) [Source 4, Source 3].\n    *   The formula is $L = \\max(0, 1 - y \\hat{y})$, where $\\hat{y}$ is the predicted output [Source 2].\n\n*   **Mean Squared Error (MSE)**:\n    *   Measures the average squared distance between predictions and true values [Source 3].\n    *   It is commonly used in linear regression for prediction tasks [Source 1].\n    *   Pros include penalizing large errors and being mathematically convenient for gradient calculation [Source 3].\n    *   Cons include sensitivity to outliers [Source 3].\n    *   The formula is $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $n$ is the number of samples, $y_i$ is the true value, and $\\hat{y}_i$ is the predicted value [Source 4, Source 12].\n\n*   **Root Mean Square Error (RMSE)**:\n    *   Calculated as $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$, where $n$ is the number of samples, $y_i$ is the true value, and $\\hat{y}_i$ is the predicted value [Source 2].\n\n*   **Mean Absolute Error (MAE)**:\n    *   Another loss function mentioned in the context, though its specific formula or detailed influence isn't provided beyond its name [Source 4, Source 6].\n\nThese loss functions are fundamental in guiding the optimization process of various machine learning algorithms, including linear regression, logistic regression, and SVMs, by providing a quantifiable measure of error that the models aim to minimize during training [Source 2, Source 3].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        }
      ],
      "response_time": 3.6650428771972656,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2434,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key loss functions (e.g., cross-entropy, hinge loss, MSE) and their roles in machine learning model training. They comprehensively address the query's focus on how loss functions influence training.",
          "faithfulness": "The answer is completely faithful to the retrieved context. All claims, formulas, and explanations are directly supported by the provided documents, with no unsupported additions or contradictions.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It explains the role of loss functions in training, provides specific examples with formulas, and highlights their applications and trade-offs. The inclusion of pros/cons (e.g., for MSE) adds depth."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 36,
      "question": "What are the differences between Linear Regression and Logistic Regression?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:00:14",
      "answer": "Based on the provided context, here are the key differences between Linear Regression and Logistic Regression:\n\n## Type of Prediction\n- **Linear Regression**: Used when the target is continuous and gives real value predictions\n- **Logistic Regression**: Used when the target is binary (0 or 1, true or false) and gives probability predictions between 0 and 1\n[Source 1]\n\n## Loss Function\n- **Linear Regression**: Uses **mean squared error (MSE)** in most applications\n- **Logistic Regression**: Uses **cross-entropy loss** as the cost function that measures the difference between predicted probability distribution and observed distribution\n[Source 1]\n\n## Assumptions\n- **Linear Regression**: \n  - Makes linear assumptions between the features and the target\n  - Assumes there is a linear relationship between each input variable and the target\n  - More sensitive to outliers\n[Source 1, Source 5]\n\n- **Logistic Regression**:\n  - Does not make any assumptions about the distribution of the input variables\n  - Less sensitive to outliers\n[Source 1]\n\n## Pros and Cons\n\n### Logistic Regression\n**Pros**:\n- Easy to explain, implement and interpret\n- Outputs the probability of class membership\n[Source 2]\n\n**Cons**:\n- Maximizes the conditional likelihoods of the training data, making it prone to outliers\n- Prone to overfitting in high-dimensional data\n[Source 2]\n\n### Linear Regression\n**Cons**:\n- Makes linear assumptions between the features and the target\n- Prone to overfitting in high-dimensional data\n- Sensitive to outliers\n[Source 3]\n\nBoth models are **parametric models** that learn a set of parameters from the dataset to make predictions [Source 1].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 12.747287034988403,
      "num_citations": 3,
      "num_retrieved_docs": 30,
      "answer_length": 1658,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document contains some relevant information about the differences between Logistic Regression and other models (e.g., SVM), but it does not directly or comprehensively address the differences between Linear Regression and Logistic Regression. Key aspects like the type of prediction and loss functions are partially covered, but other critical differences (e.g., assumptions, use cases) are missing or implied rather than explicitly stated.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes several unsupported details. For example, the explanation of assumptions (e.g., linear assumptions in Linear Regression, sensitivity to outliers) and some pros/cons (e.g., overfitting in high-dimensional data) are not explicitly mentioned in the provided context. The loss function details are correctly attributed, but other claims lack direct support from the retrieved document.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers the key differences (type of prediction, loss function) and provides additional useful information (assumptions, pros/cons). However, some of the added details (e.g., assumptions, pros/cons) are not fully supported by the retrieved context, which slightly reduces its completeness and accuracy."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 36,
      "question": "What are the differences between Linear Regression and Logistic Regression?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:00:23",
      "answer": "Based on the provided context from various sources, here are the differences between **Linear Regression** and **Logistic Regression**:\n\n### Linear Regression\n- **Type of Prediction**:\n  - Used when the target is continuous [Source 1].\n- **Prediction**:\n  - Gives real value predictions [Source 1].\n- **Loss Function**:\n  - Uses mean squared error (MSE) in most applications [Source 1].\n- **Assumptions**:\n  - Assumes a linear relationship between each input variable and the target [Source 1].\n- **Sensitivity to Outliers**:\n  - More sensitive to outliers [Source 1].\n\n### Logistic Regression\n- **Type of Prediction**:\n  - Used when the target is binary [Source 1].\n- **Prediction**:\n  - Uses a logistic function as the prediction function that gives a value between 0 and 1, interpretable as a probability [Source 1].\n- **Loss Function**:\n  - Uses the cross-entropy loss as the cost function [Source 1].\n- **Assumptions**:\n  - Makes no assumptions about the distribution of input variables [Source 1].\n- **Sensitivity to Outliers**:\n  - Less sensitive to outliers [Source 1].\n\nThese differences highlight the distinct characteristics and applications of Linear Regression and Logistic Regression in predictive modeling tasks.\n\nSources: [Source 1]",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        }
      ],
      "response_time": 4.014096021652222,
      "num_citations": 1,
      "num_retrieved_docs": 30,
      "answer_length": 1248,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document contains some relevant information about the differences between Linear Regression and Logistic Regression, particularly regarding their loss functions. However, it is incomplete and includes irrelevant details (e.g., SVM comparison) that do not address the query fully. Key aspects like prediction types, assumptions, and sensitivity to outliers are missing from the retrieved context.",
          "faithfulness": "The answer is mostly faithful to the general concepts of Linear and Logistic Regression but includes several details (e.g., assumptions, sensitivity to outliers, prediction types) that are not explicitly supported by the retrieved context. Some claims are reasonable but not directly grounded in the provided documents.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. However, it could be improved by explicitly noting which details are not directly supported by the retrieved context. The inclusion of unsupported but reasonable details slightly detracts from completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 36,
      "question": "What are the differences between Linear Regression and Logistic Regression?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:00:30",
      "answer": "The key differences between **Linear Regression** and **Logistic Regression** are primarily in their **type of prediction**, **loss function**, and **assumptions** about the data [Source 1]:\n\n*   **Type of Prediction**:\n    *   **Linear Regression** is used when the **target variable is continuous**, providing real-value predictions [Source 1].\n    *   **Logistic Regression** is used when the **target variable is binary**, such as 0 or 1, true or false. It uses a logistic function to give a value between 0 and 1, which can be interpreted as a **probability** [Source 1].\n\n*   **Loss Function**:\n    *   **Linear Regression** typically uses **Mean Squared Error (MSE)** as its loss function [Source 1]. MSE measures the average squared distance between predictions and true values and penalizes large errors [Source 3].\n    *   **Logistic Regression** uses **cross-entropy loss** (also known as log loss) as its cost function. This measures the difference between the predicted probability distribution and the observed distribution [Source 1, Source 3]. Cross-entropy loss measures the performance of a binary classification model by comparing the output distribution to observations [Source 4].\n\n*   **Assumptions**:\n    *   **Linear Regression** assumes there is a **linear relationship** between each input variable and the target [Source 1]. It also makes linear assumptions between features and the target [Source 2].\n    *   **Logistic Regression** does not make assumptions about the distribution of the input variables [Source 1].\n\nBoth **Linear Regression** and **Logistic Regression** are **parametric models** that learn a set of parameters from the dataset to make predictions [Source 1]. However, Logistic Regression is more sensitive to **outliers** than models like Random Forest [Source 1]. It is also prone to **overfitting in high-dimensional data** and maximizes the conditional likelihoods of the training data, making it prone to outliers [Source 2, Source 7].\n\nRegarding their general characteristics:\n*   **Linear Regression** is sensitive to outliers [Source 2].\n*   **Logistic Regression** is easy to explain, implement, and interpret, and it outputs the probability of class membership [Source 2, Source 7].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        }
      ],
      "response_time": 2.525528907775879,
      "num_citations": 4,
      "num_retrieved_docs": 30,
      "answer_length": 2239,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about the differences between Linear Regression and Logistic Regression, particularly focusing on their use cases, loss functions, and assumptions. However, the context also includes irrelevant details about SVM, which slightly detracts from the overall relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the differences in type of prediction, loss functions, and assumptions. However, there are minor unsupported additions (e.g., details about outliers and overfitting) that are not explicitly present in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the key differences between Linear Regression and Logistic Regression. It could be slightly improved by omitting unsupported details and ensuring all claims are directly tied to the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 37,
      "question": "In what scenarios would you use Logistic Regression over SVMs?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:00:53",
      "answer": "Based on the provided context, here are the key scenarios where Logistic Regression would be preferable over SVMs:\n\n## Probability Requirements\n- If you need **probability estimates** for your predictions, Logistic Regression is the better choice because:\n  - It uses a logistic function that gives values between 0 and 1, which can be interpreted as probabilities [Source 1]\n  - SVMs do not directly provide probability estimates [Source 2]\n\n## Ease of Implementation\nLogistic Regression would be preferred when you need:\n- A model that is **easy to explain, implement and interpret** [Source 2]\n- **Simple interpretation** through the values and signs of estimated coefficients [Source 3]\n\n## Dataset Considerations\nChoose Logistic Regression when:\n- Working with **smaller datasets**, as SVMs can be slow to train on large datasets [Source 2]\n- Dealing with **simpler linear classification problems** where you don't need complex decision boundaries [Source 3]\n\n## Limitations to Consider\nHowever, note that both models have some similar limitations:\n- Both are prone to overfitting in high-dimensional data [Source 2]\n- Logistic Regression is more sensitive to outliers [Source 1]\n- SVMs require careful selection of kernel functions and regularization terms when features outnumber samples [Source 2]\n\nThe choice ultimately depends on your specific needs regarding:\n- Probability outputs\n- Interpretation requirements\n- Dataset size\n- Computational resources available\n\nRemember that Logistic Regression finds linear decision boundaries focusing on maximizing data likelihood, while SVMs find separating hyperplanes that maximize the margin between classes [Source 3].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        }
      ],
      "response_time": 14.330014944076538,
      "num_citations": 3,
      "num_retrieved_docs": 25,
      "answer_length": 1673,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents provide relevant information about the differences between Logistic Regression and SVMs, particularly focusing on their use cases, strengths, and weaknesses. However, the context is somewhat fragmented and does not comprehensively cover all aspects of the query (e.g., missing explicit comparisons in specific scenarios like interpretability or dataset size).",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the documents, such as the probability estimates from Logistic Regression, ease of implementation, dataset considerations, and limitations. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is excellent, clear, and well-structured. It comprehensively addresses the query by outlining specific scenarios where Logistic Regression is preferable over SVMs, such as probability requirements, ease of implementation, and dataset considerations. The answer is also well-organized and easy to follow."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 37,
      "question": "In what scenarios would you use Logistic Regression over SVMs?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:01:03",
      "answer": "**In what scenarios would you use Logistic Regression over SVMs?**\n\n**Logistic Regression** would be preferred over **Support Vector Machines (SVMs)** in the following scenarios, based on the information from the provided context:\n\n- **Type of Prediction**:\n  - Logistic Regression is used when the target is **binary**, whereas SVMs are more suitable when the target is not binary or when dealing with **continuous** targets. [Source 1 - Page 9]\n\n- **Sensitivity to Outliers**:\n  - Logistic Regression is **less sensitive to outliers** compared to SVMs. [Source 1 - Page 9]\n\n- **Interpretability**:\n  - Logistic Regression models can be interpreted by the **value and sign of the estimated coefficients**, providing a more straightforward interpretation compared to SVMs. [Source 3 - Page 10]\n\n- **Computational Efficiency**:\n  - Logistic Regression is generally **less computationally expensive** compared to SVMs. [Source 6 - Page 7]\n\nBased on the above points, Logistic Regression would be more suitable in scenarios where interpretability, handling of outliers, and computational efficiency are important considerations, and when dealing with binary classification tasks.",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE..."
        }
      ],
      "response_time": 3.7650809288024902,
      "num_citations": 11,
      "num_retrieved_docs": 25,
      "answer_length": 1176,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document provides some relevant information about the differences between Logistic Regression and SVMs, particularly regarding prediction types and computational considerations. However, it misses key aspects of the query, such as interpretability, sensitivity to outliers, and specific scenarios where Logistic Regression is preferred over SVMs. The document is somewhat relevant but incomplete.",
          "faithfulness": "The answer includes claims about interpretability, sensitivity to outliers, and computational efficiency that are not supported by the provided context. The context does not mention these aspects, and the answer introduces unsupported details (e.g., references to 'Source 1 - Page 9' or 'Source 3 - Page 10' which are not part of the retrieved context). There are also minor contradictions, such as the claim about binary vs. continuous targets, which is not clearly addressed in the context.",
          "answer_quality": "The answer addresses the query and provides a structured response, but it relies on unsupported claims and lacks clarity in some areas (e.g., referencing non-existent sources). It is adequate but could be improved by sticking to the provided context and avoiding unsupported additions."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 37,
      "question": "In what scenarios would you use Logistic Regression over SVMs?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:01:10",
      "answer": "You would use **Logistic Regression** over **Support Vector Machines (SVMs)** in scenarios where:\n\n*   **Prediction of probabilities is needed**: Logistic Regression outputs a value between 0 and 1, which can be interpreted as a probability of class membership. SVMs, on the other hand, typically predict only class labels (0 or 1) and do not directly provide probability estimates [Source 1, Source 2].\n*   **Interpretability of coefficients is important**: Logistic Regression models can be interpreted by the value and sign of their estimated coefficients, offering insights into the relationship between features and the outcome. While SVMs can generate feature importance, it is less straightforward compared to decision trees [Source 3].\n*   **The target variable is binary**: Logistic Regression is specifically designed for situations where the target variable is binary (e.g., 0 or 1, true or false) [Source 1].\n*   **The model needs to be easy to explain, implement, and interpret**: Logistic Regression is generally considered easy to explain, implement, and interpret [Source 2, Source 5].\n\nHowever, it's important to note that Logistic Regression can be **prone to overfitting in high-dimensional data** and is **sensitive to outliers** because it maximizes the conditional likelihoods of the training data [Source 2, Source 5]. SVMs, in contrast, can be effective in high-dimensional spaces, handle outliers, and are less sensitive to them [Source 2].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        }
      ],
      "response_time": 2.2018628120422363,
      "num_citations": 3,
      "num_retrieved_docs": 25,
      "answer_length": 1465,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document directly addresses the comparison between Logistic Regression and SVMs, focusing on their differences and use cases. It provides relevant details that help answer the query about when to use Logistic Regression over SVMs.",
          "faithfulness": "The model answer is completely faithful to the retrieved context. It accurately reflects the information provided in the document, such as the probabilistic output of Logistic Regression, interpretability of coefficients, and its suitability for binary classification. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is excellent—clear, comprehensive, and well-structured. It addresses the query thoroughly by listing specific scenarios where Logistic Regression is preferable, while also acknowledging its limitations. The explanation is easy to follow and logically organized."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 38,
      "question": "What are some challenges associated with data and feature engineering?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:01:28",
      "answer": "Based on the provided documents, here are the key challenges associated with data and feature engineering:\n\n1. **Missing Values** [Source 2]:\n- Requires gathering more data or backfilling\n- Needs imputation techniques to infer missing values as a pre-processing step\n\n2. **Imbalanced Data** [Source 1]:\n- Occurs when one or more labels make up the majority of the dataset\n- Affects both classification and regression tasks\n- Requires specific model-level methods to address [Source 3]:\n   - Needs more robust models that penalize wrong classifications of minority class\n   - Solutions are often model-specific\n\n3. **Feature Selection Challenges**:\n- **For Intrinsic Feature Selection** [Source 2]:\n   - Model-dependent\n   - Limited choice of models\n   - While fast, these methods are constrained by the specific models they work with\n\n4. **Computational Issues** [Source 1]:\n- Significant computation time when dealing with large number of features\n- Potential for overfitting features to the training data\n\nThese challenges require careful consideration during the data preprocessing and model development phases to ensure robust and reliable machine learning solutions.",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        }
      ],
      "response_time": 9.78743863105774,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1171,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key challenges like missing values, imbalanced data, computational issues, and feature selection. However, the context is fragmented and lacks depth on some aspects (e.g., no explicit mention of 'feature engineering' beyond selection).",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately citing challenges like missing values, imbalanced data, and computational time. Minor additions (e.g., 'imputation techniques' for missing values) are reasonable inferences but not explicitly stated in the context. No contradictions are present.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query comprehensively. It organizes challenges logically and provides actionable insights (e.g., model-level methods for imbalanced data). However, it could slightly improve by explicitly linking 'feature engineering' to broader challenges beyond selection."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 38,
      "question": "What are some challenges associated with data and feature engineering?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:01:38",
      "answer": "**Challenges associated with data and feature engineering** based on the provided context are:\n\n- **Significant computation time** when dealing with a large number of features [Source 1].\n- **Potential to overfit the features to the training data** [Source 1].\n- **Imbalanced Data**, where one or more labels dominate the dataset, can pose challenges for both classification and regression tasks [Source 1].\n- **Missing Values** in the data require strategies such as gathering more data or imputation based on prior knowledge [Source 2].\n- **Model-dependent** limitations and a restricted choice of models in **Filter Feature Selection Methods** [Source 2].\n\nThese challenges highlight the complexities and considerations involved in data and feature engineering processes as outlined in the provided sources.",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        }
      ],
      "response_time": 2.683770179748535,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 810,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key challenges like computation time, overfitting, imbalanced data, and missing values. However, the context is fragmented and incomplete (e.g., cut-off sections like 'Support Vector Ma'), which slightly limits full relevance.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the challenges mentioned (e.g., computation time, overfitting, imbalanced data). The inclusion of 'Model-dependent limitations' is slightly unsupported as it is not explicitly stated in the provided context, but this is minor.",
          "answer_quality": "The answer is clear, well-structured, and addresses most challenges comprehensively. However, it could be slightly improved by elaborating on the implications of these challenges or addressing the incomplete context (e.g., the cut-off section)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 38,
      "question": "What are some challenges associated with data and feature engineering?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:01:47",
      "answer": "Some challenges associated with data and feature engineering include:\n\n*   **Missing Values**: This problem requires strategies like gathering more data to backfill or inferring missing values through imputation, which is often a pre-processing step [Source 2].\n*   **Imbalanced Data**: This occurs when one or more labels constitute the majority of the dataset, leaving fewer examples for other labels, affecting both classification and regression tasks [Source 1]. This challenge can be addressed by choosing appropriate evaluation metrics or making the model more robust to class imbalance [Source 3].\n*   **Feature Selection**:\n    *   **Intrinsic methods** are model-dependent and limit the choice of models [Source 2].\n    *   **Filter methods** can involve significant computation time when the number of features is large and have the most potential to overfit the features to the training data [Source 1].\n*   **Overfitting**:\n    *   This is a concern in high-dimensional data for models like Support Vector Machines and Linear Regression [Source 3].\n    *   When the number of features greatly exceeds the number of samples, careful selection of Kernel functions and regularization terms is crucial to avoid overfitting [Source 4].",
      "retrieved_docs": [
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with exter..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 2.105654001235962,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 1242,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key challenges like missing values, imbalanced data, feature selection, and overfitting. However, the context is fragmented and lacks a structured overview, which slightly limits its comprehensiveness.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting challenges like missing values, imbalanced data, and overfitting. However, it includes minor unsupported additions (e.g., 'choosing appropriate evaluation metrics' and 'making the model more robust to class imbalance' are not explicitly mentioned in the context).",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers most challenges comprehensively but could be slightly more detailed (e.g., explaining imputation methods or kernel functions) to achieve a perfect score."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 39,
      "question": "How does model comparison aid in selecting the best machine learning model?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:02:09",
      "answer": "Based on the provided context, I'll explain how model comparison helps in selecting the best machine learning model by highlighting the key differences between various models:\n\n## Key Model Comparisons\n\n1. **Random Forest vs. Gradient Boosting**\n- Training approach:\n  - Random Forest: Trees trained independently\n  - Gradient Boosting: Trees trained sequentially to correct previous errors\n- Overfitting characteristics:\n  - Random Forest: Less prone to overfitting due to averaging multiple tree predictions\n  - Gradient Boosting: More prone to overfitting due to sequential training\n[Source 1]\n\n2. **Decision Tree vs. Random Forest**\n- Structure:\n  - Decision Tree: Single tree trained on dataset\n  - Random Forest: Ensemble of independently trained trees\n- Interpretability:\n  - Decision Tree: More interpretable with simple rules\n  - Random Forest: Interpreted through feature importance, less straightforward\n[Source 6, Source 9]\n\n3. **SVM vs. Logistic Regression**\n- Prediction capabilities:\n  - SVM: Predicts only class labels\n  - Logistic Regression: Predicts probabilities between 0 and 1\n- Loss functions:\n  - SVM: Uses hinge loss\n  - Logistic Regression: Uses cross-entropy loss\n[Source 9, Source 13]\n\n4. **Random Forest vs. Logistic Regression**\n- Structure differences:\n  - Random Forest: Ensemble of decision trees\n  - Logistic Regression: Simple, linear model\n- Overfitting characteristics:\n  - Random Forest: More prone to overfitting with infinite tree growth\n  - Logistic Regression: Less prone to overfitting due to simpler structure\n[Source 2]\n\n## Benefits of Model Comparison\n\n1. **Performance Evaluation**\n- Allows assessment through various metrics:\n  - Precision, recall, F1 score\n  - ROC curves and Precision-Recall curves\n  - Accuracy (though should be avoided for imbalanced data)\n[Source 1, Source 7]\n\n2. **Understanding Trade-offs**\nEach model has distinct pros and cons that can be evaluated based on:\n- Computational efficiency\n- Interpretability\n- Prediction accuracy\n- Training speed\n- Memory requirements\n[Source 2, Source 6]\n\nModel comparison enables informed decision-making by helping practitioners understand the strengths and limitations of different algorithms for their specific use case, data characteristics, and requirements.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 15.126468896865845,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 2270,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain fragmented and incomplete information about machine learning models, primarily focusing on specific models like Random Forest and SVM. While the context touches on some aspects of model comparison (e.g., overfitting, dimensionality), it does not comprehensively address how model comparison aids in selecting the best machine learning model. Key aspects of the query, such as systematic evaluation methods or broader comparison frameworks, are missing.",
          "faithfulness": "The answer includes several claims and comparisons (e.g., Random Forest vs. Gradient Boosting, SVM vs. Logistic Regression) that are not fully supported or mentioned in the retrieved context. For example, the context does not provide details about Gradient Boosting, Decision Trees, or Logistic Regression, nor does it discuss metrics like precision, recall, or ROC curves. The answer introduces significant unsupported information.",
          "answer_quality": "The answer is well-structured and addresses the query in a general sense, but it relies heavily on external knowledge not present in the retrieved context. While the explanation of model comparison is clear and logical, the lack of grounding in the provided context reduces its completeness and reliability. The answer could be improved by focusing only on the information available in the retrieved documents."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 39,
      "question": "How does model comparison aid in selecting the best machine learning model?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:02:23",
      "answer": "Model comparison plays a crucial role in selecting the best **machine learning** model by evaluating the performance of different models based on various criteria. Here's how model comparison aids in selecting the best machine learning model based on the provided context:\n\n1. **Random Forest vs. Gradient Boosting**:\n   - **Random Forest**:\n     - **Overfitting**: Less prone to overfitting as it averages predictions from many trees, reducing variance [Source 1].\n     - **Hyperparameter**: Fewer hyperparameters compared to Gradient Boosting [Source 1].\n     - **Parallelization**: Can be trained in parallel, aiding efficiency [Source 1].\n   - **Gradient Boosting**:\n     - **Accuracy**: Produces very accurate models, outperforming Random Forest in accuracy [Source 1].\n     - **Data Pre-processing**: No data pre-processing required [Source 1].\n     - **Interpretability**: Sacrifices interpretability for accuracy, less interpretative in nature [Source 1].\n\n2. **Decision Tree vs. Random Forest**:\n   - **Decision Tree**:\n     - **Prediction**: Makes predictions based on a series of rules trained on the dataset [Source 1].\n     - **Overfitting**: More prone to overfitting as it grows in depth [Source 3].\n   - **Random Forest**:\n     - **Prediction**: Final prediction is an average of predictions from all trees [Source 1].\n     - **Hyperparameter**: More hyperparameters compared to Decision Trees [Source 2].\n     - **Interpretability**: Models can be interpreted based on feature importance [Source 2].\n\n3. **K-means vs. Gaussian Mixture Model (GMM)**:\n   - **K-means**:\n     - **Clustering**: Hard clustering algorithm assigning each data point to a single cluster [Source 4].\n     - **Overfitting**: More prone to overfitting if individual trees grow infinitely [Source 4].\n   - **Gaussian Mixture Model**:\n     - **Clustering**: Soft clustering algorithm providing probability of belonging to each cluster [Source 4].\n     - **Overfitting**: Less prone to overfitting than random forests [Source 4].\n   \nIn summary, by comparing models like **Random Forest vs. Gradient Boosting**, **Decision Tree vs. Random Forest**, and **K-means vs. Gaussian Mixture Model**, one can assess factors like overfitting, hyperparameters, parallelization, interpretability, and clustering methods to determine the best machine learning model for a specific task. Evaluation metrics and considerations such as overfitting, interpretability, and performance in handling outliers are vital in this selection process [Source 1-15].",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versat..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 6.218103885650635,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 2526,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain some relevant information about model comparison (e.g., Random Forest vs. Gradient Boosting, Decision Trees), but the context provided is fragmented and incomplete. It does not comprehensively cover how model comparison aids in selecting the best machine learning model, missing key aspects like evaluation metrics, trade-offs, or general methodologies for comparison.",
          "faithfulness": "The answer includes several unsupported claims and inaccuracies. For example, it incorrectly attributes overfitting tendencies to K-means and misrepresents the relationship between Random Forest and Gradient Boosting in the provided context. Some details (e.g., hyperparameters, parallelization) are loosely connected to the retrieved context but are not explicitly supported by it. The answer also introduces comparisons (e.g., K-means vs. GMM) that are not present in the retrieved documents.",
          "answer_quality": "The answer addresses the query and provides a structured comparison of models, but it is incomplete and contains inaccuracies. The explanation of how model comparison aids in selection is generic and not deeply tied to the query. The structure is clear, but the content lacks depth and precision, particularly in linking the comparisons to the broader goal of model selection."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 39,
      "question": "How does model comparison aid in selecting the best machine learning model?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:02:33",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about how model comparison aids in selecting the best machine learning model. The documents do mention \"Model Comparison\" as a topic [Source 1] and provide comparisons between specific models like Random Forest vs. Gradient Boosting [Source 2], Decision Tree vs. Random Forest [Source 6, Source 9], Linear Regression vs. Logistic Regression [Source 3], SVMs vs. Logistic Regression [Source 9, Source 13], Logistic Regression vs. Random Forest [Source 4], and K-means vs. Gaussian Mixture Model [Source 4], but they do not explain the general aid of model comparison in selecting the best model.",
      "retrieved_docs": [
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 1.4008121490478516,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 702,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents mention model comparisons (e.g., Random Forest vs. Gradient Boosting) but do not explicitly explain how model comparison aids in selecting the best machine learning model. The content is tangentially related but misses the core query.",
          "faithfulness": "The answer accurately reflects the content of the retrieved documents, acknowledging their lack of information on the general role of model comparison in selection. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is clear, well-structured, and honest about the limitations of the retrieved context. It could be slightly improved by suggesting alternative ways to address the query (e.g., general principles of model comparison), but it fulfills its role effectively."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 40,
      "question": "What are the pros and cons of tree-based models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:02:48",
      "answer": "Based on the provided documents, here are the pros and cons of tree-based models, specifically focusing on Decision Trees, Random Forests, and Gradient Boosting:\n\n## Decision Trees\n**Pros:**\n- Easy to understand and interpret\n- Requires little data pre-processing\n- Doesn't require feature selection\n- Efficient in prediction (logarithmic cost relative to training examples)\n[Source 1]\n\n**Cons:**\n- Prone to overfitting, especially as tree depth grows\n[Source 1, Source 5]\n\n## Random Forest\n**Pros:**\n- Better generalization performance than individual decision trees due to randomness\n- Doesn't require much parameter tuning\n- Doesn't require feature selection\n- Less sensitive to outliers\n- Generates helpful feature importance\n- Less prone to overfitting (averages predictions of many trees)\n[Source 2, Source 5]\n\n**Cons:**\n- Computationally expensive\n[Source 2]\n\n## Gradient Boosting\n**Pros:**\n- Produces very accurate models (outperforms random forest in accuracy)\n- No data pre-processing required\n- Handles missing data without imputation\n[Source 2]\n\n**Cons:**\n- Sequential process that can be slow to train\n- Computationally expensive (often requires >1000 trees)\n- Sacrifices interpretability for accuracy\n- More prone to overfitting due to sequential training\n[Source 2, Source 6]",
      "retrieved_docs": [
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 9.364687204360962,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 1290,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about tree-based models, specifically Gradient Boosting, and some details about other models like Decision Trees and Random Forests. However, the context is fragmented and lacks comprehensive coverage of all tree-based models mentioned in the answer (e.g., Decision Trees and Random Forests are not fully detailed in the retrieved context).",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately reflecting the pros and cons of Gradient Boosting as mentioned. However, some details about Decision Trees and Random Forests (e.g., 'efficient in prediction,' 'better generalization performance,' 'less sensitive to outliers') are not explicitly supported by the provided context, indicating minor unsupported additions.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive, covering the pros and cons of three major tree-based models. However, it could be slightly improved by explicitly noting which details are not directly supported by the retrieved context or by providing more complete coverage of each model's characteristics."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 40,
      "question": "What are the pros and cons of tree-based models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:02:56",
      "answer": "### Pros and Cons of Tree-based Models:\n\n**Pros:**\n- **Easy to understand and interpret** [Source 1].\n- **Requires little data pre-processing** [Source 1].\n- **Doesn't require feature selection** [Source 1].\n- **Efficient in prediction** with the cost of one prediction being logarithmic in the number of examples used to train the tree [Source 1].\n- **Better generalization performance** than an individual decision tree due to randomness [Source 2].\n- **Generates feature importance** which is helpful for interpreting the results [Source 2].\n\n**Cons:**\n- **Prone to overfitting** [Source 1, Source 2].\n- **Computationally expensive** [Source 2].\n- **Sensitive to outliers** in the dataset [Source 2].\n\nThese points summarize the **pros** and **cons** of tree-based models as outlined in the provided context from various sources.",
      "retrieved_docs": [
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        }
      ],
      "response_time": 2.6078410148620605,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 832,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about tree-based models, including their pros and cons. However, the context is somewhat fragmented and includes details about other models (e.g., Support Vector Machines), which are not directly relevant to the query. Key aspects of tree-based models are covered, but the context could be more focused.",
          "faithfulness": "The answer is largely faithful to the retrieved context, accurately reflecting the pros and cons mentioned. However, there are minor unsupported additions, such as 'Doesn't require feature selection' and 'Efficient in prediction with logarithmic cost,' which are not explicitly stated in the provided context. The rest of the points are well-grounded.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive. It effectively summarizes the pros and cons of tree-based models. However, it could be slightly improved by excluding the unsupported details and ensuring all claims are directly tied to the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 40,
      "question": "What are the pros and cons of tree-based models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:03:07",
      "answer": "Here are the pros and cons of tree-based models, based on the provided context:\n\n## Tree-based Models\n\n### Decision Trees\n\n**Pros:**\n*   **Easy to understand and interpret** [Source 1].\n*   Requires **little data pre-processing** [Source 1].\n*   **Doesn't require feature selection** [Source 1].\n*   **Efficient in prediction**: the cost of one prediction is logarithmic in the number of examples used to train the tree [Source 1].\n\n**Cons:**\n*   **Prone to overfitting** [Source 1]. More prone to overfitting as the depth of the tree grows [Source 3].\n\n### Random Forest\n\n**Pros:**\n*   Has a **better generalization performance** than an individual decision tree due to randomness [Source 2].\n*   **Doesn't require much parameter tuning** [Source 2].\n*   **Doesn't require feature selection** [Source 2].\n*   **Less sensitive to outliers** in the dataset [Source 2].\n*   It generates **feature importance**, which is helpful when interpreting the results [Source 2].\n*   **Less prone to overfitting** as it averages the predictions of many different trees [Source 5, Source 6].\n*   Can be trained in **parallel** [Source 4].\n\n**Cons:**\n*   **Computationally expensive** [Source 2].\n*   Has **more hyperparameters** compared to decision trees [Source 5, Source 6].\n*   Models can be interpreted based on feature importance, which is **less straightforward** compared with decision trees [Source 3].\n\n### Gradient Boosting\n\n**Pros:**\n*   It produces **very accurate models**, outperforming random forest in accuracy [Source 2].\n*   **No data pre-processing is required** [Source 2].\n*   **Handles missing data** - imputation not required [Source 2].\n\n**Cons:**\n*   Gradient boosting is a **sequential process that can be slow to train** [Source 2].\n*   **Computationally expensive** - often requires many trees (>1000), which can be time and memory exhaustive [Source 2].\n*   **Sacrifices interpretability for accuracy** - less interpretative in nature [Source 2].\n*   **More prone to overfitting** as it's trained sequentially and can continue to fit the data until it is perfectly fit [Source 4].\n*   Has **more hyperparameters** [Source 4].\n*   **Cannot be trained in parallel** [Source 4].",
      "retrieved_docs": [
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "ressionPros • Simple.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradie..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue ..."
        }
      ],
      "response_time": 2.4590871334075928,
      "num_citations": 4,
      "num_retrieved_docs": 8,
      "answer_length": 2192,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about tree-based models, including their pros and cons. However, the context is somewhat fragmented and includes details about other models (e.g., Support Vector Machines), which are not directly relevant to the query. The key aspects of tree-based models are covered, but the context could be more focused.",
          "faithfulness": "The answer is largely faithful to the retrieved context, accurately reflecting the pros and cons of tree-based models as described. However, there are minor unsupported additions (e.g., 'Efficient in prediction' and 'Doesn't require feature selection' for Decision Trees) that are not explicitly mentioned in the provided context. The majority of the content is well-grounded.",
          "answer_quality": "The answer is excellent in quality. It is well-structured, clear, and comprehensive, covering the pros and cons of Decision Trees, Random Forest, and Gradient Boosting in detail. The inclusion of sources adds credibility, and the response directly addresses the query with high clarity and completeness."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 41,
      "question": "What are the implications of using Linear Models in machine learning?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:03:21",
      "answer": "Based on the provided context, I can discuss the implications of Linear Models, specifically focusing on Logistic Regression as it's the main linear model discussed in the documents. Here are the key implications:\n\n**Advantages:**\n- **Easy to explain, implement and interpret** [Source 2]\n- **Provides probability outputs** - outputs the probability of class membership [Source 2]\n- **Parametric model** - learns a set of parameters from the dataset to make predictions [Source 3]\n\n**Disadvantages:**\n- **Linear assumptions** - makes linear assumptions between the features and the target [Source 1]\n- **Overfitting risk** - prone to overfitting in high-dimensional data [Source 1, Source 2]\n- **Sensitivity to outliers** - maximizes the conditional likelihoods of the training data, making it prone to outliers [Source 2]\n\nFor specific use cases, Logistic Regression:\n- **Is used for binary target variables** rather than continuous predictions [Source 3]\n- **Uses a logistic function** that gives a value between 0 and 1, which can be interpreted as a probability [Source 3]\n\nNote: While the documents mention Linear Models in general, most of the specific information provided focuses on Logistic Regression as an example of a linear model.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        }
      ],
      "response_time": 7.9759321212768555,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1243,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, as they address the pros and cons of linear models (specifically Logistic Regression). However, they do not comprehensively cover all implications of linear models in general, focusing more on Logistic Regression and Support Vector Machines (SVM) with some details missing for broader linear models.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the advantages, disadvantages, and specific use cases mentioned. However, there is a minor unsupported addition where the answer claims Logistic Regression 'maximizes the conditional likelihoods of the training data,' which is not explicitly stated in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively summarizes the implications of linear models (with a focus on Logistic Regression) and organizes the information into advantages, disadvantages, and use cases. However, it could be slightly improved by clarifying the distinction between linear models in general and Logistic Regression specifically."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 41,
      "question": "What are the implications of using Linear Models in machine learning?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:03:30",
      "answer": "The implications of using **Linear Models** in machine learning, based on the provided context, are as follows:\n\n- **Pros**:\n  - Easy to explain, implement, and interpret [Source 1, Source 2].\n  - Linear Models like **Logistic Regression** output the probability of class membership [Source 2].\n  - Linear Models, such as **Linear Regression**, are used when the target is continuous [Source 3].\n  - Linear Models are **efficient in prediction** and require little data pre-processing [Source 2].\n\n- **Cons**:\n  - Linear Models, such as Logistic Regression, **maximize the conditional likelihoods of the training data**, making them prone to outliers [Source 2].\n  - Linear Models make **linear assumptions between the features and the target** which may not capture complex relationships in the data [Source 1].\n  - Linear Models like Logistic Regression are **prone to overfitting in high-dimensional data** [Source 1, Source 2].\n  \nThese implications highlight both the advantages and disadvantages of incorporating Linear Models, such as Logistic Regression and Linear Regression, in machine learning tasks.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector Ma..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the targ..."
        }
      ],
      "response_time": 2.8483591079711914,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1111,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses the pros and cons of linear models. However, it misses some broader implications (e.g., computational efficiency, use cases in specific domains) that could have been covered if more comprehensive documents were retrieved.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes unsupported details (e.g., mentions of 'Logistic Regression outputting probabilities' and 'Linear Regression for continuous targets' are not present in the provided context). Some claims are extrapolated beyond the given information.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It covers the key pros and cons of linear models. However, it could be slightly improved by sticking strictly to the retrieved context and avoiding unsupported additions."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 41,
      "question": "What are the implications of using Linear Models in machine learning?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:03:37",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about the implications of using Linear Models in machine learning. While \"Linear Models\" are mentioned [Source 1], the context provided only lists pros and cons for specific linear models like Logistic Regression [Source 1, Source 2, Source 3] and discusses their differences with other models, but does not generalize to \"Linear Models\" as a whole.",
      "retrieved_docs": [
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        }
      ],
      "response_time": 1.4088850021362305,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 457,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents mention linear models (e.g., Logistic Regression) and provide some pros and cons, which are tangentially related to the query. However, they do not comprehensively address the broader implications of using Linear Models in machine learning, missing key aspects like theoretical foundations, use cases, or comparisons with non-linear models.",
          "faithfulness": "The answer accurately reflects the limitations of the retrieved context. It correctly notes that the documents do not provide a generalized discussion on the implications of Linear Models, only fragmented details about specific models. No unsupported claims or contradictions are present.",
          "answer_quality": "The answer is clear, well-structured, and honest about the lack of relevant information. It could be slightly improved by suggesting where such information *might* be found (e.g., other sections of the document or external sources), but it fully addresses the query's constraints."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 42,
      "question": "How does feature selection improve the efficiency of machine learning models?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:03:59",
      "answer": "Based on the provided context, I'll explain how feature selection improves machine learning model efficiency:\n\n## Purpose\nFeature selection is a **pre-processing step** that involves selecting a subset of the original features for model training [Source 1].\n\n## Main Methods of Feature Selection\n\n1. **Intrinsic Feature Selection Methods**\n- Naturally embedded within training process of models like tree-based models and regularization models\n- **Pros**:\n  - Fast implementation\n  - No external feature tools needed\n  - Direct connection between feature selection and object function\n- **Cons**:\n  - Model-dependent\n  - Limited choice of models\n[Source 1]\n\n2. **Filter Feature Selection Methods**\n- Selects features that correlate well with target variables\n- Uses univariate statistical analysis and feature importance scores\n- Process is performed once and is independent of model algorithm\n[Source 1]\n\n3. **Wrapper Feature Selection Methods**\n- Uses an iterative process\n- Repeatedly adds subset features to the model\n- Uses resulting model performance to guide next subset selection\n- **Pros**:\n  - Searches for wider variety of feature subsets than other methods\n  - Fast and simple\n  - Effective at capturing large trends\n- **Cons**:\n  - Significant computation time with large feature sets\n  - Highest potential for overfitting features to training data\n  - Tends to select redundant features\n  - Ignores relationships among features\n[Source 3]\n\n## Benefits of Feature Selection\n- Improves model efficiency by reducing the number of input features\n- Helps avoid overfitting by removing irrelevant or redundant features\n- Can improve model performance by focusing on the most important features\n[Source 1, Source 3]\n\nNote that some algorithms, such as Random Forest and Gradient Boosting, don't require feature selection as part of their preprocessing steps, as they have built-in feature selection capabilities [Source 2].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 14.590651750564575,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1930,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of feature selection methods and their impact on machine learning model efficiency. However, the context provided is somewhat fragmented (e.g., references to 'Source 1' and 'Source 3' without full clarity on their content), which slightly limits the comprehensiveness of the relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately summarizing the methods and benefits of feature selection. However, it includes minor unsupported additions (e.g., details about 'Filter Feature Selection Methods' and some pros/cons of wrapper methods) that are not explicitly mentioned in the provided context. The claim about Random Forest and Gradient Boosting is also not directly supported by the given context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. It provides a good overview of feature selection methods and their benefits. However, it could be slightly more concise and avoid introducing details not explicitly supported by the retrieved context to improve completeness and precision."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 42,
      "question": "How does feature selection improve the efficiency of machine learning models?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:04:10",
      "answer": "Feature selection improves the efficiency of machine learning models in the following ways based on the provided context:\n\n1. **Intrinsic Feature Selection Methods**:\n   - Have feature selection naturally embedded with the training process of models like tree-based models and regularization models.\n   - Pros:\n     - Fast and does not require an external feature tool.\n     - Provides a direct connection between feature selection and the objective function. [Source 1]\n\n2. **Filter Feature Selection Methods**:\n   - Select features that correlate well with target variables using methods like univariate statistical analysis and feature importance scores.\n   - The process is performed only once and is independent of the model algorithm.\n   - Cons:\n     - Model-dependent, limiting the choice of models. [Source 1]\n\n3. **Wrapper Feature Selection Methods**:\n   - An iterative process that adds subset features to the model based on resulting model performance.\n   - Pros:\n     - Searches for a wider variety of feature subsets compared to other methods.\n     - Fast and simple, effective at capturing large trends in the dataset.\n   - Cons:\n     - Significant computation time when dealing with a large number of features.\n     - Has the most potential to overfit the features to the training data. [Source 3]\n\n4. **Machine Learning Models**:\n   - **Random Forest**:\n     - Doesn't require feature selection, reducing the need for manual feature selection efforts.\n     - Provides feature importance, aiding in the interpretation of results. [Source 2]\n   - **Gradient Boosting**:\n     - Does not require data preprocessing.\n     - Handles missing data without the need for imputation. [Source 2]\n\nTherefore, feature selection methods like intrinsic, filter, and wrapper methods, along with utilizing models like Random Forest and Gradient Boosting, can significantly enhance the efficiency of machine learning models by streamlining the feature selection process and improving interpretability. [Source 1], [Source 2], [Source 3]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 4.504992961883545,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2033,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document provides some relevant information about intrinsic feature selection methods but does not comprehensively cover filter or wrapper methods, nor does it explicitly explain how feature selection improves efficiency. The answer extrapolates beyond the provided context, which only briefly mentions intrinsic methods.",
          "faithfulness": "The answer includes details about filter and wrapper methods, as well as specific models like Random Forest and Gradient Boosting, which are not supported by the retrieved context. The context only mentions intrinsic methods, so the answer introduces unsupported claims and minor contradictions.",
          "answer_quality": "The answer addresses the query and provides a structured breakdown of feature selection methods, but it includes information not grounded in the retrieved context. While the response is clear, it could be improved by sticking strictly to the provided context and ensuring completeness within those bounds."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 42,
      "question": "How does feature selection improve the efficiency of machine learning models?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:04:15",
      "answer": "I cannot answer this question based on the provided PDF documents. The documents do not contain information about how **feature selection improves the efficiency of machine learning models**. The provided text defines feature selection and lists different methods but does not elaborate on its impact on model efficiency [Source 1, Source 3].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, dec..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue ..."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 12,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 13,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 14,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1,
          "text_snippet": "MACHINELEARNINGINTERVIEWCHEAT SHEET"
        }
      ],
      "response_time": 0.9931869506835938,
      "num_citations": 14,
      "num_retrieved_docs": 40,
      "answer_length": 342,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 2,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document mentions feature selection methods but does not address how they improve the efficiency of machine learning models. While it is somewhat related to the query, it misses the key aspect of explaining the impact on efficiency.",
          "faithfulness": "The model answer accurately reflects the content of the retrieved document. It correctly states that the document does not provide the requested information about the efficiency improvements of feature selection.",
          "answer_quality": "The answer is clear, well-structured, and correctly identifies the lack of relevant information in the retrieved documents. However, it could be slightly improved by suggesting alternative sources or general knowledge about the topic (though this is not required for faithfulness)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 43,
      "question": "What are the main advantages of using Random Forest for classification?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:04:31",
      "answer": "Based on the provided documents, here are the main advantages of Random Forest:\n\n**Key Advantages of Random Forest:**\n- **Better generalization** performance compared to individual decision trees due to randomness [Source 2]\n- **Low maintenance** in terms of setup and tuning:\n  - Doesn't require much parameter tuning\n  - Doesn't require feature selection\n  - Doesn't require extensive data pre-processing\n  [Source 2]\n\n- **Robust and reliable**:\n  - Less sensitive to outliers in the dataset\n  - Less prone to overfitting compared to decision trees, as it averages predictions from many different trees [Source 3]\n\n- **Interpretability**: Generates feature importance which helps in interpreting the results [Source 2]\n\nThe model achieves these benefits through its **ensemble structure**, where multiple decision trees work together to make predictions [Source 1, Source 2], and the final prediction is determined by averaging the predictions of all trees [Source 1].\n\nThe main trade-off noted in the sources is that Random Forests can be **computationally expensive** [Source 2], and they have more hyperparameters to tune compared to simple decision trees [Source 3].",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 8.801188230514526,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1172,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document directly addresses the advantages of Random Forest for classification, covering key aspects such as generalization performance, low maintenance, and robustness. The context is highly relevant and comprehensive for the query.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the advantages mentioned (e.g., better generalization, low maintenance, robustness). However, it includes minor unsupported additions like 'feature importance' and 'ensemble structure' details that are not explicitly stated in the provided context, though they are generally true.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It addresses all key advantages, provides additional context (e.g., trade-offs), and is logically organized. The inclusion of sources adds credibility."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 43,
      "question": "What are the main advantages of using Random Forest for classification?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:04:37",
      "answer": "**Main advantages of using Random Forest for classification** based on the provided context are:\n\n- **Better generalization performance** compared to an individual decision tree due to randomness [Source 1].\n- **Doesn’t require much parameter tuning** [Source 1].\n- **Doesn't require feature selection** [Source 1].\n- **Less sensitive to outliers in the dataset** [Source 1].\n- **Generates feature importance** which aids in interpreting the results [Source 1].\n\nThese advantages highlight the strengths of Random Forest in classification tasks, showcasing its robustness, simplicity in implementation, and effectiveness in handling various types of data without the need for extensive preprocessing or tuning.",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        }
      ],
      "response_time": 1.8713030815124512,
      "num_citations": 1,
      "num_retrieved_docs": 8,
      "answer_length": 710,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query as it directly addresses the advantages of Random Forest for classification. However, it is slightly incomplete (e.g., missing details like handling non-linear relationships or parallelization), which prevents a perfect score.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes unsupported claims (e.g., 'Doesn't require feature selection,' 'Less sensitive to outliers,' and 'Generates feature importance' are not explicitly mentioned in the provided context).",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively. However, it could be improved by strictly adhering to the retrieved context and avoiding unsupported additions."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 43,
      "question": "What are the main advantages of using Random Forest for classification?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:04:44",
      "answer": "The main advantages of using Random Forest for classification include:\n\n*   **Better generalization performance** than an individual decision tree due to randomness [Source 1].\n*   **Less prone to overfitting** compared to a single decision tree, as it averages the predictions of many different trees [Source 2].\n*   **Doesn't require much parameter tuning** [Source 1].\n*   **Doesn't require feature selection** [Source 1].\n*   **Less sensitive to outliers** in the dataset [Source 1].\n*   It generates **feature importance**, which is helpful when interpreting the results [Source 1].\n*   Models can be **interpreted based on feature importance** [Source 2].",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 2.0839686393737793,
      "num_citations": 2,
      "num_retrieved_docs": 8,
      "answer_length": 661,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document is highly relevant to the query, as it directly addresses the advantages of Random Forest for classification. However, it is somewhat incomplete (e.g., missing details like handling of outliers or feature importance), which prevents a perfect score.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details (e.g., 'Less prone to overfitting' and 'Models can be interpreted based on feature importance' are not explicitly mentioned in the provided context). The attribution to 'Source 2' is also unclear since only one source is provided.",
          "answer_quality": "The answer is clear, well-structured, and covers most key advantages of Random Forest. However, it could be slightly improved by ensuring all claims are directly supported by the provided context and avoiding unsupported additions."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 44,
      "question": "How do evaluation metrics impact the assessment of model performance?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:05:07",
      "answer": "Based on the provided documents, here's a comprehensive analysis of how evaluation metrics impact model performance assessment:\n\n## Key Types of Evaluation Metrics\n\n### Classification Metrics\n1. **Accuracy**\n- Measures % of predicted labels that match true labels\n- **Warning**: Misleading when classes are imbalanced [Source 1]\n- Formula: (TP + TN)/(TP + TN + FP + FN) [Source 5]\n\n2. **Precision and Recall**\n- **Precision**: % of accurate positives compared to total claimed positives\n- **Recall**: % of claimed positives compared to actual positives in data\n- Both are important for binary classification problems [Source 1, Source 7]\n\n3. **F1 Score**\n- Harmonic mean of precision and recall\n- Particularly useful for imbalanced datasets [Source 4, Source 7]\n\n4. **ROC Curve and Precision-Recall Curve**\n- **ROC Curve**:\n  - Plots False Positive Rate vs Recall\n  - Area under curve (AUC) provides threshold-independent comparison\n- **Precision-Recall Curve**:\n  - More suitable for imbalanced data\n  - Emphasizes positive class performance [Source 1, Source 4]\n\n### Regression Metrics\n\n1. **Mean Squared Error (MSE)**\n- Pros:\n  - Penalizes large errors\n  - Mathematically convenient for gradient calculation\n- Cons:\n  - Sensitive to outliers\n  - Difficult to interpret due to squared units [Source 4]\n\n2. **Mean Absolute Error (MAE)**\n- Pros:\n  - Simpler to interpret\n  - Less sensitive to outliers than MSE\n- Cons:\n  - Not differentiable at 0, making optimization more difficult [Source 4]\n\n3. **R-squared (R²)**\n- Describes percentage of target variable variation explained by model\n- Pros:\n  - Easy to interpret\n  - Good for model comparison\n- Cons:\n  - Always increases with new variables\n  - Doesn't necessarily indicate predictive power [Source 3]\n\n## Impact on Model Assessment\n\n1. **Imbalanced Data Considerations**\n- Choose metrics carefully when dealing with imbalanced datasets\n- Precision-Recall curve is preferred over ROC curve for imbalanced data [Source 1]\n\n2. **Model-Specific Considerations**\n- Different metrics may be more appropriate for different types of models\n- Consider the specific problem context when selecting evaluation metrics [Source 2]\n\n3. **Interpretation**\n- Some metrics are easier to interpret than others\n- Trade-offs between mathematical convenience and interpretability should be considered [Source 4]\n\nThe choice of evaluation metrics significantly impacts how we assess model performance, and selecting appropriate metrics is crucial for:\n- Accurately measuring model effectiveness\n- Addressing specific problem characteristics (like class imbalance)\n- Ensuring meaningful model comparison\n- Making informed decisions about model selection and optimization",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        }
      ],
      "response_time": 17.499731302261353,
      "num_citations": 5,
      "num_retrieved_docs": 40,
      "answer_length": 2702,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key evaluation metrics for model performance assessment, including classification and regression metrics, as well as considerations for imbalanced data. The context directly addresses the query's focus on how metrics impact model evaluation.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately citing and explaining the metrics mentioned in the documents. However, there are minor additions (e.g., formulas like (TP + TN)/(TP + TN + FP + FN) and some pros/cons) that are not explicitly stated in the provided context but are generally correct and widely known. No contradictions are present.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It organizes the information logically (classification vs. regression metrics), provides detailed explanations, and addresses the broader impact of metrics on model assessment. The inclusion of warnings (e.g., accuracy's limitations) and trade-offs adds depth to the response."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 44,
      "question": "How do evaluation metrics impact the assessment of model performance?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:05:20",
      "answer": "Evaluation metrics play a crucial role in assessing **model performance** in machine learning. The impact of evaluation metrics on model assessment can be summarized as follows:\n\n1. **Precision-Recall Curve**:\n   - Gives more importance to the positive class, especially useful for dealing with imbalanced data [Source 1].\n   - Emphasizes predictions the model got right out of the total number it predicted to be positive (precision) and is sensitive to the positive class [Source 3].\n\n2. **Accuracy**:\n   - It is the percentage of predicted labels that match the true labels [Source 5].\n   - Can be misleading when classes are imbalanced [Source 1].\n\n3. **ROC Curve**:\n   - Treats both classes equally and is less sensitive to model improvement on the minority class [Source 1].\n   - Compares binary classification models based on false positive rate and recall (TPR) [Source 3].\n\n4. **F1 Score**:\n   - Represents the harmonic mean of precision and recall [Source 5].\n   - Measures a model's performance with respect to the positive class in a binary classification problem [Source 1].\n\n5. **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)**:\n   - MSE measures the average squared distance between predictions and true values, while MAE measures the average absolute difference [Source 4].\n   - MSE penalizes large errors and is mathematically convenient for obtaining gradients [Source 6].\n   - MAE is relatively simple to interpret and less sensitive to outliers compared to MSE [Source 4].\n\n6. **Confusion Matrix**:\n   - Provides insights into accuracy, precision, recall, specificity, and the F1 score [Source 5].\n   - Helps understand the model's performance in terms of true positives, true negatives, false positives, and false negatives [Source 2].\n\nEvaluation metrics are essential as they provide a quantitative measure of how well a model is performing, especially in scenarios with imbalanced data or varying priorities for different classes within the data. Each metric has its strengths and weaknesses, and the choice of metric depends on the specific characteristics of the dataset and the goals of the modeling task.",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compare..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        }
      ],
      "response_time": 5.3888328075408936,
      "num_citations": 6,
      "num_retrieved_docs": 40,
      "answer_length": 2145,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key evaluation metrics like precision, recall, F1 score, and accuracy. However, some metrics mentioned in the answer (e.g., ROC Curve, MSE, MAE, Confusion Matrix) are not explicitly present in the provided context, which slightly reduces the relevance score.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes details about metrics (e.g., ROC Curve, MSE, MAE, Confusion Matrix) that are not explicitly supported by the provided documents. Some explanations (e.g., precision-recall curve's emphasis on the positive class) align well with the context.",
          "answer_quality": "The answer is clear, well-structured, and comprehensive in addressing the impact of evaluation metrics on model performance. However, the inclusion of unsupported metrics slightly detracts from its completeness and could introduce minor inaccuracies."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 44,
      "question": "How do evaluation metrics impact the assessment of model performance?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:05:32",
      "answer": "Evaluation metrics are crucial for assessing model performance, with different metrics suited for various tasks and data characteristics.\n\n**General Impact:**\n*   **Clarity and Interpretability:** Metrics like **R-squared** are easy to interpret and allow for straightforward comparison of different models [Source 3]. Similarly, **Mean Absolute Error (MAE)** is relatively simple to interpret [Source 4].\n*   **Task-Specific Application:** Choosing appropriate evaluation metrics is essential for the task at hand [Source 1].\n    *   For **regression tasks**, metrics include **Mean Squared Error (MSE)**, **Root Mean Square Error (RMSE)**, **Mean Absolute Error (MAE)**, and **R-squared** [Source 2, Source 6, Source 8].\n    *   For **classification tasks**, metrics include **Confusion Matrix**, **Accuracy**, **Precision**, **Recall**, **Specificity**, **F1 Score**, **ROC Curve**, and **Precision-Recall Curve** [Source 3, Source 7, Source 4].\n\n**Impact on Specific Scenarios:**\n\n*   **Imbalanced Data:**\n    *   **Precision-Recall curves** are particularly helpful for dealing with imbalanced data, as they give more importance to the positive class [Source 1, Source 4].\n    *   **Precision**, **recall**, and **F1 measure** a model’s performance with respect to the positive class in a binary classification problem [Source 1].\n    *   **Accuracy** is misleading when classes are imbalanced and should be avoided in such cases [Source 1].\n    *   The **AUC of the ROC curve** treats both classes equally and is less sensitive to model improvement on the minority class, making it less ideal for imbalanced datasets [Source 1].\n\n*   **Sensitivity to Errors and Outliers:**\n    *   **Mean Squared Error (MSE)** penalizes large errors but is sensitive to outliers due to its squaring of errors [Source 4].\n    *   **Mean Absolute Error (MAE)** is less sensitive to outliers compared to MSE [Source 4].\n\n*   **Understanding Model Behavior:**\n    *   **ROC Curve** provides a systematic metric for comparing binary classification models, independent of the threshold setting [Source 2].\n    *   **Precision-Recall Curve** emphasizes predictions the model got right out of the total number it predicted to be positive (precision) [Source 2].\n\nBy carefully selecting and interpreting these metrics, data scientists can gain a comprehensive understanding of how well a model performs and identify areas for improvement.",
      "retrieved_docs": [
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "(a.k.a true negative rate)F1 ScoreThe harmonic mean of precision and recall. Accuracy=TP+TN+FP+FNTP+TNPrecision=TP+FPTPRecall=TP+FNTPSpecificity=TN+FPTNFalse positive rate=1−specificity spammedical EIO.IT.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegr..."
        }
      ],
      "response_time": 3.8015527725219727,
      "num_citations": 4,
      "num_retrieved_docs": 40,
      "answer_length": 2419,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, focusing on evaluation metrics for model performance, including precision, recall, F1, and their applicability to imbalanced data. The context directly addresses the core aspects of the query.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately referencing precision-recall curves, precision, recall, F1, and their relevance to imbalanced data. However, it introduces some metrics (e.g., R-squared, MAE, MSE) not explicitly mentioned in the provided context, which are minor unsupported additions.",
          "answer_quality": "The answer is excellent: clear, well-structured, and comprehensive. It covers general impacts, task-specific applications, and scenario-based considerations (e.g., imbalanced data, outliers). The explanation is detailed and logically organized."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 45,
      "question": "What steps are involved in the process of feature selection?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:05:53",
      "answer": "Based on the provided documents, there are three main methods of feature selection. Here are the details:\n\n## Types of Feature Selection Methods\n\n1. **Filter Feature Selection Methods**\n- Process is performed only once and independent of model algorithm\n- Selects features that correlate well with target variables\n- Uses methods like univariate statistical analysis and feature importance scores\n- **Pros**: Fast and simple\n- **Cons**: Tends to select redundant features and ignores relationships among features\n[Source 1, Source 2]\n\n2. **Wrapper Feature Selection Methods**\n- Uses an iterative process (like sequential feature selection)\n- Repeatedly adds subset features to the model\n- Uses resulting model performance to guide next subset selection\n- **Pros**: Can search for a wider variety of feature subsets\n- **Cons**: \n  - Significant computation time when many features are present\n  - Most potential to overfit features to training data\n[Source 1]\n\n3. **Intrinsic Feature Selection Methods**\n- Feature selection is naturally embedded within model training process\n- Commonly used in tree-based models and regularization models\n- **Pros**:\n  - Fast and no external feature tool needed\n  - Provides direct connection between feature selection and object function\n- **Cons**: Model-dependent and limited choice of models\n[Source 2]\n\n## General Process\nFeature selection is typically used as a **pre-processing step** before the actual learning process begins. The goal is to select a subset of the original features for model training [Source 2].\n\nIt's worth noting that some models, like **Decision Trees**, don't require feature selection as part of their process [Source 3], while others may benefit significantly from this step to improve performance and reduce complexity.",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 14.512119054794312,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 1785,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, as they discuss various methods of feature selection (e.g., filter, wrapper, and intrinsic methods) and their role as a pre-processing step. However, the context is somewhat fragmented and does not explicitly outline the *steps* involved in the feature selection process, which is the core of the query.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately describes the three main methods of feature selection (filter, wrapper, and intrinsic) and their characteristics without introducing unsupported claims or contradictions. The general process description also aligns with the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive. It effectively categorizes the feature selection methods and explains their pros and cons. However, it does not explicitly outline the *steps* involved in the feature selection process (e.g., defining the problem, selecting evaluation criteria, applying methods, validating results), which would have made it more complete."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 45,
      "question": "What steps are involved in the process of feature selection?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:06:05",
      "answer": "Based on the provided context from the PDF documents, the steps involved in the process of **feature selection** are as follows:\n\n1. **Wrapper Feature Selection Methods**:\n   - An iterative process like sequential feature selection adds subset features to the model and uses model performance to guide the selection of the next subset.\n   - Pros: Searches for a wider variety of feature subsets than other methods.\n   - Cons: Significant computation time with a large number of features, potential to overfit features to training data. [Source 1]\n\n2. **Intrinsic Feature Selection Methods**:\n   - Have feature selection embedded in the training process of models like tree-based models and regularization models.\n   - Pros: Fast, no external feature tool needed, direct connection between feature selection and the objective function.\n   - Cons: Model-dependent, limited choice of models. [Source 2]\n\n3. **Filter Feature Selection Methods**:\n   - Select features correlating well with target variables using methods like univariate statistical analysis and feature importance scores.\n   - This process is performed only once and is independent of the model algorithm. [Source 2]\n\n4. **Model-based imputation**:\n   - Use non-missing data to predict missing data with methods like KNN, linear regression, or trees.\n   - Omission: Removing missing observations or features with lots of missing values in the dataset. [Source 2]\n\nThese steps cover the different methods and considerations involved in the feature selection process as outlined in the provided context.\n\nReference: [Source 1], [Source 2]",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 3.878659963607788,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1598,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query about feature selection. They cover various methods (wrapper, intrinsic, and filter methods) and provide a good overview of the process. However, the context is somewhat fragmented and does not explicitly list 'steps' in a structured manner, which slightly limits the relevance.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately describing wrapper, intrinsic, and filter methods. However, it includes 'Model-based imputation' and 'Omission,' which are not directly related to feature selection steps and appear to be misplaced or unsupported by the provided context. This introduces minor inaccuracies.",
          "answer_quality": "The answer is clear, well-structured, and covers the key aspects of feature selection methods. However, the inclusion of unrelated concepts (e.g., model-based imputation) slightly detracts from its completeness and precision. The overall quality is high but could be improved by focusing strictly on feature selection steps."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 45,
      "question": "What steps are involved in the process of feature selection?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:06:14",
      "answer": "The process of **feature selection** involves selecting a subset of the original features for model training [Source 2]. There are several methods for feature selection:\n\n1.  **Filter Feature Selection Methods**:\n    *   Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores [Source 2].\n    *   This process is performed only once and is independent of the model algorithm [Source 2].\n\n2.  **Wrapper Feature Selection Methods**:\n    *   Involve an iterative process, such as sequential feature selection [Source 1].\n    *   This method repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset [Source 1].\n\n3.  **Intrinsic Feature Selection Methods**:\n    *   Have feature selection naturally embedded within the training process of models [Source 2].\n    *   Examples include tree-based models and regularization models [Source 2].",
      "retrieved_docs": [
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Eachdata point is assigned aprobability of belonging to eachcluster.InitializationRandom initialization.Can use either randominitialization or K-mean forinitialization..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        }
      ],
      "response_time": 1.4877090454101562,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 982,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key methods of feature selection (e.g., forward/backward selection, cross-validation-based methods, and intrinsic methods). However, the context is slightly fragmented and does not explicitly mention all steps or methods (e.g., filter methods) in a structured way, which the answer compensates for.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the methods mentioned (e.g., wrapper and intrinsic methods). However, it introduces 'filter methods' and 'univariate statistical analysis,' which are not explicitly stated in the provided context, though these are standard concepts in feature selection. This minor addition does not contradict the context.",
          "answer_quality": "The answer is excellent—clear, comprehensive, and well-structured. It categorizes feature selection methods logically, provides relevant examples, and addresses the query effectively. The inclusion of standard concepts (e.g., filter methods) enhances completeness without compromising accuracy."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 46,
      "question": "How does one compare SVMs and Logistic Regression for a given problem?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:06:36",
      "answer": "Based on the provided documents, here's a comprehensive comparison between SVMs and Logistic Regression:\n\n## Key Similarities\n- Both are **parametric models** that learn parameters from the dataset to make predictions\n- Both are used for **classification** tasks [Source 1]\n\n## Key Differences\n\n### Prediction Approach\n- **SVM**:\n  - Uses a linear function to make predictions\n  - Can only predict class labels (not probabilities)\n  - Finds the separating hyperplane that maximizes the distance of closest points to the margin\n  - Can learn non-linear decision boundaries using kernel tricks [Source 1, Source 2]\n\n- **Logistic Regression**:\n  - Uses a logistic function that gives values between 0 and 1\n  - Outputs can be interpreted as probabilities\n  - Finds linear decision boundary by maximizing data likelihood [Source 1, Source 2]\n\n### Loss Function\n- **SVM**: Uses **hinge loss** as the loss function to measure incorrect predictions\n- **Logistic Regression**: Uses **cross-entropy loss** to measure differences between predicted and observed distributions [Source 2]\n\n### Advantages\n\n**SVM Pros** [Source 3]:\n- Effective in high dimensional spaces\n- Versatile with different kernel functions\n- Memory efficient\n- Can handle outliers and overlapping classes\n- Works well when number of dimensions > number of samples\n\n**Logistic Regression Pros** [Source 3]:\n- Easy to explain\n- Easy to implement\n- Easy to interpret\n- Outputs probability of class membership\n\n### Disadvantages\n\n**SVM Cons** [Source 3]:\n- Slow to train on large datasets\n- Requires careful kernel and parameter selection\n- Does not directly provide probability estimates\n\n**Logistic Regression Cons** [Source 3, Source 5]:\n- Prone to outliers\n- Prone to overfitting in high-dimensional data\n- Makes linear assumptions between features and target\n\nThis comparison can help in choosing between the two models based on specific needs like interpretability, dataset size, dimensionality, and whether probability outputs are required.",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 14.284796953201294,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2004,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents provide highly relevant information about the comparison between SVMs and Logistic Regression, covering key aspects such as their similarities, differences in prediction approaches, and loss functions. However, some details in the model answer (e.g., advantages/disadvantages) are not explicitly present in the provided context, suggesting potential gaps in the retrieved documents.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information about similarities, prediction approaches, and loss functions. However, some claims about advantages/disadvantages and non-linear decision boundaries (e.g., kernel tricks) are not fully supported by the provided context, leading to minor unsupported additions.",
          "answer_quality": "The answer is excellent, well-structured, and comprehensive. It clearly addresses the query by comparing SVMs and Logistic Regression in a logical manner, covering similarities, differences, and practical considerations. The inclusion of pros and cons, even if not fully supported by the context, enhances the overall quality and usefulness of the response."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 46,
      "question": "How does one compare SVMs and Logistic Regression for a given problem?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:06:47",
      "answer": "**Comparing SVMs and Logistic Regression:**\n\nBased on the information from the provided PDF documents, we can compare **Support Vector Machines (SVMs)** and **Logistic Regression** in the context of machine learning models:\n\n1. **Prediction**:\n   - **SVM**:\n     - Uses a linear function for predictions and can only predict class labels.\n   - **Logistic Regression**:\n     - Uses a logistic function that gives a value between 0 and 1, interpretable as a probability.\n\n2. **Type of Prediction**:\n   - **SVM**:\n     - Focuses on maximizing the distance of the closest points to the margin.\n   - **Logistic Regression**:\n     - Maximizes the likelihood of the data.\n\n3. **Loss Function**:\n   - **SVM**:\n     - Uses hinge loss to measure prediction correctness.\n   - **Logistic Regression**:\n     - Utilizes cross-entropy loss to compare predicted probability distribution with observed distribution.\n\n4. **Sensitivity to Outliers**:\n   - **SVM**:\n     - More sensitive to outliers.\n   - **Logistic Regression**:\n     - Less sensitive to outliers.\n\n5. **Assumptions**:\n   - **SVM**:\n     - Assumes a linear relationship between variables and the target.\n   - **Logistic Regression**:\n     - Does not assume any specific distribution of input variables.\n\n6. **Interpretability**:\n   - **SVM**:\n     - Not as interpretable due to the focus on maximizing margins.\n   - **Logistic Regression**:\n     - More interpretable based on feature importance.\n\nOverall, SVMs and Logistic Regression differ in terms of their prediction functions, loss functions, handling of outliers, assumptions, and interpretability [Source 1] [Source 2].\n\nI cannot answer this question based on the provided PDF documents. The documents do not contain information about comparing SVMs and Logistic Regression for a given problem.",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        }
      ],
      "response_time": 4.234904766082764,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1799,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 3,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved document contains relevant information about the differences between SVMs and Logistic Regression, particularly focusing on prediction methods and loss functions. However, it misses some key aspects like sensitivity to outliers, assumptions, and interpretability, which the model answer partially infers or generalizes.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes some unsupported details (e.g., sensitivity to outliers, assumptions, and interpretability). These points are reasonable generalizations but are not explicitly mentioned in the provided context. The contradictory statement at the end ('I cannot answer this question...') also reduces faithfulness.",
          "answer_quality": "The answer is adequate and covers the main differences between SVMs and Logistic Regression. However, it could be improved in clarity and completeness. The inclusion of unsupported details and the contradictory closing statement detract from the overall quality. The structure is clear but not fully comprehensive."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 46,
      "question": "How does one compare SVMs and Logistic Regression for a given problem?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:06:54",
      "answer": "When comparing **Support Vector Machines (SVMs)** and **Logistic Regression**, several key differences can be considered:\n\n### Prediction\n*   **SVMs** use a linear function to make predictions and can only predict class labels, not probabilities [Source 1].\n*   **Logistic Regression** uses a logistic function that gives a value between 0 and 1, which can be interpreted as a probability [Source 1].\n\n### Loss Function\n*   **SVMs** use **hinge loss**, which measures the amount by which the model's prediction is incorrect [Source 2, Source 10].\n*   **Logistic Regression** uses **cross-entropy loss** as its cost function, measuring the difference between the predicted probability distribution and the observed distribution [Source 1, Source 2, Source 10].\n\n### Decision Boundary\n*   **SVMs** find the separating hyperplane that maximizes the distance of the closest points to the margin and can learn non-linear decision boundaries using kernel tricks [Source 2].\n*   **Logistic Regression** finds a linear decision boundary that focuses on maximizing the likelihood of the data, specifically the distance from the data to the decision boundary [Source 2].\n\n### Interpretability\n*   **Logistic Regression** is easy to explain, implement, and interpret, and its model can be interpreted by the value and sign of the estimated coefficients [Source 2, Source 3, Source 5].\n\n### Sensitivity to Outliers\n*   **SVMs** can handle outliers [Source 3].\n*   **Logistic Regression** is less sensitive to outliers compared to linear regression [Source 1]. However, it can be prone to outliers because it maximizes the conditional likelihoods of the training data [Source 3].\n\n### Overfitting\n*   **SVMs** can be effective in high-dimensional spaces, but care must be taken with kernel functions and regularization terms to avoid overfitting when the number of features is much greater than the number of samples [Source 3].\n*   **Logistic Regression** is prone to overfitting in high-dimensional data [Source 3, Source 5]. It is also less prone to overfitting than random forests because it is a simple, linear model with few parameters [Source 2].\n\n### Training\n*   **SVMs** can be slow to train on large datasets due to the need to choose the appropriate kernel and its parameters [Source 3].\n*   **Logistic Regression** involves estimating the parameters of the model by maximizing the likelihood of the observed data [Source 2].\n\n### Probability Estimates\n*   **SVMs** do not directly provide probability estimates [Source 3].\n*   **Logistic Regression** outputs the probability of class membership [Source 3].",
      "retrieved_docs": [
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "not make any assumptionsabout the distribution of the inputvariables.Sensitivity to outliersMore sensitive to outliers.Less sensitive to outliers.SVMs vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "It’s used in SVM.where  is the predicted output.Mean Squared Error (MSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Root Mean Square Error (RMSE)where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Mean Absolute Error (MAE)L=ylog(p)+(1−y)log(1−p)ypL=max(0,1−y)yMSE=(y−n1i=1∑ni)yi^2nyiyi^MSE=(y−)n1i=1∑niyi^2nyiyi^ .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Precision-Recall Curve • x-axis: Recall • y-axis: Precision • Puts emphasis on predictions the model got right out of the total number it predicted to be positive (i.e. precision). • Precision-Recall curves give more importance to the positive class which is useful when the classes are imbalanced.F=1=+precision1recall122×precision+recallprecision×recallimbalanceddata Auc4betterAulfbetter .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a line..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        }
      ],
      "response_time": 2.8486132621765137,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2606,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects such as prediction methods, loss functions, decision boundaries, interpretability, sensitivity to outliers, overfitting, training, and probability estimates for both SVMs and Logistic Regression.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information provided. However, there are minor unsupported additions (e.g., references to 'Source 10' which are not present in the provided context, and slight elaborations on kernel tricks and regularization that go beyond the given context).",
          "answer_quality": "The answer is excellent, clear, comprehensive, and well-structured. It systematically addresses the query by comparing SVMs and Logistic Regression across multiple dimensions, making it easy to understand and highly informative."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 47,
      "question": "What factors should be considered in model comparison?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:07:16",
      "answer": "Based on the provided context, here are the key factors to consider when comparing models:\n\n## Model-Specific Comparisons\n\n### Random Forest vs. Gradient Boosting\n- **Training Process**: \n  - Random Forest: Can be trained in parallel, has fewer hyperparameters\n  - Gradient Boosting: Cannot be trained in parallel, has more hyperparameters\n- **Overfitting**: Random Forest is less prone to overfitting due to averaging predictions, while Gradient Boosting is more prone to overfitting due to sequential training\n[Source 2]\n\n### Decision Tree vs. Random Forest\n- **Training Approach**: \n  - Decision Tree: Single tree trained on dataset\n  - Random Forest: Ensemble of independently trained trees\n- **Overfitting**: Decision Trees are more prone to overfitting, while Random Forests are less prone\n- **Interpretability**: Decision Trees are more interpretable through simple rules, Random Forests rely on feature importance\n[Source 6, Source 10]\n\n### Random Forest vs. Logistic Regression\n- **Structure**: Random Forest is an ensemble of decision trees, while Logistic Regression is a simple linear model\n- **Training Process**: Random Forest trains multiple trees independently, Logistic Regression estimates parameters through likelihood maximization\n- **Interpretability**: Both can be interpreted but in different ways (feature importance vs. parameter values)\n[Source 3]\n\n## Evaluation Metrics to Consider\n\n### Classification Metrics\n- **Precision-Recall Curve**: More suitable for imbalanced data\n- **F1 Score**\n- **ROC Curve**: Area under curve provides model comparison independent of threshold setting\n[Source 2, Source 11]\n\n### Regression Metrics\n- **Mean Absolute Error (MAE)**:\n  - Pros: Simple to interpret, less sensitive to outliers\n  - Cons: Not differentiable at 0, harder to optimize\n- **R-squared**:\n  - Pros: Easy to interpret and compare different models\n  - Cons: Always increases with new variables\n[Source 3, Source 8]\n\n### Important Considerations\n- For imbalanced datasets, avoid using accuracy as it can be misleading\n- ROC curve may be less sensitive to improvements on minority class\n[Source 1]",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 14.110262632369995,
      "num_citations": 3,
      "num_retrieved_docs": 25,
      "answer_length": 2121,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 2,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain some relevant information about model comparison (e.g., overfitting in Decision Trees vs. Random Forests), but they are fragmented and incomplete. Key aspects of model comparison (e.g., evaluation metrics, training processes, interpretability) are only partially covered, and some comparisons (e.g., Random Forest vs. Gradient Boosting) are not present in the retrieved context.",
          "faithfulness": "The answer includes several unsupported claims or comparisons not present in the retrieved context. For example, the comparison between Random Forest and Gradient Boosting (training process, hyperparameters) and the discussion of regression metrics (MAE, R-squared) are not grounded in the provided documents. Some minor contradictions or additions exist, such as the inclusion of Logistic Regression, which is not mentioned in the context.",
          "answer_quality": "The answer is well-structured and addresses the query, but it includes information beyond the retrieved context, which reduces its reliability. While the content is clear and organized, the lack of grounding in the provided documents limits its completeness and trustworthiness. Some key factors for model comparison (e.g., computational efficiency, scalability) are also missing."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 47,
      "question": "What factors should be considered in model comparison?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:07:32",
      "answer": "To consider factors in **model comparison**, the following aspects should be taken into account based on the provided context:\n\n- **Model-Level Methods** [Source 4 - Page 5]:\n  - Make the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g., tree-based models).\n  - **Pros**: Does not change the distribution of the training data.\n  - **Cons**: Methods are model-specific.\n\n- **Evaluation Metrics** [Source 4 - Page 5]:\n  - Choose appropriate evaluation metrics for the task that are easy to explain, implement, and interpret.\n  - **Cons**: Makes linear assumptions between the features and the target, prone to overfitting in high-dimensional data, and sensitive to outliers.\n\n- **Random Forest vs. Logistic Regression** [Source 10 - Page 8]:\n  - **Random Forest**:\n    - **Pros**: Less prone to overfitting as it averages the predictions of many different trees.\n    - **Cons**: More prone to overfitting as it's trained sequentially and can continue to fit the data until it is perfectly fit.\n  - **Logistic Regression**:\n    - **Pros**: Easy to explain, implement, and interpret.\n    - **Cons**: Prone to overfitting in high-dimensional data.\n\n- **Decision Tree vs. Random Forest** [Source 10 - Page 8]:\n  - **Decision Tree**:\n    - Trains a single tree on the dataset and makes predictions based on a series of rules.\n  - **Random Forest**:\n    - An ensemble of decision trees where each tree is trained independently, and the final prediction is an average of predictions from all trees.\n\nConsidering these factors in **model comparison** involves evaluating the trade-offs between various algorithms, their suitability for the dataset characteristics, interpretability, complexity, and potential pitfalls such as overfitting and underfitting. Each algorithm has its strengths and weaknesses, and selecting the most appropriate one depends on the specific requirements of the problem at hand.",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where ..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 7,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 8,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12,
          "text_snippet": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212."
        },
        {
          "source": 9,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 10,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        },
        {
          "source": 11,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive ..."
        },
        {
          "source": 12,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 4.299576044082642,
      "num_citations": 12,
      "num_retrieved_docs": 25,
      "answer_length": 1990,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain some relevant information about model comparison (e.g., differences between Decision Tree and Random Forest, pros and cons of algorithms). However, they miss key aspects of a comprehensive model comparison framework, such as evaluation metrics beyond overfitting, computational efficiency, scalability, interpretability, and handling of specific data types (e.g., imbalanced data). The context is fragmented and does not fully address the query.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly citing differences between Decision Tree and Random Forest, as well as pros and cons of specific models. However, there are minor unsupported additions, such as the mention of 'Model-Level Methods' and 'Evaluation Metrics' from a source not clearly present in the provided context (e.g., 'Source 4 - Page 5'). The answer also misattributes some cons (e.g., logistic regression's linear assumptions are correctly stated, but the context does not explicitly link them to evaluation metrics).",
          "answer_quality": "The answer addresses the query and provides useful comparisons between models, but it lacks clarity and structure. The inclusion of unrelated or misattributed details (e.g., 'Model-Level Methods') detracts from its coherence. While it covers some trade-offs, it does not provide a comprehensive or well-organized list of factors to consider in model comparison. The answer could be improved with better organization and focus."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 47,
      "question": "What factors should be considered in model comparison?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:07:41",
      "answer": "When comparing models, several factors should be considered, including the type of problem (classification or regression), the characteristics of the data, and specific evaluation metrics.\n\nKey factors for model comparison include:\n*   **Evaluation Metrics**:\n    *   For **regression tasks**, metrics like **Mean Squared Error (MSE)**, **Root Mean Square Error (RMSE)**, and **Mean Absolute Error (MAE)** are used [Source 1]. **R-squared (R²)** is also important, as it describes the percentage of target variable variation explained by the model, and its adjusted version incorporates the model's degree of freedom [Source 3, Source 8].\n    *   For **classification tasks**, particularly with **imbalanced data**, **Precision-Recall curves** are more informative as they give more importance to the positive class [Source 3]. Metrics such as **Accuracy**, **Precision**, **Recall**, **Specificity**, and **F1 Score** are derived from the **Confusion Matrix** [Source 2]. It's crucial to avoid accuracy for imbalanced classes, and the **AUC of the ROC curve** treats both classes equally, making it less sensitive to improvements in the minority class [Source 4, Source 11].\n    *   **Loss functions** like Cross-Entropy and Hinge Loss are also relevant [Source 1].\n*   **Model Characteristics and Performance**:\n    *   **Overfitting**: Some models like **Decision Trees** are more prone to overfitting as their depth grows, while **Random Forests** are less prone due to averaging predictions from multiple trees [Source 6, Source 10].\n    *   **Hyperparameters**: Models like **Random Forests** tend to have more hyperparameters than **Decision Trees** [Source 6, Source 10].\n    *   **Interpretability**: **Decision Trees** are generally more interpretable due to their rule-based nature, whereas **Random Forests** rely on feature importance for interpretability [Source 6, Source 9, Source 12].\n    *   **Computational Cost**: **Random Forests** can be computationally expensive [Source 5]. **Gaussian Mixture Models** are less efficient to train due to their flexibility [Source 6].\n    *   **Parallelization**: Some models, like **Random Forests**, can be trained in parallel, unlike others such as Gradient Boosting [Source 6].\n*   **Data Characteristics**:\n    *   The presence of **imbalanced data** requires careful selection of evaluation metrics and potentially model-level methods to make models more robust to class imbalance [Source 4, Source 5].\n    *   **Outliers** can significantly affect models like **Linear Regression** and **Logistic Regression**, while **Random Forests** are less sensitive [Source 4, Source 7, Source 9].\n    *   The **number of features** versus **number of samples** is also important, especially for models like **Support Vector Machines (SVMs)** to avoid overfitting [Source 4].\n\nSpecific model comparisons mentioned include:\n*   **Random Forest vs. Gradient Boosting** [Source 1]\n*   **Decision Tree vs. Random Forest** [Source 1, Source 10]\n*   **Linear Regression vs. Logistic Regression** [Source 1, Source 6]\n*   **SVMs vs. Logistic Regression** [Source 1, Source 6]\n*   **Logistic Regression vs. Random Forest** [Source 1, Source 12]\n*   **K-means vs. Gaussian Mixture Model** [Source 1]",
      "retrieved_docs": [
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": ": degrees of freedom of the estimate of the variance around the model.: total number of observations: total number of variables (i.e.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": ": The model fails to accurately model the data at all.: The model is a perfect fit for the data.MAE=∣y−n1i=1∑ni∣yi^nyiyi^ MSE=(y−n1i=1∑ni)y^i2 MAE=∣y−n1i=1∑ni∣yi^ R2R2R=20R=212.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "• Hard to interpret because the unit is squared.Mean Absolute Error (MAE)Measures the average absolute difference between predictions and true values.Pros • MAE is relatively simple to interpret. • MAE is less sensitive to outliers compared to MSE.Cons • MAE is not differentiable at 0, which makes it more difficult to optimize.  describes the percentage of the target variable variation that is explained by the model.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Rando..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model alg..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with ..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because ..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 6,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cl..."
        }
      ],
      "response_time": 4.70672082901001,
      "num_citations": 6,
      "num_retrieved_docs": 25,
      "answer_length": 3242,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 5,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of model comparison such as evaluation metrics, model characteristics (e.g., overfitting, hyperparameters, interpretability), and data-specific considerations. The context directly addresses the factors mentioned in the answer.",
          "faithfulness": "The answer is completely faithful to the retrieved context. All claims, examples, and comparisons (e.g., Decision Tree vs. Random Forest, evaluation metrics for classification/regression) are directly supported by the provided documents. No unsupported information or contradictions are present.",
          "answer_quality": "The answer is excellent: clear, comprehensive, and well-structured. It systematically covers evaluation metrics, model characteristics, and data considerations, providing specific examples and comparisons. The response is actionable and addresses the query thoroughly."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 48,
      "question": "What are the main differences between Decision Tree and Random Forest algorithms?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:08:02",
      "answer": "Based on the provided documents, here are the key differences between Decision Trees and Random Forest algorithms:\n\n## Training Process\n- **Decision Tree**: A single tree is trained on the dataset [Source 1]\n- **Random Forest**: An ensemble of decision trees, where each tree is trained independently [Source 1]\n\n## Overfitting\n- **Decision Tree**: More prone to overfitting as the depth of the tree grows [Source 2]\n- **Random Forest**: Less prone to overfitting as it averages the predictions of many different trees [Source 2]\n\n## Hyperparameters\n- **Decision Tree**: Fewer hyperparameters [Source 2]\n- **Random Forest**: More hyperparameters [Source 2]\n\n## Interpretability\n- **Decision Tree**: More interpretable as it's based on a series of simple rules that can be followed to make a prediction [Source 2]\n- **Random Forest**: Models can be interpreted based on feature importance, which is less straightforward compared to decision trees [Source 2]\n\n## Additional Random Forest Advantages\nRandom Forest offers several benefits over single Decision Trees:\n- Better generalization performance due to randomness\n- Doesn't require much parameter tuning\n- Doesn't require feature selection\n- Less sensitive to outliers\n- Generates feature importance for result interpretation\n- However, it is computationally expensive [Source 3]\n\n## Decision Tree Characteristics\nIndividual Decision Trees have these specific traits:\n- Easy to understand and interpret\n- Requires little data pre-processing\n- Doesn't require feature selection\n- Efficient in prediction\n- However, they are prone to overfitting, sensitive to noise, and not good at extrapolation [Source 4]",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 11.759375095367432,
      "num_citations": 4,
      "num_retrieved_docs": 30,
      "answer_length": 1658,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents cover most key aspects of the query, including training processes, overfitting, hyperparameters, and interpretability. However, the context is fragmented and incomplete (e.g., missing clear sources for some points like 'feature importance' and 'computationally expensive'), which slightly limits the relevance.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the differences mentioned (e.g., training, overfitting, hyperparameters). However, some details (e.g., 'feature importance,' 'computationally expensive') are not explicitly supported by the provided fragments, indicating minor unsupported additions.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive, addressing the query effectively. It could be improved by explicitly noting which points are inferred (e.g., 'feature importance') and which are directly from the context, but overall, it is high-quality."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 48,
      "question": "What are the main differences between Decision Tree and Random Forest algorithms?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:08:12",
      "answer": "Based on the information provided in the context from **Source 1** and **Source 2**, here are the main differences between the **Decision Tree** and **Random Forest** algorithms:\n\n### Decision Tree:\n- **Overfitting**:\n  - More prone to overfitting as the depth of the tree grows [Source 1].\n- **Hyperparameters**:\n  - Fewer hyperparameters compared to Random Forest [Source 1, Source 2].\n- **Interpretability**:\n  - More interpretable as it's based on a series of simple rules that can be followed to make a prediction [Source 1].\n\n### Random Forest:\n- **Overfitting**:\n  - Less prone to overfitting as it averages the predictions of many different trees [Source 1, Source 2].\n- **Hyperparameters**:\n  - More hyperparameters compared to Decision Tree [Source 1, Source 2].\n- **Interpretability**:\n  - Models can be interpreted based on feature importance, which is less straightforward compared to Decision Trees [Source 1].\n\n**In summary:**\n- Random Forest is less prone to overfitting compared to Decision Trees due to the ensemble nature of the algorithm, which averages predictions from multiple trees.\n- Decision Trees are simpler and more interpretable compared to Random Forest, as they are based on a series of straightforward rules.\n\nPlease note that these differences are specific to the overfitting tendency, hyperparameters, and interpretability aspects of Decision Trees and Random Forests as described in the provided context.",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        }
      ],
      "response_time": 4.027088165283203,
      "num_citations": 1,
      "num_retrieved_docs": 30,
      "answer_length": 1440,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key differences such as overfitting, hyperparameters, and interpretability. However, the context is incomplete (e.g., truncated text) and lacks some details like training methodology or performance metrics, which could have made it even more comprehensive.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided, such as overfitting tendencies, hyperparameter differences, and interpretability, without adding unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively addresses the query by highlighting the main differences between Decision Tree and Random Forest. However, it could be slightly improved by explicitly noting the limitations of the provided context (e.g., missing details due to truncation)."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 48,
      "question": "What are the main differences between Decision Tree and Random Forest algorithms?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:08:21",
      "answer": "The main differences between Decision Tree and Random Forest algorithms lie in their training, prediction, and propensity for overfitting, as well as their interpretability and hyperparameter complexity [Source 1], [Source 2].\n\nHere's a breakdown of their differences:\n\n*   **Training Process**\n    *   **Decision Tree:** A single tree is trained on the dataset [Source 1].\n    *   **Random Forest:** An ensemble of decision trees is used, with each tree trained independently [Source 1]. This method is an ensemble learning technique, combining predictions from multiple models [Source 1].\n*   **Prediction Mechanism**\n    *   **Decision Tree:** Makes predictions based on a series of rules learned from the dataset [Source 1].\n    *   **Random Forest:** The final prediction is an average of the predictions made by all the individual trees in the forest [Source 1].\n*   **Overfitting**\n    *   **Decision Tree:** More prone to overfitting, especially as the depth of the tree grows [Source 2], [Source 5].\n    *   **Random Forest:** Less prone to overfitting because it averages the predictions of many different trees [Source 1], [Source 2]. However, if individual trees in a Random Forest are allowed to grow infinitely, it can still be prone to overfitting [Source 3].\n*   **Hyperparameters**\n    *   **Decision Tree:** Has fewer hyperparameters to tune [Source 2].\n    *   **Random Forest:** Has more hyperparameters compared to a single decision tree [Source 2].\n*   **Interpretability**\n    *   **Decision Tree:** More interpretable as it's based on a series of simple rules that can be followed to make a prediction [Source 2], [Source 5].\n    *   **Random Forest:** Models can be interpreted based on feature importance, which is less straightforward compared to decision trees [Source 2], [Source 3], [Source 4].\n*   **Generalization Performance**\n    *   **Random Forest:** Generally has better generalization performance than an individual decision tree due to its randomness [Source 4].\n\nBoth algorithms can be used for classification and regression tasks [Source 1].",
      "retrieved_docs": [
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "• Sensitive to noise. • Not good at extrapolation.Random Forest⾮参数模型don'thaveparametersflexibleenteringbaggingbootstrap.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Gaussian Mixture ModelBoth are unsupervised models that are used to group data into a predetermined number of clusters. Both require users to specify the number of clusters to be used.DifferencesK-meansGaussian Mixture Model (GMM)Hard vs. soft clusteringA hard clustering algorithm. Eachdata point is assigned to a singlecluster.A soft clustering algorithm.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8,
          "text_snippet": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is tra..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFe..."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much p..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6,
          "text_snippet": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-b..."
        }
      ],
      "response_time": 2.9845480918884277,
      "num_citations": 5,
      "num_retrieved_docs": 30,
      "answer_length": 2082,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 5,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved documents are perfectly relevant to the query, covering all key aspects of the differences between Decision Tree and Random Forest algorithms, including training process, overfitting, hyperparameters, interpretability, and generalization performance.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately reflecting the information provided. However, it includes minor unsupported additions, such as the mention of 'Source 3', 'Source 4', and 'Source 5', which are not present in the provided retrieved context.",
          "answer_quality": "The answer is excellent, clear, comprehensive, and well-structured. It provides a detailed breakdown of the differences between the two algorithms, addressing all relevant aspects of the query in an organized manner."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 49,
      "question": "What is the role of loss functions in machine learning?",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:08:42",
      "answer": "Based on the provided context, I can only provide limited information about loss functions as they relate to specific models. Here's what the documents tell us about loss functions:\n\nLoss functions are metrics used to measure prediction errors in different types of models:\n\n1. **Common Types of Loss Functions** mentioned in the documents include:\n- **Cross-Entropy Loss**: Measures the difference between predicted probability distribution and observed distribution [Source 1, Source 3]\n- **Mean Squared Error (MSE)** [Source 1, Source 2]\n- **Hinge Loss**: Measures the amount by which a model's prediction is incorrect [Source 3, Source 4]\n- **Root Mean Square Error (RMSE)** [Source 2]\n- **Mean Absolute Error (MAE)** [Source 2]\n\n2. **Model-Specific Applications**:\n- **Linear Regression** typically uses **Mean Squared Error (MSE)** [Source 1]\n- **Logistic Regression** uses **Cross-Entropy Loss** [Source 1, Source 3]\n- **SVM** uses **Hinge Loss** [Source 3]\n\nWhile the documents mention various loss functions, they don't provide comprehensive information about the broader role of loss functions in machine learning. To fully answer this question about their general role in machine learning, we would need additional context beyond what's provided in these documents.",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        }
      ],
      "response_time": 8.81598424911499,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1276,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 3,
          "faithfulness": 4,
          "answer_quality": 3
        },
        "explanations": {
          "relevance": "The retrieved documents contain relevant information about specific types of loss functions (e.g., cross-entropy, hinge loss, MSE) and their applications in certain models (e.g., logistic regression, SVM). However, they miss key aspects of the query, such as the broader role of loss functions in machine learning (e.g., optimization, model training, gradient descent). The context is somewhat relevant but incomplete.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately describing the loss functions mentioned (cross-entropy, hinge loss, MSE, etc.) and their model-specific applications. However, it includes minor unsupported additions like 'Root Mean Square Error (RMSE)' and 'Mean Absolute Error (MAE)', which are not explicitly stated in the provided context but are logically consistent with the topic.",
          "answer_quality": "The answer is adequate and addresses part of the query by listing specific loss functions and their applications. However, it lacks clarity on the broader role of loss functions in machine learning and is incomplete. The structure is clear, but the answer could be improved in comprehensiveness and depth."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 49,
      "question": "What is the role of loss functions in machine learning?",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:08:53",
      "answer": "**The role of loss functions in machine learning** includes:\n\n- **Measuring prediction accuracy**: Loss functions like **Mean Squared Error (MSE)**, **Root Mean Square Error (RMSE)**, **Mean Absolute Error (MAE)**, **Hinge Loss**, and **Cross-Entropy** are used to measure the **difference between predicted values and actual values** [Source 1], [Source 2], [Source 3], [Source 4].\n  \n- **Optimizing model parameters**: Loss functions help in **optimizing model parameters** by quantifying the **discrepancy between predicted outcomes and actual outcomes** [Source 3].\n  \n- **Differentiating models**: Loss functions like **Hinge Loss** and **Cross-Entropy** help differentiate between **SVM** and **Logistic Regression** models based on how they measure prediction correctness [Source 3], [Source 4].\n  \n- **Improving model robustness**: Loss functions play a role in making models **more robust to class imbalance** by penalizing misclassifications of minority classes and selecting appropriate algorithms [Source 5].\n\nLoss functions are crucial in guiding the training process of machine learning models by providing a metric to minimize the error between predictions and true values, thereby enhancing the model's accuracy and performance [Source 2].",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost fu..."
        },
        {
          "source": 4,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        },
        {
          "source": 5,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5,
          "text_snippet": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class ..."
        }
      ],
      "response_time": 3.3923208713531494,
      "num_citations": 5,
      "num_retrieved_docs": 8,
      "answer_length": 1255,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved document snippets are highly relevant to the query, as they discuss specific loss functions (e.g., cross-entropy, hinge loss) and their roles in machine learning, such as measuring performance and addressing class imbalance. However, the context is somewhat fragmented and lacks a comprehensive overview of loss functions, missing broader explanations of their role in optimization or model training.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, correctly referencing cross-entropy and hinge loss and their roles. However, it introduces general concepts like MSE, RMSE, and MAE, which are not explicitly mentioned in the provided context, though they are standard loss functions. The explanation of optimization and model differentiation aligns well with the context.",
          "answer_quality": "The answer is clear, well-structured, and addresses the query effectively by outlining key roles of loss functions. It could be slightly improved by providing more depth on how loss functions guide optimization (e.g., gradient descent) or by clarifying the connection between specific loss functions and their use cases. The inclusion of sources adds credibility."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 49,
      "question": "What is the role of loss functions in machine learning?",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:09:01",
      "answer": "Loss functions in machine learning measure the **difference between predicted and observed distributions** or the **amount by which a model's prediction is incorrect** [Source 1, Source 3, Source 4]. They are a critical component in various machine learning models.\n\nCommon loss functions mentioned include:\n*   **Mean Squared Error (MSE)**: Used in most applications for real value prediction [Source 1].\n*   **Cross-entropy loss**: Used in logistic regression to measure the difference between the predicted probability distribution and the observed distribution [Source 1, Source 3]. It also measures the performance of a binary classification model by comparing output distribution to observations [Source 3].\n*   **Hinge loss**: Used in Support Vector Machines (SVM) to measure the amount by which the model's prediction is incorrect [Source 3, Source 4].\n*   **Root Mean Square Error (RMSE)** [Source 2].\n*   **Mean Absolute Error (MAE)** [Source 2].\n\nLoss functions, along with evaluation metrics, are important for machine learning tasks [Source 2, Source 5].",
      "retrieved_docs": [
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge LossHinge loss measure of the amount by which the model's prediction is incorrect.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9,
          "text_snippet": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss func..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2,
          "text_snippet": "Gaussian Mixture Model Loss functions Cross-Entropy Hinge Loss Mean Squared Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Evaluation metrics Regression."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11,
          "text_snippet": "ntropy loss measures the performance of a binary classification model by comparing the output distribution to observations.where  is the target (data label), and  is the predicted probability.Hinge Lo..."
        }
      ],
      "response_time": 1.6844279766082764,
      "num_citations": 3,
      "num_retrieved_docs": 8,
      "answer_length": 1067,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, as they discuss specific types of loss functions (e.g., cross-entropy, hinge loss) and their roles in machine learning. However, the context is somewhat fragmented and lacks a comprehensive overview of the general role of loss functions, which slightly limits the score.",
          "faithfulness": "The answer is highly faithful to the retrieved context, accurately describing the loss functions mentioned (e.g., cross-entropy, hinge loss). However, it includes minor unsupported additions like 'Root Mean Square Error (RMSE)' and 'Mean Absolute Error (MAE)', which are not explicitly referenced in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete, providing a good overview of loss functions and their roles. However, it could be improved by explicitly stating the broader role of loss functions (e.g., guiding model training via optimization) and avoiding minor unsupported details."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 50,
      "question": "Summarize the process and benefits of feature selection in machine learning.",
      "model": "claude-3.5-sonnet",
      "model_used": "anthropic/claude-3-5-sonnet",
      "timestamp": "2025-12-05 15:09:21",
      "answer": "Based on the provided documents, here is a summary of feature selection in machine learning:\n\n## Definition and Purpose\n**Feature selection** is the process of selecting a subset of the original features for model training [Source 1].\n\n## Types of Feature Selection Methods\n\n1. **Intrinsic Feature Selection Methods**\n- Naturally embedded within training process of models (e.g., tree-based models and regularization models)\n- **Pros:**\n   - Fast and no external feature tool needed\n   - Direct connection between feature selection and object function\n- **Cons:**\n   - Model-dependent\n   - Limited choice of models\n[Source 1]\n\n2. **Filter Feature Selection Methods**\n- Selects features that correlate well with target variables\n- Uses univariate statistical analysis and feature importance scores\n- Process is performed once and is independent of model algorithm\n- **Cons:**\n   - Tends to select redundant features\n   - Ignores relationships among features\n[Source 1, Source 2]\n\n3. **Wrapper Feature Selection Methods**\n- Iterative process that repeatedly adds subset features to the model\n- Uses resulting model performance to guide selection\n- **Pros:**\n   - Searches for wider variety of feature subsets\n- **Cons:**\n   - Significant computation time with large feature sets\n   - Higher potential for overfitting\n[Source 2]\n\n## Notable Benefits\nSome machine learning algorithms, like **Random Forests** and **Decision Trees**, have built-in feature selection capabilities and don't require separate feature selection processes [Source 3, Source 5]. This can be particularly advantageous as it:\n- Reduces preprocessing requirements\n- Provides feature importance rankings\n- Helps with result interpretation\n- Makes the models less sensitive to outliers\n\nThe choice of feature selection method should be based on the specific requirements of your project, considering factors like computational resources, dataset size, and the desired balance between model performance and interpretability.",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 13.049125909805298,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1990,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents provide relevant information about feature selection in machine learning, covering types of methods and some benefits. However, the context does not comprehensively address all aspects of the query, such as a detailed step-by-step process or an exhaustive list of benefits.",
          "faithfulness": "The answer is mostly faithful to the retrieved context, accurately summarizing the types of feature selection methods and their pros and cons. However, there are minor unsupported additions, such as the specific mention of 'Random Forests' and 'Decision Trees' having built-in feature selection capabilities, which may not be explicitly stated in the provided context.",
          "answer_quality": "The answer is clear, well-structured, and mostly complete. It effectively summarizes the process and benefits of feature selection. However, it could be slightly improved by providing a more detailed explanation of the step-by-step process and a more comprehensive list of benefits."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 50,
      "question": "Summarize the process and benefits of feature selection in machine learning.",
      "model": "gpt-3.5-turbo",
      "model_used": "openai/gpt-3.5-turbo",
      "timestamp": "2025-12-05 15:09:31",
      "answer": "**Feature Selection in Machine Learning:**\n\n- **Process:**\n    - **Intrinsic Feature Selection Methods:**\n        - Have feature selection embedded within the training process of models like tree-based and regularization models.\n        - **Pros:**\n            - Fast and doesn't require external feature tools.\n            - Directly connects feature selection with the objective function.\n        - **Cons:**\n            - Model-dependent with limited model choices.\n    - **Filter Feature Selection Methods:**\n        - Select features that correlate well with target variables using methods like univariate statistical analysis and feature importance scores.\n        - Process is model-independent and performed only once.\n    - **Wrapper Feature Selection Methods:**\n        - An iterative process like sequential feature selection that adds subset features based on model performance.\n        - **Pros:**\n            - Searches wider feature subsets compared to other methods.\n        - **Cons:**\n            - Computationally intensive with large feature sets.\n            - Potential to overfit features to training data.\n    - **Imbalanced Data Handling:**\n        - Involves resampling techniques like over-sampling, under-sampling, or generating synthetic examples (e.g., SMOTE).\n        - **Pros:**\n            - Simple and fast approaches.\n        - **Cons:**\n            - Risk of overfitting in some methods and potential data loss in others.\n\n- **Benefits of Feature Selection:**\n    - Helps **improve model performance** by focusing on relevant features.\n    - **Reduces overfitting** by selecting the most important features.\n    - **Enhances model interpretability** by simplifying the model complexity.\n    - **Improves computational efficiency** by working with a subset of features rather than the entire set.\n\nBy incorporating appropriate feature selection techniques, machine learning models can become more efficient, accurate, and interpretable. \n\nSources: [Source 1], [Source 2], [Source 3]",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        },
        {
          "source": 3,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10,
          "text_snippet": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa b..."
        }
      ],
      "response_time": 4.626798152923584,
      "num_citations": 3,
      "num_retrieved_docs": 40,
      "answer_length": 2017,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 4,
          "answer_quality": 5
        },
        "explanations": {
          "relevance": "The retrieved document is somewhat relevant but does not fully cover all aspects of feature selection (e.g., it only mentions intrinsic methods explicitly). However, the model answer expands on the topic comprehensively, covering intrinsic, filter, and wrapper methods, which aligns well with the query's intent.",
          "faithfulness": "The answer is mostly faithful to the retrieved context but includes additional details (e.g., filter and wrapper methods, pros/cons, and imbalanced data handling) that are not explicitly mentioned in the provided context. However, these additions are accurate and relevant to the topic, with no contradictions.",
          "answer_quality": "The answer is excellent—clear, well-structured, and comprehensive. It addresses the query thoroughly by explaining the process (with multiple methods) and benefits of feature selection, making it highly informative and easy to understand."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    },
    {
      "qa_id": 50,
      "question": "Summarize the process and benefits of feature selection in machine learning.",
      "model": "google/gemini-2.5-flash",
      "model_used": "google/gemini-2.5-flash",
      "timestamp": "2025-12-05 15:09:40",
      "answer": "Feature selection is the process of **selecting a subset of the original features for model training** [Source 1]. It is often used as a **pre-processing step before the actual learning** [Source 1].\n\nThere are several methods for feature selection:\n\n*   **Intrinsic Feature Selection Methods**:\n    *   **Process**: These methods have feature selection naturally embedded within the training process of models, such as tree-based models and regularization models [Source 1].\n    *   **Benefits**: They are fast, do not require an external feature tool, and provide a direct connection between feature selection and the objective function [Source 1].\n    *   **Drawbacks**: They are model-dependent, and the choice of models is limited [Source 1].\n\n*   **Filter Feature Selection Methods**:\n    *   **Process**: These methods select features that correlate well with target variables, using techniques like univariate statistical analysis and feature importance scores. This process is performed only once and is independent of the model algorithm [Source 1].\n    *   **Benefits**: They are fast and simple, and can be effective at capturing large trends in the dataset [Source 2].\n    *   **Drawbacks**: They tend to select redundant features and ignore relationships among features [Source 2].\n\n*   **Wrapper Feature Selection Methods**:\n    *   **Process**: This is an iterative process, such as sequential feature selection, where subset features are repeatedly added to the model. The resulting model's performance then guides the selection of the next subset [Source 2].\n    *   **Benefits**: They search for a wider variety of feature subsets compared to other methods [Source 2].\n    *   **Drawbacks**: They can require significant computation time when the number of features is large and have the most potential to overfit the features to the training data [Source 2].\n\nSome models, like **Decision Trees** and **Random Forests**, do not require feature selection [Source 3, Source 5].",
      "retrieved_docs": [
        {
          "text": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tree-based models and regularization models.Pros • Fast and no external feature tool is needed.• Provides a direct connection between feature selection and the object function.Filter Feature Selection MethodsCons • Model-dependent and the choice of models is limited.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Select features that correlate well with target variables, such as univariate statistical analysis and feature importance scores. The process is performed only once and is independent of the model algorithm.ProsCons Confusion Matrix Accuracy Precision Recall Specificity F1 Score ROC Curve and Precision-Recall Curve.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Sforward selection backward selection cross-validation based feature selection .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "• Tend to select redundant features.• Ignore relationships among features.Wrapper Feature Selection MethodsAn iterative process, such as sequential feature selection, repeatedly adds subset features to the model and then uses the resulting model performance to guide the selection of the next subset.Pros • Search for a wider variety of feature subsets than other methods.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 6Pros • Has a better generalization performance than an individual decision tree due to randomness. • Doesn’t require much parameter tuning. • Doesn't require feature selection. • Less sensitive to outliers in the dataset. • It generates feature importance which is helpful when interpreting the results.Cons • Computationally expensive.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Easy to explain, implement and interpret.Cons • Makes linear assumptions between the features and the target. • Prone to overfitting in high-dimensional data. • Sensitive to outliers.Support Vector MachineProsCons .",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• If the number of features is much greater than the number of samples, avoid overfitting in choosing Kernel functions and regularization terms are crucial.  • Slow to train on large datasets because of the need to choose the appropriate kernel and its parameters.  • Do not directly provide probability estimates.Logistic RegressionPros • Easy to explain, implement and interpret.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Evaluation metrics Data and feature engineering Missing Values Feature Selection Imbalanced Data Pros and Cons of ML Algorithms Linear Models Tree-based Models Clustering Models Model Comparison Random Forest vs. Gradient Boosting Decision Tree vs. Random Forest Linear Regression vs. Logistic Regression SVMs vs. Logistic Regression Logistic Regression vs. Random Forest K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Cons • Significant computation time when the number of features is large.• Have the most potential to overfit the features to the training data.Imbalanced DataAn imbalanced dataset is a dataset where one or more labels make up the majority of the dataset, leaving far fewer examples of other labels. This problem applies to both classification and regression tasks.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 4Model-level MethodsMake the model more robust to class imbalance by penalizing wrong classifications of the minority class more or by selecting appropriate algorithms (e.g. tree-based models).Pros • Does not change the distribution of the training data.Cons • Methods are model specific.Evaluation MetricsChoose appropriate evaluation metrics for the task.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "• Outputs the probability of class membership.Cons • Maximize the conditional likelihoods of the training data, which makes it prone to outliers. • Prone to overfitting in high-dimensional data.Tree-based ModelsDecision TreesPros • Easy to understand and interpret. • Requires little data pre-processing. • Doesn't require feature selection. • Efficient in prediction: the cost of one prediction is logarithmic in the number of examples used to train the tree.Cons • Prone to overfitting.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 3 • Fast and Simple.• can be effective at capturing large trends in the dataset.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 5 • Effective in high dimensional spaces and where the number of dimensions is greater than the number of samples.  • Versatile: different Kernel functions can be specified for the decision function. It is also possible to specify custom kernels.  • Memory efficient since the decision boundary depends on a few support vectors.  • Can handle outliers and overlapping classes.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 6
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 2Data and feature engineeringMissing ValuesGather more data: See if there is any way to backfill the data or join with external datasets. Imputation: Infer the missing values by leveraging our prior knowledge of the existing data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Machine Learning Interview Cheat Sheet 1 Machine Learning Interview Cheat Sheet This comprehensive guide serves as a quick reference for various concepts, algorithms, and techniques. It walks you through the following question types in Machine Learning interviews:1. Data and feature engineering2. Pros and cons of machine learning models3. Model comparison4. Loss functions5.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 2
        },
        {
          "text": "Gradient BoostingPros • It produces very accurate models, it outperforms random forest in accuracy. • No data pre-processing is required. • Handles missing data - imputation not required.Cons • Gradient boosting is a sequential process that can be slow to train. • Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive. • Sacrifices interpretability for accuracy - less interpretative in nature.Clustering ModelsK-meansPros • Easy to implement.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 7
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 7Gaussian Mixture ModelPros • A soft clustering algorithm. Each data point is assigned a probability of belonging to each cluster. • Clusters can have different shapes and sizes.Cons • Less efficient to train due to its flexibility.Model ComparisonRandom Forest vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 12Pros • R-squared is easy to interpret. • Easy to compare the performance of different models.Cons • Always increases upon adding a new variable. • Does not show the predicting power of the model.Adjusted  Adjusted  incorporates the model’s degree of freedom.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "MACHINELEARNINGINTERVIEWCHEAT SHEET",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 1
        },
        {
          "text": "Model-based imputation: use non-missing data to predict missing data with KNN/linear regression/trees.Omission: removing missing observations or features with lots of missing values in the dataset. Feature SelectionSelect a subset of the original features for model training.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3
        },
        {
          "text": "Random ForestDifferencesRandom forestLogistic regressionStructureAn ensemble of decision treesthat work together to make aprediction.A simple, linear model that isused to predict the probability ofa binary outcome.Training processInvolve training many decisiontrees independently.Involves estimating theparameters of the model bymaximizing the likelihood of theobserved data.InterpretabilityThe model can be interpretedbased on the feature importance.The model can be interpreted bythe value and sign",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "variance.Reduces both bias and variance.OverfittingLess prone to overfitting as itaverages the predictions of manydifferent trees.More prone to overfitting as it’strained sequentially and cancontinue to fit the data until it isperfectly fit.HyperparameterFewer hyperparameters.More hyperparameters.ParallelizationCan be trained in parallel.Cannot be trained in parallel.Decision Tree vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "features) in the modeladjusted Add a feature (column)IncreasesIncreases only if the featureimproves the accuracy of themodelAdd observation (row)may increase, decrease orstay the samemay increase, decrease orstay the sameClassificationConfusion MatrixR=2=TSSRSS1−=TSSESS1−(y−)∑i=1niyˉ2(y−)∑i=1niy^i2 R2R2=Rˉ21−TSS/dftRSS/dfrdfrn−pnpR2R2.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 13
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 11where  is the number of samples,  is the true value of the i-th sample, and  is the predicted value.Evaluation metricsRegressionMean Squared Error (MSE)Measures the average squared distance between predictions and true values.Pros • Penalizes large errors. • Mathematically convenient to obtain gradient.Cons • Sensitive to outliers since outliers.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 12
        },
        {
          "text": "8DifferencesDecision TreeRandom ForestOverfittingMore prone to overfitting as thedepth of the tree grows.Less prone to overfitting as itaverages the predictions of manydifferent trees.HyperparameterFewer hyperparametersMore hyperparametersInterpretabilityMore interpretable as it’s basedon a series of simple rules thatcan be followed to make aprediction.Models can be interpreted basedon the feature importance whichis less straightforward comparedwith decision trees.Linear Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Metrics to Consider • Precision-Recall curve gives more importance to the positive class and is helpful for dealing with imbalanced data.• Precision, recall, and F1 measure a model’s performance with respect to the positive class in a binary classification problem.Metrics to avoid • Accuracy is misleading when classes are imbalanced.• AUC of the ROC curve treats both classes equally and is less sensitive to model improvement on minority class.Pros and Cons of ML AlgorithmsLinear ModelsLinear Reg",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 5
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 140 → worst, 1 → best (perfect precision and recall).ROC Curve and Precision-Recall CurveROC Curve • x-axis: False Positive Rate • y-axis: Recall (TPR) • The area under a ROC curve is a more systematic metric comparing binary classification models because it’s independent of how we set the threshold.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 15
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved. 13 Accuracy% predicted labels that match true labels.Precision% of the number of accurate positives the model claims compared to the total number of positives it claims.Recall% of the number of positives the model claims compared to the actual number of positives in the data.Specificity% of the number of negatives the model claims compared to the actual number of negatives in the data.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 14
        },
        {
          "text": "Gradient BoostingBoth are ensemble learning methods — training a group of models and combining their predictions to make a more accurate final prediction.DifferencesRandom ForestGradient BoostingTrainingEach learner is trainedindependently.Each learner is trainedsequentially to correct the errorsmade by the previous tree.Making PredictionsThe final prediction is an averageof the predictions of all trees.The overall prediction is given bya weighted sum of the collection.OptimizationMainly reduces",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Random ForestBoth can be used for classification and regression tasks.DifferencesDecision TreeRandom ForestTrainingA single tree is trained on thedataset.An ensemble of decision trees.Each tree is trainedindependently.PredictionMakes predictions based on aseries of rules trained on thedataset.The final prediction is an averageof the predictions of all trees..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 8
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Machine Learning Interview Cheat Sheet - emmading.com. All rights reserved.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 11
        },
        {
          "text": "ision boundarythat focuses on maximizing thelikelihood of the data — thedistance from the data to thedecision boundary.Logistic Regression vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "A few ways to deal with imbalanced data:ResamplingChange the distribution of the training data to reduce the level of class imbalance by either upsampling the minority class or downsampling the majority class.Methods include:Over-samplingUnder-samplingGenerate synthetic examplesSMOTE (synthetic minority oversampling technique)Pros • Simple and fast.Cons • Risk of overfitting training data (over-sampling) and losing important information from removing data (under-sampling).I XnewxitxcxkixisZEco.I",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4
        },
        {
          "text": "9DifferencesSVMLogistic regressionLoss functionUses hinge loss as the lossfunction which is a measure ofthe amount by which the model'sprediction is incorrect.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.Decision boundaryFinds the separating hyperplanethat maximizes the distance ofthe closest points to the margin.Can learn non-linear decisionboundaries by using kernel tricks.Finds linear dec",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "of theestimated coefficients.OverfittingMore prone to overfitting ifindividual trees are allowed togrow infinitely.Less prone to overfitting thanrandom forests, as it is a simple,linear model with fewparameters.K-means vs.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 10
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions. Both are classification models.DifferencesSVMLogistic regressionPredictionUses a linear function to makepredictions. Can only predictclass labels instead ofprobabilities.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability..",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "Logistic RegressionBoth are parametric models that learn a set of parameters from the dataset to make predictions.DifferencesLinear regressionLogistic regressionType of predictionIs used when the target iscontinuous.Is used when the target is binary.E.g.",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        },
        {
          "text": "0 or 1, true or false, etc.PredictionGives real value prediction.Uses a logistic function as theprediction function that gives avalue between 0 and 1, whichcan be interpreted as aprobability.Loss functionUses mean squared error (MSE)in most applications.Uses the cross-entropy loss asthe cost function that measuresthe difference between thepredicted probability distributionand the observed distribution.AssumptionsAssumes there is a linearrelationship between each inputvariable and the target.Does",
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 9
        }
      ],
      "citations": [
        {
          "source": 1,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 3,
          "text_snippet": "It is usually used as a pre-processing step before doing the actual learning.Intrinsic Feature Selection MethodsHave feature selection naturally embedded with the training process of models such as tr..."
        },
        {
          "source": 2,
          "pdf_filename": "Machine_Learning_Interview_Cheat_Sheet.pdf",
          "page": 4,
          "text_snippet": "Sforward selection backward selection cross-validation based feature selection ."
        }
      ],
      "response_time": 2.7428650856018066,
      "num_citations": 2,
      "num_retrieved_docs": 40,
      "answer_length": 1995,
      "judge_evaluation": {
        "success": true,
        "scores": {
          "relevance": 4,
          "faithfulness": 5,
          "answer_quality": 4
        },
        "explanations": {
          "relevance": "The retrieved documents are highly relevant to the query, covering key aspects of feature selection in machine learning, including methods and some benefits. However, the context does not comprehensively address all potential benefits (e.g., reducing overfitting, improving model interpretability, or computational efficiency in detail), which slightly limits the score.",
          "faithfulness": "The answer is completely faithful to the retrieved context. It accurately reflects the information provided in the documents, including the processes, benefits, and drawbacks of different feature selection methods without introducing unsupported claims or contradictions.",
          "answer_quality": "The answer is clear, well-structured, and mostly comprehensive. It effectively summarizes the process and benefits of feature selection, categorizing methods and their trade-offs. However, it could be improved by explicitly mentioning additional benefits (e.g., reducing overfitting or improving interpretability) that are often associated with feature selection but not covered in the retrieved context."
        },
        "judge_model": "mistralai/mistral-large"
      },
      "judge_error": null
    }
  ],
  "summary": {
    "claude-3.5-sonnet": {
      "total_questions": 50,
      "successful_evaluations": 50,
      "avg_scores": {
        "relevance": 3.82,
        "faithfulness": 3.96,
        "answer_quality": 4.12,
        "overall_score": 3.9666666666666672
      },
      "total_response_time": 541.8740634918213,
      "avg_response_time": 10.837481269836426,
      "total_citations": 149,
      "avg_citations": 2.98,
      "total_retrieved_docs": 1459,
      "avg_retrieved_docs": 29.18
    },
    "gpt-3.5-turbo": {
      "total_questions": 50,
      "successful_evaluations": 50,
      "avg_scores": {
        "relevance": 3.68,
        "faithfulness": 3.58,
        "answer_quality": 3.7,
        "overall_score": 3.6533333333333333
      },
      "total_response_time": 181.95853090286255,
      "avg_response_time": 3.639170618057251,
      "total_citations": 178,
      "avg_citations": 3.56,
      "total_retrieved_docs": 1459,
      "avg_retrieved_docs": 29.18
    },
    "google/gemini-2.5-flash": {
      "total_questions": 50,
      "successful_evaluations": 50,
      "avg_scores": {
        "relevance": 3.94,
        "faithfulness": 4.2,
        "answer_quality": 4.32,
        "overall_score": 4.153333333333332
      },
      "total_response_time": 108.59196019172668,
      "avg_response_time": 2.1718392038345335,
      "total_citations": 175,
      "avg_citations": 3.5,
      "total_retrieved_docs": 1459,
      "avg_retrieved_docs": 29.18
    }
  }
}